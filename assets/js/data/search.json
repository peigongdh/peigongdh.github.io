[ { "title": "go向关闭的channel发送数据bug修复", "url": "/posts/send-on-closed-channel/", "categories": "cs", "tags": "go, channel", "date": "2022-08-15 00:00:00 +0800", "snippet": "背景 遇到一个bug，程序偶发报错panic：send on closed channel 经过定位，可以发现协程刚好在向channel写数据时关闭还原场景package mainimport ( &quot;fmt&quot; &quot;time&quot;)func main() { work() &amp;lt;-time.After(2000 * time.Millisecond)}func work() { fmt.Println(&quot;[work] start&quot;) resultChan := make(chan int) // 打开该注释以重现panic // defer close(resultChan) stopChan := make(chan interface{}, 1) defer close(stopChan) go func() { fmt.Println(&quot;[job goroutine] start&quot;) result := job() select { case &amp;lt;-stopChan: fmt.Println(&quot;[job goroutine] stop channel close&quot;) return default: fmt.Println(&quot;[job goroutine] normal return&quot;) resultChan &amp;lt;- result } fmt.Println(&quot;[job goroutine] end&quot;) }() select { case result := &amp;lt;-resultChan: fmt.Printf(&quot;[work] done, result: %d\\n&quot;, result) case &amp;lt;-time.After(1000 * time.Millisecond): stopChan &amp;lt;- true fmt.Println(&quot;[work] timeout&quot;) } fmt.Println(&quot;[work] end&quot;)}func job() int { fmt.Println(&quot;[job] start&quot;) time.Sleep(1000 * time.Millisecond) fmt.Println(&quot;[job] end&quot;) return 1}当我们打开第31行的注释执行时，会发现在一定概率下报错：[work] start[job goroutine] start[job] start[job] end[job goroutine] normal return[work] timeout[work] endpanic: send on closed channel 还原的场景中，work协程和job goroutine都刚好在time.Sleep 1s后操作channel，一定概率出现在发送中关闭channel，即panic 对于channel的使用建议，是在发送端关闭channel，但是此处需要在接收端关闭channel，如何解决？ 一个可行的方案是，不要手动关闭channel，交给GC处理:)" }, { "title": "使用vscode的正则表达式提取文本", "url": "/posts/vscode-pickup/", "categories": "tool", "tags": "vscode, regular-expression", "date": "2022-05-04 00:00:00 +0800", "snippet": "背景工作中遇到需要从日志中提取用户uid，并做去重，如何使用vscode快速实现解决方案提取日志，按换行符分隔，每行一条日志，例如：Info 2022-04-01 00:06:00.748+08:00 uid = &#39;asdasdafsss&#39; and condition_a = &#39;1&#39; Cost:2.03msInfo 2022-04-02 00:05:00.748+08:00 uid = &#39;asdasdasdfa&#39; and condition_a = &#39;1&#39; Cost:2.03msInfo 2022-04-03 00:04:00.748+08:00 uid = &#39;safafsfasxx&#39; and condition_a = &#39;1&#39; Cost:2.03msInfo 2022-04-04 00:03:00.748+08:00 uid = &#39;uufghfhgfhf&#39; and condition_a = &#39;1&#39; Cost:2.03msInfo 2022-04-05 00:02:00.748+08:00 uid = &#39;fesxaxsaffa&#39; and condition_a = &#39;1&#39; Cost:2.03msInfo 2022-04-06 00:01:00.748+08:00 uid = &#39;sagevyutyio&#39; and condition_a = &#39;1&#39; Cost:2.03ms找出日志的通用格式，使用正则表达式筛选出包含uid的关键内容：uid = &#39;[a-z]*&#39;使用快捷键选中内容：shift+command+L或者使用菜单选中内容：selection -&amp;gt; select all occurrences复制到新的文件中，再做两次全局替换将【uid = ‘】和【’】替换为空白即可去重使用插件Transformer，使用shift+command+p打开命令行，使用unique lines去重" }, { "title": "go string指针", "url": "/posts/string-pointer/", "categories": "cs", "tags": "go, string, pointer", "date": "2022-01-21 00:00:00 +0800", "snippet": "背景最近开发中观察到业务中使用了大量的string指针，感觉极其别扭之前对go中string的理解是类似于c++的vector这样的容器，已经在内部封装了string header的引用不考虑二级指针的场景，笔者认为没有必要再次对string使用引用来节省空间，这里做了一些验证实验package mainimport ( &quot;reflect&quot; &quot;unsafe&quot; &quot;github.com/davecgh/go-spew/spew&quot;)func xx(s string) { sh := *(*reflect.StringHeader)(unsafe.Pointer(&amp;amp;s)) spew.Dump(sh)}func xx1(s string) { sh := *(*reflect.StringHeader)(unsafe.Pointer(&amp;amp;s)) sh.Data += 1 spew.Dump(sh)}func main() { s := &quot;xx&quot; sh := *(*reflect.StringHeader)(unsafe.Pointer(&amp;amp;s)) spew.Dump(sh) xx(s) xx(s[:1]) xx(s[1:]) xx1(s)}(reflect.StringHeader) { Data: (uintptr) 0x10d42c1, Len: (int) 2}(reflect.StringHeader) { Data: (uintptr) 0x10d42c1, Len: (int) 2}(reflect.StringHeader) { Data: (uintptr) 0x10d42c1, Len: (int) 1}(reflect.StringHeader) { Data: (uintptr) 0x10d42c2, Len: (int) 1}(reflect.StringHeader) { Data: (uintptr) 0x10d42c2, Len: (int) 2}参考 https://blog.thinkeridea.com/201902/go/string_ye_shi_yin_yong_lei_xing.html" }, { "title": "git实践 笔记", "url": "/posts/git-practice-note/", "categories": "cs", "tags": "git", "date": "2022-01-11 00:00:00 +0800", "snippet": "rebase之前rebase用的不多，现在新的代码规范要求在自己的分支上rebase主干后再提交commit重新梳理了一遍rebase的含义，其关键可以从字面理解，re-base，base就是我们checkout的那个commitmine : -&amp;gt; D -&amp;gt; Emaster : A -&amp;gt; B -&amp;gt; C -&amp;gt; F当我们完成自己的开发想把自己的分支合并上去的时候，此时主干上已经有了新提交如果我们不希望使用merge使主干变得杂乱，就用rebase将原来commit的基点从C变为F，即变基mine : -&amp;gt; D -&amp;gt; Emaster : A -&amp;gt; B -&amp;gt; C -&amp;gt; F虽然这样历史提交的真实过程被篡改了，但是如果只是在自己的分支上使用，可以使提交记录维护地非常干净squash像上面的rebase过程中，经常会遇到反复fix同一处conflict的问题，为了解决类似的问题，可以使用squash来压缩commitmine : -&amp;gt; D1 -&amp;gt; D2master : A -&amp;gt; B -&amp;gt; C以这个场景为例，我们使用rebase merge的方式，将mine的内容合并到master中git checkout minegit rebase -i master# git rebase -i master 类似于下面的操作# git rebase -i HEAD~N# git rebase master命令行提示如下pick 442ea7d 10th commit in feature/reabase-practicepick 266af78 11th commit in feature/reabase-practice# Rebase e8252be..266af78 onto e8252be (2 commands)## Commands:# p, pick &amp;lt;commit&amp;gt; = use commit# r, reword &amp;lt;commit&amp;gt; = use commit, but edit the commit message# e, edit &amp;lt;commit&amp;gt; = use commit, but stop for amending# s, squash &amp;lt;commit&amp;gt; = use commit, but meld into previous commit# f, fixup &amp;lt;commit&amp;gt; = like &quot;squash&quot;, but discard this commit&#39;s log message# x, exec &amp;lt;command&amp;gt; = run command (the rest of the line) using shell# b, break = stop here (continue rebase later with &#39;git rebase --continue&#39;)# d, drop &amp;lt;commit&amp;gt; = remove commit# l, label &amp;lt;label&amp;gt; = label current HEAD with a name# t, reset &amp;lt;label&amp;gt; = reset HEAD to a label# m, merge [-C &amp;lt;commit&amp;gt; | -c &amp;lt;commit&amp;gt;] &amp;lt;label&amp;gt; [# &amp;lt;oneline&amp;gt;]# . create a merge commit using the original merge commit&#39;s# . message (or the oneline, if no original merge commit was# . specified). Use -c &amp;lt;commit&amp;gt; to reword the commit message.## These lines can be re-ordered; they are executed from top to bottom.## If you remove a line here THAT COMMIT WILL BE LOST.## However, if you remove everything, the rebase will be aborted.#我们编辑pick的选项为需要的效果，比如squash或者fixup（丢失log）squash后的commit会在顶部的pick中保留全部logmine : -&amp;gt; D&#39;master : A -&amp;gt; B -&amp;gt; C即D1和D2合并为D’此时再提交merge request，进行后续code review流程" }, { "title": "跨源资源共享 笔记", "url": "/posts/cors-note/", "categories": "cs, web", "tags": "http, cors", "date": "2021-12-29 00:00:00 +0800", "snippet": "背景日常在开发过程中经常遇到CORS相关的错误，解决方案也一直比较模糊，这次一起来把跨域相关的内容做一次梳理 Q：什么是跨域？ A：跨源HTTP请求的一个例子：运行在 https://domain-a.com 的 JavaScript 代码使用 XMLHttpRequest 来发起一个到 https://domain-b.com/data.json 的请求。 Q：为什么要做跨域的限制？ A：出于安全性，浏览器限制脚本内发起的跨源HTTP请求。 例如，XMLHttpRequest 和 Fetch API 遵循同源策略。这意味着使用这些 API 的 Web 应用程序只能从加载应用程序的同一个域请求 HTTP 资源。 A：补充：考虑某个钓鱼网站A抓包模拟了网站B的API，A伪装B，误导用户在A中填写B的表单（例如账号密码），如果没有跨域的保护（这一步是在浏览器主动做出的限制），那么就存在很明显的安全隐患。 Q：现在主流的前后端开发分离，前后端的域名经常存在不同的情况，如何解决跨域问题？ A：使用CORS，也就是跨域资源共享的方案，CORS是一个W3C标准，目的就是解决此类跨域共享的问题。简单请求比如说，假如站点 https://foo.example 的网页应用想要访问 https://bar.other 的资源。foo.example 的网页中可能包含类似于下面的 JavaScript 代码：const xhr = new XMLHttpRequest();const url = &#39;https://bar.other/resources/public-data/&#39;;xhr.open(&#39;GET&#39;, url);xhr.onreadystatechange = someHandler;xhr.send();以下是浏览器发送给服务器的请求报文：GET /resources/public-data/ HTTP/1.1Host: bar.otherUser-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.14; rv:71.0) Gecko/20100101 Firefox/71.0Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8Accept-Language: en-us,en;q=0.5Accept-Encoding: gzip,deflateConnection: keep-aliveOrigin: https://foo.example请求首部字段 Origin 表明该请求来源于 http://foo.example。HTTP/1.1 200 OKDate: Mon, 01 Dec 2008 00:23:53 GMTServer: Apache/2Access-Control-Allow-Origin: *Keep-Alive: timeout=2, max=100Connection: Keep-AliveTransfer-Encoding: chunkedContent-Type: application/xml[XML Data]本例中，服务端返回的 Access-Control-Allow-Origin: * 表明，该资源可以被 任意 外域访问。Access-Control-Allow-Origin: * 使用 Origin 和 Access-Control-Allow-Origin 就能完成最简单的访问控制。如果服务端仅允许来自 https://foo.example 的访问，该首部字段的内容如下：Access-Control-Allow-Origin: https://foo.example预检请求与前述简单请求不同，“需预检的请求”要求必须首先使用 OPTIONS 方法发起一个预检请求到服务器，以获知服务器是否允许该实际请求。”预检请求“的使用，可以避免跨域请求对服务器的用户数据产生未预期的影响。如下是一个需要执行预检请求的 HTTP 请求：const xhr = new XMLHttpRequest();xhr.open(&#39;POST&#39;, &#39;https://bar.other/resources/post-here/&#39;);xhr.setRequestHeader(&#39;X-PINGOTHER&#39;, &#39;pingpong&#39;);xhr.setRequestHeader(&#39;Content-Type&#39;, &#39;application/xml&#39;);xhr.onreadystatechange = handler;xhr.send(&#39;&amp;lt;person&amp;gt;&amp;lt;name&amp;gt;Arun&amp;lt;/name&amp;gt;&amp;lt;/person&amp;gt;&#39;);下面是服务端和客户端完整的信息交互。首次交互是 预检请求/响应：OPTIONS /doc HTTP/1.1Host: bar.otherUser-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.14; rv:71.0) Gecko/20100101 Firefox/71.0Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8Accept-Language: en-us,en;q=0.5Accept-Encoding: gzip,deflateConnection: keep-aliveOrigin: https://foo.exampleAccess-Control-Request-Method: POSTAccess-Control-Request-Headers: X-PINGOTHER, Content-TypeHTTP/1.1 204 No ContentDate: Mon, 01 Dec 2008 01:15:39 GMTServer: Apache/2Access-Control-Allow-Origin: https://foo.exampleAccess-Control-Allow-Methods: POST, GET, OPTIONSAccess-Control-Allow-Headers: X-PINGOTHER, Content-TypeAccess-Control-Max-Age: 86400Vary: Accept-Encoding, OriginKeep-Alive: timeout=2, max=100Connection: Keep-Alive浏览器检测到，从 JavaScript 中发起的请求需要被预检。从上面的报文中，我们看到，第 1~10 行发送了一个使用 OPTIONS 方法 的“预检请求”。OPTIONS 是 HTTP/1.1 协议中定义的方法，用以从服务器获取更多信息。该方法不会对服务器资源产生影响。 预检请求中同时携带了下面两个首部字段：Access-Control-Request-Method: POSTAccess-Control-Request-Headers: X-PINGOTHER, Content-Type首部字段 Access-Control-Request-Method 告知服务器，实际请求将使用 POST 方法。首部字段 Access-Control-Request-Headers 告知服务器，实际请求将携带两个自定义请求首部字段：X-PINGOTHER 与 Content-Type。服务器据此决定，该实际请求是否被允许。第 13~22 行为预检请求的响应，表明服务器将接受后续的实际请求。重点看第 16~19 行：Access-Control-Allow-Origin: https://foo.exampleAccess-Control-Allow-Methods: POST, GET, OPTIONSAccess-Control-Allow-Headers: X-PINGOTHER, Content-TypeAccess-Control-Max-Age: 86400服务器的响应携带了 Access-Control-Allow-Origin: https://foo.example，从而限制请求的源域。同时，携带的 Access-Control-Allow-Methods 表明服务器允许客户端使用 POST 和 GET 方法发起请求（与 Allow 响应首部类似，但其具有严格的访问控制）。首部字段 Access-Control-Allow-Headers 表明服务器允许请求中携带字段 X-PINGOTHER 与 Content-Type。与 Access-Control-Allow-Methods 一样，Access-Control-Allow-Headers 的值为逗号分割的列表。最后，首部字段 Access-Control-Max-Age 表明该响应的有效时间为 86400 秒，也就是 24 小时。在有效时间内，浏览器无须为同一请求再次发起预检请求。请注意，浏览器自身维护了一个 最大有效时间，如果该首部字段的值超过了最大有效时间，将不会生效。预检请求完成之后，发送实际请求：POST /doc HTTP/1.1Host: bar.otherUser-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.14; rv:71.0) Gecko/20100101 Firefox/71.0Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8Accept-Language: en-us,en;q=0.5Accept-Encoding: gzip,deflateConnection: keep-aliveX-PINGOTHER: pingpongContent-Type: text/xml; charset=UTF-8Referer: https://foo.example/examples/preflightInvocation.htmlContent-Length: 55Origin: https://foo.examplePragma: no-cacheCache-Control: no-cache&amp;lt;person&amp;gt;&amp;lt;name&amp;gt;Arun&amp;lt;/name&amp;gt;&amp;lt;/person&amp;gt;HTTP/1.1 200 OKDate: Mon, 01 Dec 2008 01:15:40 GMTServer: Apache/2Access-Control-Allow-Origin: https://foo.exampleVary: Accept-Encoding, OriginContent-Encoding: gzipContent-Length: 235Keep-Alive: timeout=2, max=99Connection: Keep-AliveContent-Type: text/plain[Some XML payload]附带身份凭证的请求XMLHttpRequest 或 Fetch 与 CORS 的一个有趣的特性是，可以基于 HTTP cookies 和 HTTP 认证信息发送身份凭证。一般而言，对于跨源 XMLHttpRequest 或 Fetch 请求，浏览器 不会 发送身份凭证信息。如果要发送凭证信息，需要设置 XMLHttpRequest 的某个特殊标志位。本例中，https://foo.example 的某脚本向 https://bar.other 发起一个GET 请求，并设置 Cookies：const invocation = new XMLHttpRequest();const url = &#39;https://bar.other/resources/credentialed-content/&#39;;function callOtherDomain() { if (invocation) { invocation.open(&#39;GET&#39;, url, true); invocation.withCredentials = true; invocation.onreadystatechange = handler; invocation.send(); }}第 7 行将 XMLHttpRequest 的 withCredentials 标志设置为 true，从而向服务器发送 Cookies。因为这是一个简单 GET 请求，所以浏览器不会对其发起“预检请求”。但是，如果服务器端的响应中未携带 Access-Control-Allow-Credentials: true，浏览器将不会把响应内容返回给请求的发送者。客户端与服务器端交互示例如下：GET /resources/credentialed-content/ HTTP/1.1Host: bar.otherUser-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.14; rv:71.0) Gecko/20100101 Firefox/71.0Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8Accept-Language: en-us,en;q=0.5Accept-Encoding: gzip,deflateConnection: keep-aliveReferer: https://foo.example/examples/credential.htmlOrigin: https://foo.exampleCookie: pageAccess=2HTTP/1.1 200 OKDate: Mon, 01 Dec 2008 01:34:52 GMTServer: Apache/2Access-Control-Allow-Origin: https://foo.exampleAccess-Control-Allow-Credentials: trueCache-Control: no-cachePragma: no-cacheSet-Cookie: pageAccess=3; expires=Wed, 31-Dec-2008 01:34:53 GMTVary: Accept-Encoding, OriginContent-Encoding: gzipContent-Length: 106Keep-Alive: timeout=2, max=100Connection: Keep-AliveContent-Type: text/plain[text/plain payload]即使第 10 行指定了 Cookie 的相关信息，但是，如果 https://bar.other 的响应中缺失 Access-Control-Allow-Credentials: true（第 17 行），则响应内容不会返回给请求的发起者。服务端实践日常开发中，以go/gin为例，我们常常这样设置中间件解决问题：func Cors() gin.HandlerFunc { return func(c *gin.Context) { method := c.Request.Method if origin != &quot;&quot; { c.Header(&quot;Access-Control-Allow-Origin&quot;, &quot;*&quot;) c.Header(&quot;Access-Control-Allow-Methods&quot;, &quot;POST, GET, OPTIONS, PUT, DELETE, UPDATE&quot;) c.Header(&quot;Access-Control-Allow-Headers&quot;, &quot;Origin, X-Requested-With, Content-Type, Accept, Authorization&quot;) c.Header(&quot;Access-Control-Expose-Headers&quot;, &quot;Content-Length, Access-Control-Allow-Origin, Access-Control-Allow-Headers, Cache-Control, Content-Language, Content-Type&quot;) c.Header(&quot;Access-Control-Allow-Credentials&quot;, &quot;true&quot;) } if method == &quot;OPTIONS&quot; { c.AbortWithStatus(http.StatusNoContent) } c.Next() }}参考 https://developer.mozilla.org/zh-CN/docs/Web/HTTP/CORS https://www.ruanyifeng.com/blog/2016/04/cors.html" }, { "title": "http get列表参数 笔记", "url": "/posts/http-get-params-list/", "categories": "cs, web", "tags": "http", "date": "2021-12-23 00:00:00 +0800", "snippet": "背景考虑在http中使用get传递列表类型的参数，在网上很容易找到如下例子： ?k[]=2&amp;amp;k[]=3 ?k=2&amp;amp;k=3我们通过测试来验证：测试使用gin：package mainimport &quot;github.com/gin-gonic/gin&quot;func main() { r := gin.Default() r.GET(&quot;/xget&quot;, func(c *gin.Context) { ks := c.QueryArray(&quot;k&quot;) c.JSON(200, gin.H{ &quot;ks&quot;: ks, }) }) r.Run() // listen and serve on 0.0.0.0:8080 (for windows &quot;localhost:8080&quot;)}curl &#39;http://127.0.0.1:8080/xget?k\\[\\]=1&amp;amp;k\\[\\]=2&#39;{&quot;ks&quot;:[]}curl &#39;http://127.0.0.1:8080/xget?k=1&amp;amp;k=2&#39;{&quot;ks&quot;:[&quot;1&quot;,&quot;2&quot;]}显然，应该使用方案2来解决http中列表参数的问题注意网络上有很多其他流行的说法： 使用json格式encode参数列表 在get中使用request body 使用k=1,2这样逗号分隔符来解决的其中点1，3可以解决问题，但是相当于对参数做了一层额外的编码解码，不利于使用点2最容易误导人，因为在http的标准中并没有在get中使用request body，参考MDN的文档：https://developer.mozilla.org/en-US/docs/Web/HTTP/Methods/GET Sending body/payload in a GET request may cause some existing implementations to reject the request — while not prohibited by the specification, the semantics are undefined. It is better to just avoid sending payloads in GET requests." }, { "title": "go-disqueue 笔记", "url": "/posts/nsq-diskqueue-note/", "categories": "cs, middleware", "tags": "nsq, go-disqueue", "date": "2021-12-21 00:00:00 +0800", "snippet": "源码 https://github.com/nsqio/go-diskqueue背景阅读nsq源码的时候遇到了该依赖项，得知在nsq中，当channel缓冲区超过mem_queue_size时，其他消息往文件队列里面写入func NewTopic(topicName string, nsqd *NSQD, deleteCallback func(*Topic)) *Topic { // ... if nsqd.getOpts().MemQueueSize &amp;gt; 0 { t.memoryMsgChan = make(chan *Message, nsqd.getOpts().MemQueueSize) } // ...}func (t *Topic) put(m *Message) error { select { case t.memoryMsgChan &amp;lt;- m: default: err := writeMessageToBackend(m, t.backend) t.nsqd.SetHealth(err) if err != nil { t.nsqd.logf(LOG_ERROR, &quot;TOPIC(%s) ERROR: failed to write message to backend - %s&quot;, t.name, err) return err } } return nil}func writeMessageToBackend(msg *Message, bq BackendQueue) error { buf := bufferPoolGet() defer bufferPoolPut(buf) _, err := msg.WriteTo(buf) if err != nil { return err } return bq.Put(buf.Bytes())}// BackendQueue represents the behavior for the secondary message// storage systemtype BackendQueue interface { Put([]byte) error ReadChan() &amp;lt;-chan []byte // this is expected to be an *unbuffered* channel Close() error Delete() error Depth() int64 Empty() error}其中DiskQueue就是BackendQueue的一个实现源码阅读func (d *diskQueue) ioLoop() { var dataRead []byte var err error var count int64 var r chan []byte syncTicker := time.NewTicker(d.syncTimeout) for { // dont sync all the time :) if count == d.syncEvery { d.needSync = true } if d.needSync { err = d.sync() if err != nil { d.logf(ERROR, &quot;DISKQUEUE(%s) failed to sync - %s&quot;, d.name, err) } count = 0 } // readPos int64 // 文件读取的位置 // writePos int64 // 文件写入的位置 // readFileNum int64 // 读取文件编号 // writeFileNum int64 // 写入文件编号 // 判断读取 if (d.readFileNum &amp;lt; d.writeFileNum) || (d.readPos &amp;lt; d.writePos) { if d.nextReadPos == d.readPos { dataRead, err = d.readOne() if err != nil { d.logf(ERROR, &quot;DISKQUEUE(%s) reading at %d of %s - %s&quot;, d.name, d.readPos, d.fileName(d.readFileNum), err) d.handleReadError() continue } } r = d.readChan } else { r = nil } select { // the Go channel spec dictates that nil channel operations (read or write) // in a select are skipped, we set r to d.readChan only when there is data to read // 如果没有读取到，则此处跳过 case r &amp;lt;- dataRead: count++ // moveForward sets needSync flag if a file is removed d.moveForward() // 响应empty()，删除所有问题件 case &amp;lt;-d.emptyChan: d.emptyResponseChan &amp;lt;- d.deleteAllFiles() count = 0 // 写入到磁盘 case dataWrite := &amp;lt;-d.writeChan: count++ d.writeResponseChan &amp;lt;- d.writeOne(dataWrite) // 定时器触发同步标识 case &amp;lt;-syncTicker.C: if count == 0 { // avoid sync when there&#39;s no activity continue } d.needSync = true // 退出 case &amp;lt;-d.exitChan: goto exit } }exit: d.logf(INFO, &quot;DISKQUEUE(%s): closing ... ioLoop&quot;, d.name) syncTicker.Stop() d.exitSyncChan &amp;lt;- 1} needSync可以被两个条件触发： A. count阈值超过syncEvery（默认2500） B. ticket触发且count不为0（默认2s） &amp;lt;- d.emptyChan删除所有相关的元数据文件，下次写入会创建新的读写文件，不会停止程序// sync fsyncs the current writeFile and persists metadatafunc (d *diskQueue) sync() error { if d.writeFile != nil { err := d.writeFile.Sync() if err != nil { d.writeFile.Close() d.writeFile = nil return err } } err := d.persistMetaData() if err != nil { return err } d.needSync = false return nil} sync()做了两件事: A. 同步writeFile（d.writeFile.Sync()） B. 持久化了元数据（d.persistMetaData()） sync系统调用：将所有修改过的缓冲区排入写队列，然后就返回了，它并不等实际的写磁盘的操作结束。所以它的返回并不能保证数据的安全性。通常会有一个update系统守护进程每隔30s调用一次sync。 https://www.cnblogs.com/ZhuChangwu/p/14047108.html// readOne performs a low level filesystem read for a single []byte// while advancing read positions and rolling files, if necessaryfunc (d *diskQueue) readOne() ([]byte, error) { var err error var msgSize int32 // 初始化readFile if d.readFile == nil { curFileName := d.fileName(d.readFileNum) d.readFile, err = os.OpenFile(curFileName, os.O_RDONLY, 0600) if err != nil { return nil, err } d.logf(INFO, &quot;DISKQUEUE(%s): readOne() opened %s&quot;, d.name, curFileName) // 根据readPos恢复到之前读取的位置 if d.readPos &amp;gt; 0 { _, err = d.readFile.Seek(d.readPos, 0) if err != nil { d.readFile.Close() d.readFile = nil return nil, err } } // 使用缓冲区来读取 d.reader = bufio.NewReader(d.readFile) } // 使用大端字节序读取4字节的msgSize err = binary.Read(d.reader, binary.BigEndian, &amp;amp;msgSize) if err != nil { d.readFile.Close() d.readFile = nil return nil, err } if msgSize &amp;lt; d.minMsgSize || msgSize &amp;gt; d.maxMsgSize { // this file is corrupt and we have no reasonable guarantee on // where a new message should begin d.readFile.Close() d.readFile = nil return nil, fmt.Errorf(&quot;invalid message read size (%d)&quot;, msgSize) } // 根据msgSize一次性读取整个消息体 readBuf := make([]byte, msgSize) _, err = io.ReadFull(d.reader, readBuf) if err != nil { d.readFile.Close() d.readFile = nil return nil, err } totalBytes := int64(4 + msgSize) // we only advance next* because we have not yet sent this to consumers // (where readFileNum, readPos will actually be advanced) d.nextReadPos = d.readPos + totalBytes d.nextReadFileNum = d.readFileNum // TODO: each data file should embed the maxBytesPerFile // as the first 8 bytes (at creation time) ensuring that // the value can change without affecting runtime // 当超过文件最大值时文件id自增 if d.nextReadPos &amp;gt; d.maxBytesPerFile { if d.readFile != nil { d.readFile.Close() d.readFile = nil } d.nextReadFileNum++ d.nextReadPos = 0 } return readBuf, nil}延伸：大小端字节序利弊：Big-endian 的内存顺序和数字的书写顺序是一致的，方便阅读理解。Little-endian 在变量指针转换的时候地址保持不变，比如 int64* 转到 int32* https://blog.csdn.net/waitingbb123/article/details/80504093https://www.zhihu.com/question/29266331/answer/43794715TODO: nextReadPos, nextReadFileNum如何同步到readPos, readFileNum？// writeOne performs a low level filesystem write for a single []byte// while advancing write positions and rolling files, if necessaryfunc (d *diskQueue) writeOne(data []byte) error { var err error // 初始化 if d.writeFile == nil { curFileName := d.fileName(d.writeFileNum) d.writeFile, err = os.OpenFile(curFileName, os.O_RDWR|os.O_CREATE, 0600) if err != nil { return err } d.logf(INFO, &quot;DISKQUEUE(%s): writeOne() opened %s&quot;, d.name, curFileName) if d.writePos &amp;gt; 0 { _, err = d.writeFile.Seek(d.writePos, 0) if err != nil { d.writeFile.Close() d.writeFile = nil return err } } } dataLen := int32(len(data)) if dataLen &amp;lt; d.minMsgSize || dataLen &amp;gt; d.maxMsgSize { return fmt.Errorf(&quot;invalid message write size (%d) maxMsgSize=%d&quot;, dataLen, d.maxMsgSize) } // 重置writeBuf，write的方式与read对应 d.writeBuf.Reset() err = binary.Write(&amp;amp;d.writeBuf, binary.BigEndian, dataLen) if err != nil { return err } // 先写buf _, err = d.writeBuf.Write(data) if err != nil { return err } // only write to the file once // 再通过buf写入到文件 _, err = d.writeFile.Write(d.writeBuf.Bytes()) if err != nil { d.writeFile.Close() d.writeFile = nil return err } // 更新writePos，depth自增1 totalBytes := int64(4 + dataLen) d.writePos += totalBytes atomic.AddInt64(&amp;amp;d.depth, 1) // 当超过文件最大值时文件id自增 if d.writePos &amp;gt;= d.maxBytesPerFile { d.writeFileNum++ d.writePos = 0 // sync every time we start writing to a new file // 每当新的文件创建时会做一次sync err = d.sync() if err != nil { d.logf(ERROR, &quot;DISKQUEUE(%s) failed to sync - %s&quot;, d.name, err) } if d.writeFile != nil { d.writeFile.Close() d.writeFile = nil } } return err}func (d *diskQueue) moveForward() { oldReadFileNum := d.readFileNum d.readFileNum = d.nextReadFileNum d.readPos = d.nextReadPos depth := atomic.AddInt64(&amp;amp;d.depth, -1) // see if we need to clean up the old file if oldReadFileNum != d.nextReadFileNum { // sync every time we start reading from a new file d.needSync = true fn := d.fileName(oldReadFileNum) err := os.Remove(fn) if err != nil { d.logf(ERROR, &quot;DISKQUEUE(%s) failed to Remove(%s) - %s&quot;, d.name, fn, err) } } d.checkTailCorruption(depth)} moveForward在写入完成后更新readFileNum和readPos 当readFileNum文件自增id后，也就是读取的位置已经到了下个文件，即可清理旧文件注意diskqueue使用了一个metadata文件存储了读写位置的相关信息使用了id自增的文件存储了持久化的消息数据（当读到新的切片时，旧的文件会被清理掉）readFileNum, writeRileNum是被一起写入到了meta文件中readFile, writeFile则是分别使用读写的方式打开的同一文件（也就是消息数据的文件）" }, { "title": "2021-12-21日常 笔记", "url": "/posts/daily-note/", "categories": "daily", "tags": "daily", "date": "2021-12-21 00:00:00 +0800", "snippet": "在日常中经常遇到表达如下表达：当，仅当，当且仅当如此不自然的表达很显然是个舶来品，研究下了，发现该表达对应英语：if, only if, if and only if那么从逻辑上应该如何理解？ A if B 对应 A -&amp;gt; B，理解为A是B的充分条件 A only if B 对应 A &amp;lt;- B，理解为A是B的必要条件（或者说B是A的充分条件） A and if only if B 对应 A &amp;lt;-&amp;gt; B，理解为A是B的充要条件这里需要重点理解点1和点2的区别，在于A和B的区别是表达式调换了主语即：A if B 等价于 B only if A" }, { "title": "nsq队列 笔记", "url": "/posts/nsq-note/", "categories": "cs, middleware", "tags": "mq, nsq", "date": "2021-12-14 00:00:00 +0800", "snippet": "源码 https://github.com/nsqio/nsq使用1.20版本 https://github.com/nsqio/go-diskqueuensq依赖的FIFO磁盘队列源码阅读启动make编译后，在build下逐个启动nsq相关进程// 启动nsqlookup./nsqlookupd// 新建nsqd数据目录mkdir /tmp/nsqdata1 /tmp/nsqdata2// 启动2个nsqd实例./nsqd --lookupd-tcp-address=127.0.0.1:4160 -broadcast-address=127.0.0.1 -tcp-address=127.0.0.1:4150 -http-address=0.0.0.0:4151 -data-path=/tmp/nsqdata1./nsqd --lookupd-tcp-address=127.0.0.1:4160 -broadcast-address=127.0.0.1 -tcp-address=127.0.0.1:4152 -http-address=0.0.0.0:4153 -data-path=/tmp/nsqdata2// 启动nsqadmin./nsqadmin --lookupd-http-address=localhost:4161磁盘加载我们在nsqd1上publish一个消息，但是不做消费producer.Publish(&quot;test-normal&quot;, []byte(&quot;x&quot;))中断nsqd1进程，观察数据目录：➜ nsqdata1 lltotal 8drwxr-xr-x 3 zhangpei wheel 96B Dec 14 16:46 .drwxrwxrwt 11 root wheel 352B Dec 14 15:30 ..-rw------- 1 zhangpei wheel 88B Dec 14 16:46 nsqd.dat➜ nsqdata1 tail -f nsqd.dat{&quot;topics&quot;:[{&quot;channels&quot;:[],&quot;name&quot;:&quot;test-normal&quot;,&quot;paused&quot;:false}],&quot;version&quot;:&quot;1.2.1-alpha&quot;}q^C➜ nsqdata1 lltotal 24drwxr-xr-x 5 zhangpei wheel 160B Dec 14 16:46 .drwxrwxrwt 11 root wheel 352B Dec 14 15:30 ..-rw------- 1 zhangpei wheel 88B Dec 14 16:46 nsqd.dat-rw------- 1 zhangpei wheel 31B Dec 14 16:46 test-normal.diskqueue.000000.dat-rw------- 1 zhangpei wheel 11B Dec 14 16:46 test-normal.diskqueue.meta.dat➜ nsqdata1 tail -f test-normal.diskqueue.000000.dat���b&quot;4�0ffd2790f4857000x^C➜ nsqdata1 tail -f test-normal.diskqueue.meta.dat10,00,31nsqd.dat用于存放nsq的meta数据，也就是所有的topic和channel，还有version等数据test-normal.diskqueue.meta.dat用于存放磁盘队列的meta数据，加载的顺序是：LoadMetadata -&amp;gt; GetTopic -&amp;gt; NewTopic -&amp;gt; diskqueue.Newfunc NewTopic(topicName string, nsqd *NSQD, deleteCallback func(*Topic)) *Topic { // ... if strings.HasSuffix(topicName, &quot;#ephemeral&quot;) { t.ephemeral = true t.backend = newDummyBackendQueue() } else { dqLogf := func(level diskqueue.LogLevel, f string, args ...interface{}) { opts := nsqd.getOpts() lg.Logf(opts.Logger, opts.LogLevel, lg.LogLevel(level), f, args...) } t.backend = diskqueue.New( topicName, nsqd.getOpts().DataPath, nsqd.getOpts().MaxBytesPerFile, int32(minValidMsgLength), int32(nsqd.getOpts().MaxMsgSize)+minValidMsgLength, nsqd.getOpts().SyncEvery, nsqd.getOpts().SyncTimeout, dqLogf, ) } // ...}我们继续进入diskqueue阅读New -&amp;gt; retrieveMetaData -&amp;gt; ioLoopretrieveMetaData载入了文件系统的游标参数，其中对应了上面的&amp;amp;depth,&amp;amp;d.readFileNum, &amp;amp;d.readPos,&amp;amp;d.writeFileNum, &amp;amp;d.writePos10,00,31// New instantiates an instance of diskQueue, retrieving metadata// from the filesystem and starting the read ahead goroutinefunc New(name string, dataPath string, maxBytesPerFile int64, minMsgSize int32, maxMsgSize int32, syncEvery int64, syncTimeout time.Duration, logf AppLogFunc) Interface { // ... // no need to lock here, nothing else could possibly be touching this instance err := d.retrieveMetaData() if err != nil &amp;amp;&amp;amp; !os.IsNotExist(err) { d.logf(ERROR, &quot;DISKQUEUE(%s) failed to retrieveMetaData - %s&quot;, d.name, err) } go d.ioLoop() return &amp;amp;d}// retrieveMetaData initializes state from the filesystemfunc (d *diskQueue) retrieveMetaData() error { var f *os.File var err error fileName := d.metaDataFileName() f, err = os.OpenFile(fileName, os.O_RDONLY, 0600) if err != nil { return err } defer f.Close() var depth int64 _, err = fmt.Fscanf(f, &quot;%d\\n%d,%d\\n%d,%d\\n&quot;, &amp;amp;depth, &amp;amp;d.readFileNum, &amp;amp;d.readPos, &amp;amp;d.writeFileNum, &amp;amp;d.writePos) if err != nil { return err } atomic.StoreInt64(&amp;amp;d.depth, depth) d.nextReadFileNum = d.readFileNum d.nextReadPos = d.readPos return nil}TODO: ioLoop分析参考资料： https://zhuanlan.zhihu.com/p/268041954// ioLoop provides the backend for exposing a go channel (via ReadChan())// in support of multiple concurrent queue consumers//// it works by looping and branching based on whether or not the queue has data// to read and blocking until data is either read or written over the appropriate// go channels//// conveniently this also means that we&#39;re asynchronously reading from the filesystemfunc (d *diskQueue) ioLoop() { var dataRead []byte var err error var count int64 var r chan []byte syncTicker := time.NewTicker(d.syncTimeout) for { // dont sync all the time :) if count == d.syncEvery { d.needSync = true } if d.needSync { err = d.sync() if err != nil { d.logf(ERROR, &quot;DISKQUEUE(%s) failed to sync - %s&quot;, d.name, err) } count = 0 } if (d.readFileNum &amp;lt; d.writeFileNum) || (d.readPos &amp;lt; d.writePos) { if d.nextReadPos == d.readPos { dataRead, err = d.readOne() if err != nil { d.logf(ERROR, &quot;DISKQUEUE(%s) reading at %d of %s - %s&quot;, d.name, d.readPos, d.fileName(d.readFileNum), err) d.handleReadError() continue } } r = d.readChan } else { r = nil } select { // the Go channel spec dictates that nil channel operations (read or write) // in a select are skipped, we set r to d.readChan only when there is data to read case r &amp;lt;- dataRead: count++ // moveForward sets needSync flag if a file is removed d.moveForward() case &amp;lt;-d.emptyChan: d.emptyResponseChan &amp;lt;- d.deleteAllFiles() count = 0 case dataWrite := &amp;lt;-d.writeChan: count++ d.writeResponseChan &amp;lt;- d.writeOne(dataWrite) case &amp;lt;-syncTicker.C: if count == 0 { // avoid sync when there&#39;s no activity continue } d.needSync = true case &amp;lt;-d.exitChan: goto exit } }exit: d.logf(INFO, &quot;DISKQUEUE(%s): closing ... ioLoop&quot;, d.name) syncTicker.Stop() d.exitSyncChan &amp;lt;- 1}继续寻觅我们另一个文件的来源，也就是上面提到的test-normal.diskqueue.000000.datfunc (d *diskQueue) fileName(fileNum int64) string { return fmt.Sprintf(path.Join(d.dataPath, &quot;%s.diskqueue.%06d.dat&quot;), d.name, fileNum)}" }, { "title": "gcache源码阅读 笔记", "url": "/posts/gcache-note/", "categories": "cs, middleware", "tags": "cache, gcache, lru", "date": "2021-12-13 00:00:00 +0800", "snippet": "实践样例func main() { gc := gcache.New(30). LRU(). LoaderExpireFunc(func(key interface{}) (value interface{}, expire *time.Duration, err error) { duration := 1 * time.Second expire = &amp;amp;duration value, err = loadValue(key) return }). Build() for { randInt := rand.Int() for i := 0; i &amp;lt; 3; i++ { value, err := gc.Get(fmt.Sprintf(&quot;%d&quot;, randInt)) if err != nil { fmt.Println(err) } else { fmt.Println(&quot;print: &quot;, value.(string)) } time.Sleep(500 * time.Millisecond) } fmt.Println() time.Sleep(time.Second) }}func loadValue(key interface{}) (interface{}, error) { fmt.Println(&quot;loadValue: &quot;, key.(string)) return key, nil}打印如下：loadValue: 5577006791947779410print: 5577006791947779410print: 5577006791947779410loadValue: 5577006791947779410print: 5577006791947779410loadValue: 8674665223082153551print: 8674665223082153551print: 8674665223082153551loadValue: 8674665223082153551print: 8674665223082153551loadValue: 6129484611666145821print: 6129484611666145821print: 6129484611666145821loadValue: 6129484611666145821print: 6129484611666145821源码阅读接口定义type Cache interface { Set(key, value interface{}) error SetWithExpire(key, value interface{}, expiration time.Duration) error Get(key interface{}) (interface{}, error) GetIFPresent(key interface{}) (interface{}, error) GetALL(checkExpired bool) map[interface{}]interface{} get(key interface{}, onLoad bool) (interface{}, error) Remove(key interface{}) bool Purge() Keys(checkExpired bool) []interface{} Len(checkExpired bool) int Has(key interface{}) bool statsAccessor}type baseCache struct { clock Clock size int loaderExpireFunc LoaderExpireFunc evictedFunc EvictedFunc purgeVisitorFunc PurgeVisitorFunc addedFunc AddedFunc deserializeFunc DeserializeFunc serializeFunc SerializeFunc expiration *time.Duration mu sync.RWMutex loadGroup Group *stats}以LRU的实现为例func (c *LRUCache) getWithLoader(key interface{}, isWait bool) (interface{}, error) { if c.loaderExpireFunc == nil { return nil, KeyNotFoundError } value, _, err := c.load(key, func(v interface{}, expiration *time.Duration, e error) (interface{}, error) { if e != nil { return nil, e } c.mu.Lock() defer c.mu.Unlock() item, err := c.set(key, v) if err != nil { return nil, err } if expiration != nil { t := c.clock.Now().Add(*expiration) item.(*lruItem).expiration = &amp;amp;t } return v, nil }, isWait) if err != nil { return nil, err } return value, nil}// load a new value using by specified key.func (c *baseCache) load(key interface{}, cb func(interface{}, *time.Duration, error) (interface{}, error), isWait bool) (interface{}, bool, error) { v, called, err := c.loadGroup.Do(key, func() (v interface{}, e error) { defer func() { if r := recover(); r != nil { e = fmt.Errorf(&quot;Loader panics: %v&quot;, r) } }() return cb(c.loaderExpireFunc(key)) }, isWait) if err != nil { return nil, called, err } return v, called, nil}func (c *LRUCache) getValue(key interface{}, onLoad bool) (interface{}, error) { c.mu.Lock() item, ok := c.items[key] if ok { it := item.Value.(*lruItem) if !it.IsExpired(nil) { c.evictList.MoveToFront(item) v := it.value c.mu.Unlock() if !onLoad { c.stats.IncrHitCount() } return v, nil } c.removeElement(item) } c.mu.Unlock() if !onLoad { c.stats.IncrMissCount() } return nil, KeyNotFoundError} load中使用了LoaderExpireFunc获取value和expire getWithLoader在其后设置了kv和过期时间 过期的key是在被访问时判断是否过期LRU的具体实现参考之前实现的算法题： https://peigongdh.github.io/posts/lru-cache-146/" }, { "title": "nsq延时队列 笔记2", "url": "/posts/nsq-defer-note2/", "categories": "cs", "tags": "mq, nsq, defer", "date": "2021-11-30 00:00:00 +0800", "snippet": "源码 https://github.com/wangcn/nsq.git实践启动make编译后，在build下逐个启动nsq相关进程// 启动nsqlookup./nsqlookupd// 新建nsqd数据目录mkdir /tmp/nsqdata1 /tmp/nsqdata2// 启动2个nsqd实例./nsqd --lookupd-tcp-address=127.0.0.1:4160 -broadcast-address=127.0.0.1 -tcp-address=127.0.0.1:4150 -http-address=0.0.0.0:4151 -data-path=/tmp/nsqdata1./nsqd --lookupd-tcp-address=127.0.0.1:4160 -broadcast-address=127.0.0.1 -tcp-address=127.0.0.1:4152 -http-address=0.0.0.0:4153 -data-path=/tmp/nsqdata2// 启动nsqadmin./nsqadmin --lookupd-http-address=localhost:4161无延迟消息场景新建一个topic：test-normal新建一个该topic的channel：sensor01可以观察到/tmp/nsqdata1目录下的nsqd.dat的meta信息如下：{ &quot;topics&quot;:[ { &quot;channels&quot;:[ { &quot;name&quot;:&quot;sensor01&quot;, &quot;paused&quot;:false } ], &quot;name&quot;:&quot;test-normal&quot;, &quot;paused&quot;:false } ], &quot;version&quot;:&quot;1.2.1-alpha&quot;}使用ctrl+c中断该nsqd进程，观察目录nsqdata1 lltotal 24drwxr-xr-x 6 zhangpei wheel 192B Nov 30 14:59 .drwxrwxrwt 11 root wheel 352B Nov 30 10:57 ..drwxr-xr-x 3 zhangpei wheel 96B Nov 30 14:59 __deferQ-rw------- 1 zhangpei wheel 122B Nov 30 14:59 nsqd.dat-rw------- 1 zhangpei wheel 10B Nov 30 14:59 test-normal.diskqueue.meta.dat-rw------- 1 zhangpei wheel 10B Nov 30 14:59 test-normal:sensor01.diskqueue.meta.dat➜ nsqdata1 tail test-normal.diskqueue.meta.dat00,00,0➜ nsqdata1 tail test-normal:sensor01.diskqueue.meta.dat00,00,0➜ nsqdata1结合代码的加载逻辑，可知三行分别代表：&amp;amp;depth,&amp;amp;d.readFileNum, &amp;amp;d.readPos,&amp;amp;d.writeFileNum, &amp;amp;d.writePosTODO: 遗留问题：此处的test-normal/test-normal:sensor01的前缀是从何而来的？// retrieveMetaData initializes state from the filesystemfunc (d *backendQueue) retrieveMetaData() error { var f *os.File var err error fileName := d.metaDataFileName() f, err = os.OpenFile(fileName, os.O_RDONLY, 0600) if err != nil { return err } defer f.Close() var depth int64 _, err = fmt.Fscanf(f, &quot;%d\\n%d,%d\\n%d,%d\\n&quot;, &amp;amp;depth, &amp;amp;d.readFileNum, &amp;amp;d.readPos, &amp;amp;d.writeFileNum, &amp;amp;d.writePos) if err != nil { return err } atomic.StoreInt64(&amp;amp;d.depth, depth) d.nextReadFileNum = d.readFileNum d.nextReadPos = d.readPos d.writeBufPos = d.writePos return nil}进入__deferQ目录，观察：nsqdata1 cd __deferQ➜ __deferQ lltotal 0drwxr-xr-x 3 zhangpei wheel 96B Nov 30 14:59 .drwxr-xr-x 6 zhangpei wheel 192B Nov 30 14:59 ..-rw------- 1 zhangpei wheel 0B Nov 30 14:59 defer_queue.meta.dat➜ __deferQ tail defer_queue.meta.dat➜ __deferQ目前nsq中没有延迟相关的消息，所有的消息都已经推送完毕延迟消息场景发送延迟消息，但先不消费新建一个topic：test-defer新建一个该topic的channel：sensor-defer01观察如下：➜ nsqdata1 lltotal 24drwxr-xr-x 6 zhangpei wheel 192B Nov 30 16:01 .drwxrwxrwt 11 root wheel 352B Nov 30 10:57 ..drwxr-xr-x 6 zhangpei wheel 192B Nov 30 16:01 __deferQ-rw------- 1 zhangpei wheel 213B Nov 30 16:01 nsqd.dat-rw------- 1 zhangpei wheel 10B Nov 30 14:59 test-normal.diskqueue.meta.dat-rw------- 1 zhangpei wheel 10B Nov 30 14:59 test-normal:sensor01.diskqueue.meta.dat➜ nsqdata1 tail nsqd.dat{&quot;topics&quot;:[{&quot;channels&quot;:[{&quot;name&quot;:&quot;sensor01&quot;,&quot;paused&quot;:false}],&quot;name&quot;:&quot;test-normal&quot;,&quot;paused&quot;:false},{&quot;channels&quot;:[{&quot;name&quot;:&quot;sensor-defer01&quot;,&quot;paused&quot;:false}],&quot;name&quot;:&quot;test-defer&quot;,&quot;paused&quot;:false}],&quot;version&quot;:&quot;1.2.1-alpha&quot;}% ➜ nsqdata1 ll __deferQtotal 40drwxr-xr-x 7 zhangpei wheel 224B Nov 30 16:02 .drwxr-xr-x 8 zhangpei wheel 256B Nov 30 16:02 ..-rw------- 1 zhangpei wheel 16B Nov 30 16:02 1638259200.delivery_index.dat-rw------- 1 zhangpei wheel 2B Nov 30 16:02 1638259200.delivery_index.meta.dat-rw------- 1 zhangpei wheel 66B Nov 30 16:01 1638259200.diskqueue.000000.dat-rw------- 1 zhangpei wheel 12B Nov 30 16:01 1638259200.diskqueue.meta.dat-rw------- 1 zhangpei wheel 11B Nov 30 16:01 defer_queue.meta.dattest-normal.diskqueue.meta.dat没有变化test-normal:sensor01.diskqueue.meta.dat没有变化ctrl+c中断nsqd1的进程，观察目录：➜ nsqdata1 lltotal 56drwxr-xr-x 10 zhangpei wheel 320B Nov 30 16:06 .drwxrwxrwt 11 root wheel 352B Nov 30 16:04 ..drwxr-xr-x 7 zhangpei wheel 224B Nov 30 16:06 __deferQ-rw------- 1 zhangpei wheel 213B Nov 30 16:06 nsqd.dat-rw------- 1 zhangpei wheel 32B Nov 30 16:02 test-defer.diskqueue.000000.dat-rw------- 1 zhangpei wheel 12B Nov 30 16:06 test-defer.diskqueue.meta.dat-rw------- 1 zhangpei wheel 32B Nov 30 16:06 test-defer:sensor-defer01.diskqueue.000000.dat-rw------- 1 zhangpei wheel 11B Nov 30 16:06 test-defer:sensor-defer01.diskqueue.meta.dat-rw------- 1 zhangpei wheel 10B Nov 30 16:06 test-normal.diskqueue.meta.dat-rw------- 1 zhangpei wheel 10B Nov 30 16:06 test-normal:sensor01.diskqueue.meta.dat➜ nsqdata1 ll __deferQtotal 40drwxr-xr-x 7 zhangpei wheel 224B Nov 30 16:06 .drwxr-xr-x 10 zhangpei wheel 320B Nov 30 16:06 ..-rw------- 1 zhangpei wheel 16B Nov 30 16:02 1638259200.delivery_index.dat-rw------- 1 zhangpei wheel 2B Nov 30 16:06 1638259200.delivery_index.meta.dat-rw------- 1 zhangpei wheel 66B Nov 30 16:01 1638259200.diskqueue.000000.dat-rw------- 1 zhangpei wheel 12B Nov 30 16:06 1638259200.diskqueue.meta.dat-rw------- 1 zhangpei wheel 11B Nov 30 16:06 defer_queue.meta.dat可以观察到，__deferQ中的内容没有变化，但是/tmp/nsqdata1中新增了相关带有延时队列的相关信息：nsqdata1 tail test-defer:sensor-defer01.diskqueue.meta.dat10,00,32➜ nsqdata1 tail test-defer:sensor-defer01.diskqueue.000000.dat�D :�:80febed4056c57000xx% ➜ nsqdata1 tail test-defer.diskqueue.meta.dat00,320,32➜ nsqdata1 tail test-defer.diskqueue.000000.dat�D :�:80febed4056c57000xx%因为存在延时消息的堆积，channel中的readPos和topic中的readPos不一致继续观察__deferQ中的内容：tail defer_queue.meta.dat1638259200➜ __deferQ tail 1638259200.diskqueue.meta.dat10,660,66➜ __deferQ tail 1638259200.diskqueue.000000.dat&amp;gt;��1�0febed4056c57000�2�xx�3��D :�:8�4�test-defer�5�Hv�% ➜ __deferQ tail 1638259200.delivery_index.dat0febed4056c57000% ➜ __deferQ tail 1638259200.delivery_index.meta.dat16%其中defer_queue.meta.dat中存放了时间轮，按行分隔 1638259200.diskqueue.meta.dat对应时间轮中的消息TODO: 分析剩余文件的含义TODO: 对比研究原版nsq的持久化逻辑" }, { "title": "nsq延时队列 笔记", "url": "/posts/nsq-defer-note/", "categories": "cs", "tags": "mq, nsq, defer", "date": "2021-11-22 00:00:00 +0800", "snippet": "源码 https://github.com/wangcn/nsq.git磁盘加载我们顺着磁盘加载的逻辑去阅读源码：func (h *deferQueue) load() { fileName := h.metaDataFileName() h.pool.Load(fileName) h.tw = NewTimeWheel(time.Second, h.timeSeg, h.logf) h.tw.RegCallback(h.twCallback) h.tw.Start()}func (h *deferQueue) metaDataFileName() string { return path.Join(h.dataPath, h.subPath, &quot;defer_queue.meta.dat&quot;)}可以观察到，延迟度列加载了固定的延迟信息文件：固定文件名为：defer_queue.meta.dat（后面可知该文件在data/__deferQ/目录下）func (h *deferBackendPool) Load(fileName string) { var f *os.File var err error var line string f, err = os.OpenFile(fileName, os.O_RDONLY, 0600) if err != nil &amp;amp;&amp;amp; !os.IsNotExist(err) { panic(err) } if os.IsNotExist(err) { return } defer f.Close() r := bufio.NewReader(f) for { line, err = r.ReadString(&#39;\\n&#39;) if errors.Is(err, io.EOF) { break } if err != nil { panic(err) } name := strings.TrimSpace(line) startPoint, _ := strconv.ParseInt(name, 10, 64) h.Create(startPoint, h.logf) }}按行读取该文件，根据时间戳创建延迟队列func (h *deferBackendPool) Create(startTs int64, logf AppLogFunc) BackendInterface { h.Lock() defer h.Unlock() if h.linkList.Len() == 0 { h.linkList.PushBack(startTs) } else if startTs &amp;lt; h.linkList.Front().Value.(int64) { h.linkList.PushFront(startTs) } else { for e := h.linkList.Front(); e != nil; e = e.Next() { ts := e.Value.(int64) if startTs &amp;gt; ts &amp;amp;&amp;amp; (e.Next() == nil || startTs &amp;lt; e.Next().Value.(int64)) { h.linkList.InsertAfter(startTs, e) break } } } q := h.newDiskQueue(startTs, logf) h.logf(INFO, &quot;create backend %d&quot;, startTs) h.data[startTs] = q return q}前半部分是根据时间戳排序链表后半部分是根据时间戳创建磁盘队列，磁盘队列中存放的是消息type deferBackendPool struct { data map[int64]BackendInterface linkList *list.List dataPath string subPath string logf AppLogFunc sync.RWMutex}回看deferBackendPool的定义list中存放的是时间戳的有序列表，每个时间戳都有对应的datadata中存放的是根据时间戳对应的磁盘队列func (h *deferBackendPool) newDiskQueue(startTs int64, logf AppLogFunc) BackendInterface { name := strconv.FormatInt(startTs, 10) return NewBackend( nil, name, path.Join(h.dataPath, h.subPath), Size100GB, SizeMinMsg, SizeMaxMsg, -1, time.Second, logf, )}回到newDiskQeue的逻辑来看，这里将map中时间戳的key指向的队列返回了一个BackendInterface{}// New instantiates an instance of diskQueue, retrieving metadata// from the filesystem and starting the read ahead goroutinefunc NewBackend(deliverChan chan []byte, name string, dataPath string, maxBytesPerFile int64, minMsgSize uint32, maxMsgSize uint32, syncEvery int64, syncTimeout time.Duration, logf AppLogFunc) BackendInterface { d := backendQueue{ name: name, dataPath: dataPath, maxBytesPerFile: maxBytesPerFile, minMsgSize: minMsgSize, maxMsgSize: maxMsgSize, readChan: deliverChan, writeChan: make(chan []byte), writeResponseChan: make(chan error), emptyChan: make(chan int), emptyResponseChan: make(chan error), exitChan: make(chan int), exitSyncChan: make(chan int), checkFinishChan: make(chan struct{}), checkFinishRespChan: make(chan bool), syncEvery: syncEvery, syncTimeout: syncTimeout, logf: logf, bytes4: make([]byte, 4), } // no need to lock here, nothing else could possibly be touching this instance err := d.retrieveMetaData() if err != nil &amp;amp;&amp;amp; !os.IsNotExist(err) { d.logf(ERROR, &quot;BACKENDQUEUE(%s) failed to retrieveMetaData - %s&quot;, d.name, err) } // TODO: could be optimized d.readFileNum = 0 d.readPos = 0 d.nextReadPos = 0 go d.ioLoop() return &amp;amp;d}这里面需要思考，在从磁盘加载的过程中，一共有多少个文件被载入？path.Join(h.dataPath, h.subPath)此处subPath为固定的路径__deferQ，也就是说我们的磁盘队列放在/data/__deferQ的目录下nsqd启动在没有延时消息的情况下，nsqd的加载中会依赖持久化文件nsqd.dat，这里面存放了nsqd中topic和channels等相关信息：type meta struct { Topics []struct { Name string `json:&quot;name&quot;` Paused bool `json:&quot;paused&quot;` Channels []struct { Name string `json:&quot;name&quot;` Paused bool `json:&quot;paused&quot;` } `json:&quot;channels&quot;` } `json:&quot;topics&quot;`}func (n *NSQD) LoadMetadata() error { atomic.StoreInt32(&amp;amp;n.isLoading, 1) defer atomic.StoreInt32(&amp;amp;n.isLoading, 0) fn := newMetadataFile(n.getOpts()) data, err := readOrEmpty(fn) if err != nil { return err } if data == nil { n.startDeferQueue() return nil // fresh start } var m meta err = json.Unmarshal(data, &amp;amp;m) if err != nil { return fmt.Errorf(&quot;failed to parse metadata in %s - %s&quot;, fn, err) } // ... return nil}而后LoadMetadata会实例化topic和channel对象，并运行起来func (n *NSQD) LoadMetadata() error { // ... for _, t := range m.Topics { if !protocol.IsValidTopicName(t.Name) { n.logf(LOG_WARN, &quot;skipping creation of invalid topic %s&quot;, t.Name) continue } topic := n.GetTopic(t.Name) if t.Paused { topic.Pause() } for _, c := range t.Channels { if !protocol.IsValidChannelName(c.Name) { n.logf(LOG_WARN, &quot;skipping creation of invalid channel %s&quot;, c.Name) continue } channel := topic.GetChannel(c.Name) if c.Paused { channel.Pause() } } topic.Start() } n.startDeferQueue()}// GetTopic performs a thread safe operation// to return a pointer to a Topic object (potentially new)func (n *NSQD) GetTopic(topicName string) *Topic { // most likely, we already have this topic, so try read lock first. n.RLock() t, ok := n.topicMap[topicName] n.RUnlock() if ok { return t } n.Lock() t, ok = n.topicMap[topicName] if ok { n.Unlock() return t } deleteCallback := func(t *Topic) { n.DeleteExistingTopic(t.name) } t = NewTopic(topicName, n, deleteCallback) n.topicMap[topicName] = t n.deferQueue.RegDownStream(topicName, t.backend) n.Unlock() n.logf(LOG_INFO, &quot;TOPIC(%s): created&quot;, t.name) // topic is created but messagePump not yet started // if loading metadata at startup, no lookupd connections yet, topic started after load if atomic.LoadInt32(&amp;amp;n.isLoading) == 1 { return t } // if using lookupd, make a blocking call to get the topics, and immediately create them. // this makes sure that any message received is buffered to the right channels lookupdHTTPAddrs := n.lookupdHTTPAddrs() if len(lookupdHTTPAddrs) &amp;gt; 0 { channelNames, err := n.ci.GetLookupdTopicChannels(t.name, lookupdHTTPAddrs) if err != nil { n.logf(LOG_WARN, &quot;failed to query nsqlookupd for channels to pre-create for topic %s - %s&quot;, t.name, err) } for _, channelName := range channelNames { if strings.HasSuffix(channelName, &quot;#ephemeral&quot;) { continue // do not create ephemeral channel with no consumer client } t.GetChannel(channelName) } } else if len(n.getOpts().NSQLookupdTCPAddresses) &amp;gt; 0 { n.logf(LOG_ERROR, &quot;no available nsqlookupd to query for channels to pre-create for topic %s&quot;, t.name) } // now that all channels are added, start topic messagePump t.Start() return t}// GetChannel performs a thread safe operation// to return a pointer to a Channel object (potentially new)// for the given Topicfunc (t *Topic) GetChannel(channelName string) *Channel { t.Lock() channel, isNew := t.getOrCreateChannel(channelName) t.Unlock() if isNew { // update messagePump state select { case t.channelUpdateChan &amp;lt;- 1: case &amp;lt;-t.exitChan: } } return channel}func (t *Topic) Start() { select { case t.startChan &amp;lt;- 1: default: }}" }, { "title": "一致性概念 笔记", "url": "/posts/consistency-note/", "categories": "cs", "tags": "cap, acid", "date": "2021-11-22 00:00:00 +0800", "snippet": "一致性的概念 https://www.zhihu.com/question/275845393/answer/419372793提问： 请问分布式事务一致性与raft或paxos协议解决的一致性问题是同一回事吗？ 分布式事务一致性，一般列举的方案是：两阶段提交，tcc，或者消息队列等方法； 而raft或者paxos等解决多副本日志一致的问题； 请问下这两个的场景是否不一样，或者能否以raft等协议，解决分布式事务一致性的问题？解答： paxos/raft说的是我要在A,B,C上同时做a，保证做完后A,B,C仍然是一致的（至少超过半数达成了共识），一般的实现是在主节点上实施，然后复制到其它节点； 分布式事务说的是我要在A机器（或集群）上做a，B机器上做b，C机器上做c，三个操作合在一起还要形成事务，注意这时候没有任何一台机器把a,b,c都运行了一遍来保证事务，而是分布式运行、通过协议协同。" }, { "title": "clone-graph-133", "url": "/posts/clone-graph-133/", "categories": "cs, algorithm", "tags": "graph, bfs, dfs", "date": "2021-11-21 00:00:00 +0800", "snippet": "题目Given a reference of a node in a connected undirected graph.Return a deep copy (clone) of the graph.Each node in the graph contains a value (int) and a list (List[Node]) of its neighbors.class Node { public int val; public List neighbors; }Test case format:For simplicity, each node’s value is the same as the node’s index (1-indexed). For example, the first node with val == 1, the second node with val == 2, and so on. The graph is represented in the test case using an adjacency list.An adjacency list is a collection of unordered lists used to represent a finite graph. Each list describes the set of neighbors of a node in the graph.The given node will always be the first node with val = 1. You must return the copy of the given node as a reference to the cloned graph.思路典型的BFS或者DFS题型需要使用map保存遍历过的节点使用BFS+递归的写法较为简单算法func cloneGraph(node *Node) *Node { // old *Node -&amp;gt; new *Node nodeSet := make(map[*Node]*Node) return doCloneGraph(node, nodeSet)}// cloneGraph (BFS)func doCloneGraph(node *Node, nodeSet map[*Node]*Node) *Node { if node == nil { return nil } if n, ok := nodeSet[node]; ok { return n } newNode := &amp;amp;Node{ Val: node.Val, Neighbors: make([]*Node, 0), } nodeSet[node] = newNode for _, v := range node.Neighbors { newNode.Neighbors = append(newNode.Neighbors, doCloneGraph(v, nodeSet)) } return newNode}" }, { "title": "number-of-islands-200", "url": "/posts/number-of-islands-200/", "categories": "cs, algorithm", "tags": "dfs, bfs", "date": "2021-11-18 00:00:00 +0800", "snippet": "题目 https://leetcode.com/problems/number-of-islandsGiven an m x n 2D binary grid grid which represents a map of ‘1’s (land) and ‘0’s (water), return the number of islands.An island is surrounded by water and is formed by connecting adjacent lands horizontally or vertically. You may assume all four edges of the grid are all surrounded by water.Example 1:Input: grid = [ [“1”,”1”,”1”,”1”,”0”], [“1”,”1”,”0”,”1”,”0”], [“1”,”1”,”0”,”0”,”0”], [“0”,”0”,”0”,”0”,”0”]]Output: 1Example 2:Input: grid = [ [“1”,”1”,”0”,”0”,”0”], [“1”,”1”,”0”,”0”,”0”], [“0”,”0”,”1”,”0”,”0”], [“0”,”0”,”0”,”1”,”1”]]Output: 3思路尝试使用深度优先（DFS）的思路解题初始化相同的二维数组intGrid，使用-1标识没有访问过，用于数据暂存从grid初始节点开始，访问相邻节点（上下左右），如果为1则开始染色intGrid，如果为0，则跳过标识intGrid为0代表访问过则一次深度优先遍历下来，即可找到一个完整的island或者判断为无效的water如此即可找到整个island的数量算法func numIslands(grid [][]byte) int { intGrid := make([][]int, len(grid)) for i := 0; i &amp;lt; len(grid); i++ { intGrid[i] = make([]int, len(grid[i])) for j := 0; j &amp;lt; len(grid[i]); j++ { intGrid[i][j] = -1 } } var num int for i := 0; i &amp;lt; len(grid); i++ { for j := 0; j &amp;lt; len(grid[i]); j++ { var isNewLand bool doNumIslands(grid, intGrid, i, j, num, &amp;amp;isNewLand) if isNewLand { num++ } // fmt.Println(i, &quot; &quot;, j, &quot; &quot;, isNewLand) // printGrid(intGrid) } } // printGrid(intGrid) return num}func doNumIslands(grid [][]byte, intGrid [][]int, i int, j int, num int, isNewLand *bool) { if intGrid[i][j] &amp;gt; -1 { return } if grid[i][j] == &#39;0&#39; { intGrid[i][j] = 0 return } if grid[i][j] == &#39;1&#39; { intGrid[i][j] = num + 1 *isNewLand = true } // try top, left, bottom, right topIndex := i - 1 if topIndex &amp;gt;= 0 { doNumIslands(grid, intGrid, topIndex, j, num, isNewLand) } leftIndex := j - 1 if leftIndex &amp;gt;= 0 { doNumIslands(grid, intGrid, i, leftIndex, num, isNewLand) } bottomIndex := i + 1 if bottomIndex &amp;lt; len(grid) { doNumIslands(grid, intGrid, bottomIndex, j, num, isNewLand) } rightIndex := j + 1 if rightIndex &amp;lt; len(grid[i]) { doNumIslands(grid, intGrid, i, rightIndex, num, isNewLand) } return}func printGrid(intGrid [][]int) { for i := 0; i &amp;lt; len(intGrid); i++ { for j := 0; j &amp;lt; len(intGrid[i]); j++ { print(intGrid[i][j], &quot; &quot;) } println(&quot;&quot;) }}注意// 0 0 true// 1 1 0 -1 -1// 1 1 0 -1 -1// 0 0 -1 -1 -1// -1 -1 -1 -1 -1// 2 2 true// 1 1 0 0 0// 1 1 0 0 0// 0 0 2 0 -1// -1 -1 0 -1 -1// 3 3 true// 1 1 0 0 0// 1 1 0 0 0// 0 0 2 0 0// 0 0 0 3 3使用num染色每次深度优先遍历为island的所有节点以上为3次判定island=true时intGrid的打印" }, { "title": "insert-delete-getrandom-o1-380", "url": "/posts/insert-delete-getrandom-o1-380/", "categories": "cs, algorithm", "tags": "map", "date": "2021-11-05 00:00:00 +0800", "snippet": "题目Implement the RandomizedSet class:RandomizedSet() Initializes the RandomizedSet object.bool insert(int val) Inserts an item val into the set if not present. Returns true if the item was not present, false otherwise.bool remove(int val) Removes an item val from the set if present. Returns true if the item was present, false otherwise.int getRandom() Returns a random element from the current set of elements (it’s guaranteed that at least one element exists when this method is called). Each element must have the same probability of being returned.You must implement the functions of the class such that each function works in average O(1) time complexity.Example 1:Input[“RandomizedSet”, “insert”, “remove”, “insert”, “getRandom”, “remove”, “insert”, “getRandom”][[], [1], [2], [2], [], [1], [2], []]Output[null, true, false, true, 2, true, false, 2]思路很显然我们应该使用map来达到O(1)的时间复杂度，但是go并不提供map.random的方法要达到getRandom()为O(1)的目标，我们必须冗余一层关系考虑额外使用一个list保存所有的values，再使用map保存value-&amp;gt;index的关系，index为list的下标这样，我们就可以在slice中使用O(1)的时间随机一个元素算法实现的关键在于，如何在insert和remove中同步map和list关键在于remove的时候，需要将list中的该元素与列表最后一个元素交换位置（或者说使用最后一个元素覆盖），这样不会影响list中其他元素的index最后更新map中刚才list所交换最后一个元素的index，即可完成算法type RandomizedSet struct { // value list values []int // value -&amp;gt; index indexMap map[int]int}func Constructor() RandomizedSet { randomizedSet := RandomizedSet{ values: make([]int, 0), indexMap: make(map[int]int), } return randomizedSet}func (this *RandomizedSet) Insert(val int) bool { _, ok := this.indexMap[val] if ok { return false } else { this.values = append(this.values, val) this.indexMap[val] = len(this.values) - 1 return true }}func (this *RandomizedSet) Remove(val int) bool { index, ok := this.indexMap[val] if !ok { return false } // replace index use tail this.values[index] = this.values[len(this.values)-1] this.indexMap[this.values[index]] = index // delete index delete(this.indexMap, val) this.values = this.values[:len(this.values)-1] return true}func (this *RandomizedSet) GetRandom() int { rand.Seed(time.Now().UnixNano()) randomIndex := rand.Intn(len(this.values)) return this.values[randomIndex]}注意在实现Remove时，不能先delete(this.indexMap, val)，需要在list交换和map更新后再delete// replace index use tailthis.values[index] = this.values[len(this.values)-1]this.indexMap[this.values[index]] = index// delete indexdelete(this.indexMap, val)this.values = this.values[:len(this.values)-1]" }, { "title": "lru-cache-146", "url": "/posts/lru-cache-146/", "categories": "cs, algorithm", "tags": "lru, cache, map, queue, linklist", "date": "2021-11-04 00:00:00 +0800", "snippet": "题目 https://leetcode.com/problems/lru-cacheDesign a data structure that follows the constraints of a Least Recently Used (LRU) cache.Implement the LRUCache class:LRUCache(int capacity) Initialize the LRU cache with positive size capacity.int get(int key) Return the value of the key if the key exists, otherwise return -1.void put(int key, int value) Update the value of the key if the key exists. Otherwise, add the key-value pair to the cache. If the number of keys exceeds the capacity from this operation, evict the least recently used key.The functions get and put must each run in O(1) average time complexity.Example 1:Input[“LRUCache”, “put”, “put”, “get”, “put”, “get”, “put”, “get”, “get”, “get”][[2], [1, 1], [2, 2], [1], [3, 3], [2], [4, 4], [1], [3], [4]]Output[null, null, null, 1, null, -1, null, -1, 3, 4]思路LRU的字面意思Least Recently Used，最近最少使用算法的思路很简单，将最近访问和插入的节点放在队尾，队首就是最近最少使用的节点了，可以根据容量淘汰这里应该使用双向链表来维护这个队列，因为涉及到首尾队列的删除和插入使用哈希表维护键和链表节点的关系，在哈希表的值中需要额外维护键名，便于淘汰时删除因为删除的逻辑中，是直接删除链表的首节点，以及首节点在哈希表中的对应的键值此时若没有在哈希表的value中维护key，则仅有value没有key，会非常难受，所以这里也是实现中的一处关键所在算法type LRUCache struct { linkList *list.List elementMap map[int]*list.Element capacity int}type entry struct { key int value interface{}}func Constructor(capacity int) LRUCache { cache := LRUCache{ linkList: list.New(), elementMap: make(map[int]*list.Element), capacity: capacity, } return cache}func (this *LRUCache) Get(key int) int { var value int element, ok := this.elementMap[key] if ok { this.linkList.MoveToBack(element) value = element.Value.(*entry).value.(int) } else { value = -1 } return value}func (this *LRUCache) Put(key int, value int) { element, ok := this.elementMap[key] if ok { // do update newValue := &amp;amp;entry{ key: key, value: value, } element.Value = newValue this.linkList.MoveToBack(element) } else { // delete front if this.linkList.Len() &amp;gt;= this.capacity { frontElement := this.linkList.Front() this.linkList.Remove(frontElement) delete(this.elementMap, frontElement.Value.(*entry).key) } // do insert newValue := &amp;amp;entry{ key: key, value: value, } this.linkList.PushBack(newValue) this.elementMap[key] = this.linkList.Back() }}注意container/list包在使用时PushBack插入的是list.Element.Value，而不是list.Element，需要注意" }, { "title": "evaluate-reverse-polish-notation-150", "url": "/posts/evaluate-reverse-polish-notation-150/", "categories": "cs, algorithm", "tags": "stack", "date": "2021-11-03 00:00:00 +0800", "snippet": "题目 https://leetcode.com/problems/evaluate-reverse-polish-notation/Evaluate the value of an arithmetic expression in Reverse Polish Notation.Valid operators are +, -, *, and /. Each operand may be an integer or another expression.Note that division between two integers should truncate toward zero.It is guaranteed that the given RPN expression is always valid. That means the expression would always evaluate to a result, and there will not be any division by zero operation.Example 1:Input: tokens = [“2”,”1”,”+”,”3”,”*”]Output: 9Explanation: ((2 + 1) * 3) = 9思路在解这道题的时候思路要自然，这道题属于stack标签下，尝试使用stack来解决顺序遍历tokens，将数字依次入栈，遇到符号时连续出栈2个数字，按照运算符做二元运算后，将计算结果再次入栈依次类推，遍历结束后，堆栈应只有一个元素即计算结果算法func evalRPN(tokens []string) int { var numberStack util.Stack numberStack = util.NewCustomizeStack() for _, v := range tokens { if isOp(v) { var result int n2 := numberStack.Pop().(int) n1 := numberStack.Pop().(int) switch v { case &quot;+&quot;: result = n1 + n2 case &quot;-&quot;: result = n1 - n2 case &quot;*&quot;: result = n1 * n2 case &quot;/&quot;: result = n1 / n2 } numberStack.Push(result) } else { number, _ := strconv.Atoi(v) numberStack.Push(number) } } return numberStack.Pop().(int)}func isOp(token string) bool { return token == &quot;+&quot; || token == &quot;-&quot; || token == &quot;*&quot; || token == &quot;/&quot;}" }, { "title": "implement-queue-using-stack-232", "url": "/posts/implement-queue-using-stack-232/", "categories": "cs, algorithm", "tags": "stack, queue", "date": "2021-11-01 00:00:00 +0800", "snippet": "题目 https://leetcode.com/problems/implement-queue-using-stacks/Implement a first in first out (FIFO) queue using only two stacks. The implemented queue should support all the functions of a normal queue (push, peek, pop, and empty).Implement the MyQueue class:void push(int x) Pushes element x to the back of the queue.int pop() Removes the element from the front of the queue and returns it.int peek() Returns the element at the front of the queue.boolean empty() Returns true if the queue is empty, false otherwise.思路看到这道题的第一反应是同类型的implement-stack-using-queue，于是尝试抄袭那道题的算法来写，结果翻车这里一定不能陷入相似题型解法也相似误区，应该从底层分析栈与队列的区别经过思考可以发现，队列就是入栈顺序，我们只需要用一个栈A来入队，在取数据的时候用另一个辅助栈B把栈A的数据全部翻转过来就是原来的序列了比较精巧一点的改进是，我们用A,B两个栈分别维护入队和出队的容器，只在栈B没有数据的情况下从A中倒腾数据过来算法type MyQueue struct { stack1 util.Stack stack2 util.Stack}func Constructor() MyQueue { myQueue := MyQueue{ stack1: util.NewCustomizeStack(), stack2: util.NewCustomizeStack(), } return myQueue}func (this *MyQueue) Push(x int) { this.stack1.Push(x)}func (this *MyQueue) Pop() int { if this.Empty() { return 0 } if this.stack2.Len() == 0 { for { if this.stack1.Len() == 0 { break } this.stack2.Push(this.stack1.Pop()) } } item := this.stack2.Pop() return item.(int)}func (this *MyQueue) Peek() int { if this.Empty() { return 0 } if this.stack2.Len() == 0 { for { if this.stack1.Len() == 0 { break } this.stack2.Push(this.stack1.Pop()) } } item := this.stack2.Peek() return item.(int)}func (this *MyQueue) Empty() bool { return this.stack1.Len() == 0 &amp;amp;&amp;amp; this.stack2.Len() == 0}" }, { "title": "min-stack-155", "url": "/posts/min-stack-155/", "categories": "cs, algorithm", "tags": "stack", "date": "2021-10-29 00:00:00 +0800", "snippet": "题目 https://leetcode.com/problems/min-stack/Design a stack that supports push, pop, top, and retrieving the minimum element in constant time.Implement the MinStack class:MinStack() initializes the stack object.void push(int val) pushes the element val onto the stack.void pop() removes the element on the top of the stack.int top() gets the top element of the stack.int getMin() retrieves the minimum element in the stack.思路可以一眼看出关键是要O(1)的GetMiN()第一次刷到这个题，没找到思路，使用最笨的方法： 每次push更新最小值 每次pop遍历剩下的stack找到最小值这个思路确实太简单的，看了一眼别的答案，抄袭思路： 我们并不需要维持一个有序的的栈元素，只需要能够找到最小值 因为每次push/pop操作都不是随机寻址，都是固定栈顶元素的变化，我们可以维持一个同步的stack，姑且叫他minStack 该minStack和stack同步push/pop操作，不同点是，push插入的不是当前值，而是当前值和minStack栈顶中的较小值 这样minStack就维护了一个和stack大小相同的栈，栈里面的每个元素都对应stack在该长度下的最小值算法type MinStack struct { items []int // minList keep min list sync items pop/push minList []int}func Constructor() MinStack { return MinStack{ items: make([]int, 0), minList: make([]int, 0), }}func (this *MinStack) Push(val int) { if len(this.items) == 0 { this.minList = append(this.minList, val) } else if val &amp;lt; this.GetMin() { this.minList = append(this.minList, val) } else { this.minList = append(this.minList, this.GetMin()) } this.items = append(this.items, val)}func (this *MinStack) Pop() { if len(this.items) == 0 { return } this.items = this.items[:len(this.items)-1] this.minList = this.minList[:len(this.minList)-1]}func (this *MinStack) Top() int { if len(this.items) == 0 { return 0 } return this.items[len(this.items)-1]}func (this *MinStack) GetMin() int { return this.minList[len(this.minList)-1]}" }, { "title": "maximum-length-of-repeated-subarray-718", "url": "/posts/maximum-length-of-repeated-subarray-718/", "categories": "cs, algorithm", "tags": "dp, lcs", "date": "2021-10-25 00:00:00 +0800", "snippet": "题目 https://leetcode.com/problems/maximum-length-of-repeated-subarrayGiven two integer arrays nums1 and nums2, return the maximum length of a subarray that appears in both arrays.Example 1:Input: nums1 = [1,2,3,2,1], nums2 = [3,2,1,4,7]Output: 3Explanation: The repeated subarray with maximum length is [3,2,1].Example 2:Input: nums1 = [0,0,0,0,0], nums2 = [0,0,0,0,0]Output: 5思路感慨一下，上次做动态规划的题还是上学竞赛时候的事，时光荏苒考虑将两个数组合并为一个二维数组grid[][]，行表示nums1，列表示nums2当nums1[i]和nums[j]相等时，在对应的grid[i][j]上标注1这样，在这个网格中，找到最大的值为1的斜线长度即为最大子串的长度，例如： ij 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0可以进一步优化，在grid[i][j]中，将数字1优化为当前的长度，这样就不用在构造grid同时记录最大子串的长度： ij 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 2 0 0 0 0 0 3 0 0算法func longestCommonSubsequence(text1 string, text2 string) int { maxLen := -1 grid := make([][]int, len(text1)) for i := 0; i &amp;lt; len(grid); i++ { grid[i] = make([]int, len(text2)) for j := 0; j &amp;lt; len(grid[i]); j++ { if text1[i] != text2[j] { grid[i][j] = 0 continue } // text1[i] == text2[j] if i == 0 || j == 0 { grid[i][j] = 1 } else { grid[i][j] = 1 + grid[i-1][j-1] } if maxLen &amp;lt; grid[i][j] { maxLen = grid[i][j] } } } return maxLen}" }, { "title": "implement-stack-using-queues-225", "url": "/posts/implement-stack-using-queues-225/", "categories": "cs, algorithm", "tags": "queue, stack", "date": "2021-10-21 00:00:00 +0800", "snippet": "题目 https://leetcode.com/problems/implement-stack-using-queues/Implement a last-in-first-out (LIFO) stack using only two queues. The implemented stack should support all the functions of a normal stack (push, top, pop, and empty).Implement the MyStack class:思路使用双队列模拟栈，主要的思路是将队列逆序，即为栈使用Q1，Q2两个队列，前者作为存储，后者作为辅助考虑场景Q1中存在A,B,C,D当一个新元素E入栈时，首先入栈Q2然后将Q1全部出栈，然后依次入栈Q2，此时Q2为E,A,B,C,D即，每次入栈的元素都被插入到了队列的头部，即可完成栈LIFO的特性算法type MyStack struct { Q1 *Queue Q2 *Queue}func Constructor() MyStack { return MyStack{ Q1: NewQueue(), Q2: NewQueue(), }}func (this *MyStack) Push(x int) { this.Q2.Push(x) for { if this.Q1.Empty() { break } e := this.Q1.Pop() this.Q2.Push(e) } q1 := this.Q1 this.Q1 = this.Q2 this.Q2 = q1}func (this *MyStack) Pop() int { return this.Q1.Pop().(int)}func (this *MyStack) Top() int { return this.Q1.Top().(int)}func (this *MyStack) Empty() bool { return this.Q1.Empty()}type Queue []interface{}func (self *Queue) Push(x interface{}) { *self = append(*self, x)}func (self *Queue) Pop() interface{} { h := *self var el interface{} l := len(h) el, *self = h[0], h[1:l] // Or use this instead for a Stack // el, *self = h[l-1], h[0:l-1] return el}func (self *Queue) Top() interface{} { return (*self)[0]}func (self *Queue) Empty() bool { return len(*self) == 0}func NewQueue() *Queue { return &amp;amp;Queue{}}" }, { "title": "reverse-linked-list-206", "url": "/posts/reverse-linked-list-206/", "categories": "cs, algorithm", "tags": "reverse", "date": "2021-10-18 00:00:00 +0800", "snippet": "题目 https://leetcode.com/problems/reverse-linked-listGiven the head of a singly linked list, reverse the list, and return the reversed list.思路逆序链表，关键是需要保存3个节点，分别是逆序的当前两个节点，还有一个后续节点若不保存后续节点，则会丢失head，无法找到下一个节点参考一个比较形象的解释：https://zhuanlan.zhihu.com/p/56870189算法func reverseList(head *ListNode) *ListNode { if head == nil { return head } var reverseHead *ListNode var p1, p2, p3 *ListNode p1 = head p2 = head.Next for { if p2 == nil { break } p3 = p2.Next p2.Next = p1 p1 = p2 p2 = p3 } head.Next = nil reverseHead = p1 return reverseHead}" }, { "title": "middle-of-the-linked-list-876", "url": "/posts/middle-of-the-linked-list-876/", "categories": "cs, algorithm", "tags": "middle", "date": "2021-10-18 00:00:00 +0800", "snippet": "题目 https://leetcode.com/problems/middle-of-the-linked-listGiven the head of a singly linked list, reverse the list, and return the reversed list.Given the head of a singly linked list, return the middle node of the linked list.If there are two middle nodes, return the second middle node.Input: head = [1,2,3,4,5]Output: [3,4,5]Explanation: The middle node of the list is node 3.思路一个简单的思路，遍历一遍后获得长度，再遍历第二遍到1/2的长度即可获得中点这个思路太过于简单以至于不好意思写出来，考虑如何在一次遍历中定位到中点？两个指针，前者的步距保持为后者的一半，这样即可在后者达到尾部时，前者达到中点算法func middleNode(head *ListNode) *ListNode { var i, j int middleHead := head p := head for { if p == nil { break } i++ newJ := i / 2 if newJ &amp;gt; j { j = newJ middleHead = middleHead.Next } p = p.Next } return middleHead}" }, { "title": "kth-largest-element-in-an-array-215", "url": "/posts/kth-largest-element-in-an-array-215/", "categories": "cs, algorithm", "tags": "sort, quick-sort", "date": "2021-10-18 00:00:00 +0800", "snippet": "题目 https://leetcode.com/problems/kth-largest-element-in-an-array/Given an integer array nums and an integer k, return the kth largest element in the array.Note that it is the kth largest element in the sorted order, not the kth distinct element.Example 1:Input: nums = [3,2,1,5,6,4], k = 2Output: 5思路典型的快排快排中最重要的一步，每次找到一个数在整个序列中的绝对位置，再做二分如何一步定位一个数的绝对位置？经典快排的思路是，使用最后一个数作为锚点（也就是找到这个数在序列中的绝对位置），从前往后扫描快排算法的该部分简述如下，按照从大到小排序： 使用start, end下标定位序列的最前最后的index 使用指针p从前往后扫描 若当前值大于锚点值，则交换start, p的位置 若当前值小于锚点值，则忽略 交换最后一个值与start后面的一个值，完毕至此，序列被分割为[0, start], start+1, [start+2, end]3个部分，只需要在3个部分中继续重复即可找到kth算法func findKthLargest(nums []int, k int) int { if len(nums) == 0 { return 0 } if len(nums) == 1 { return nums[0] } start := 0 end := len(nums) - 1 p := 0 for { if p &amp;gt;= end { swap(nums, start, end) break } if nums[p] &amp;lt; nums[end] { // ignore } else if nums[p] &amp;gt;= nums[end] { swap(nums, start, p) start++ } p++ } if start+1 == k { return nums[start] } if start+1 &amp;lt; k { return findKthLargest(nums[start+1:], k-len(nums[:start+1])) } if start+1 &amp;gt; k { return findKthLargest(nums[:start], k) } return 0}func swap(nums []int, i, j int) { swapNum := nums[i] nums[i] = nums[j] nums[j] = swapNum}扩展考虑使用构造堆的方法解决该问题" }, { "title": "kth-largest-element-in-an-array-215-2", "url": "/posts/kth-largest-element-in-an-array-215-2/", "categories": "cs, algorithm", "tags": "sort, heap", "date": "2021-10-18 00:00:00 +0800", "snippet": "题目 https://leetcode.com/problems/kth-largest-element-in-an-array/Given an integer array nums and an integer k, return the kth largest element in the array.Note that it is the kth largest element in the sorted order, not the kth distinct element.Example 1:Input: nums = [3,2,1,5,6,4], k = 2Output: 5思路本次采用构造堆解决该问题，考虑使用最大堆还是最小堆？应该使用最小堆，而不是最大堆，此处是关键遍历序列，构造并维护一个长度为k的最小堆，保证堆内为最大的k个数，其中堆顶（最小值）即为kth算法func findKthLargest2(nums []int, k int) int { if len(nums) == 0 { return 0 } if len(nums) == 1 { return nums[0] } intHeap := &amp;amp;IntHeap{} heap.Init(intHeap) for i := 0; i &amp;lt; len(nums); i++ { if intHeap.Len() &amp;lt; k { heap.Push(intHeap, nums[i]) continue } if intHeap.Top() &amp;lt; nums[i] { heap.Push(intHeap, nums[i]) heap.Pop(intHeap) } } return intHeap.Top()}// An IntHeap is a min-heap of ints.type IntHeap []intfunc (h IntHeap) Len() int { return len(h) }func (h IntHeap) Less(i, j int) bool { return h[i] &amp;lt; h[j] }func (h IntHeap) Swap(i, j int) { h[i], h[j] = h[j], h[i] }func (h *IntHeap) Push(x interface{}) { // Push and Pop use pointer receivers because they modify the slice&#39;s length, // not just its contents. *h = append(*h, x.(int))}func (h *IntHeap) Pop() interface{} { old := *h n := len(old) x := old[n-1] *h = old[0 : n-1] return x}func (h *IntHeap) ToArray() []int { return *h}func (h *IntHeap) Top() int { return (*h)[0]}扩展堆的应用： 优先级队列，例如实现一个定时器：将后续时间放入小顶堆中，这样就不用每隔固定时间轮询，而是根据距离堆顶的最小值算出下次间隔时间执行 top-k：也就是本例中的算法 中位数：维护两个堆，一个大顶堆，一个小顶堆。大顶堆中存储前半部分数据，小顶堆中存储后半部分数据，且小顶堆中的数据都大于大顶堆中的数据 快速求接口的99%的响应时间： 维护两个堆，一个大顶堆，一个小顶堆。 假设当前总数据的个数是n，大顶堆中保存 n99% 个数据，小顶堆中保存 n1% 个数据。 大顶堆堆顶的数据就是我们要找的 99% 响应时间。 每次插入一个数据的时候，我们要判断这个数据跟大顶堆和小顶堆堆顶数据的大小关系，然后决定插入到哪个堆中。如果这个新插入的数据比大顶堆的堆顶数据小，那就插入大顶堆；如果这个新插入的数据比小顶堆的堆顶数据大，那就插入小顶堆。 但是，为了保持大顶堆中的数据占 99%，小顶堆中的数据占 1%，在每次新插入数据之后，我们都要重新计算，这个时候大顶堆和小顶堆中的数据个数，是否还符合 99:1 这个比例。如果不符合，我们就将一个堆中的数据移动到另一个堆，直到满足这个比例。移动的方法类似前面求中位数的方法。 https://driverzhang.github.io/post/go%E6%A0%87%E5%87%86%E5%BA%93%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E7%B3%BB%E5%88%97%E4%B9%8B%E5%A0%86heap/" }, { "title": "largest-number-179", "url": "/posts/largest-number-179/", "categories": "cs, algorithm", "tags": "sort", "date": "2021-10-12 00:00:00 +0800", "snippet": "题目 https://leetcode.com/problems/largest-number/submissions/Given a list of non-negative integers nums, arrange them such that they form the largest number.Note: The result may be very large, so you need to return a string instead of an integer.Example 1:Input: nums = [10,2]Output: “210”思路这是一道和排序相关的题，从排序思路上下手我们只需要对这个数组进行排序，保证“大”的数在前算法的关键就在于比较函数上，进一步思考如何比较简单思考：最高位的数越大，则该数应该越大当高位一样时，应该继续比较次一位的数，以此类推问题在于，当两个数的高位一样但长度不一样时，应该如何比较以[3, 31]为例，我们通过穷举得知331 &amp;gt; 313，即3应该大于31此时思路是，将比较的数字拆分，拆成同样大小的长度，即：[3, 31] -&amp;gt; [3, 3]，此时两者相等我们应该进一步比较原较短长度的数（也就是前者3）和后者截断前者长度后的数（也就是1）即：[3, 1]此时3 &amp;gt; 1，即认为3 &amp;gt; 31解法func largestNumber(nums []int) string { // fmt.Println(nums) sort.SliceStable(nums, func(i, j int) bool { return compareBigger(nums[i], nums[j]) }) // fmt.Println(nums) // NOTE-1: if nums == [0, 0], return &quot;0&quot; (not &quot;00&quot;) if len(nums) &amp;gt; 0 &amp;amp;&amp;amp; nums[0] == 0 { return &quot;0&quot; } var largest string for _, v := range nums { largest += strconv.Itoa(v) } return largest}func compareBigger(x, y int) bool { // fmt.Printf(&quot;x:%d, y:%d\\n&quot;, x, y) xNumList := toNumList(x) yNumList := toNumList(y) if len(xNumList) == len(yNumList) { return x &amp;gt; y } minLength := len(xNumList) if minLength &amp;gt; len(yNumList) { minLength = len(yNumList) } for i := 0; i &amp;lt; minLength; i++ { if xNumList[i] != yNumList[i] { return xNumList[i] &amp;gt; yNumList[i] } } if len(xNumList) &amp;gt; len(yNumList) { return compareBigger(toNum(xNumList[minLength:]), y) } else { return compareBigger(x, toNum(yNumList[minLength:])) }}func toNumList(num int) []int { var numList []int for { if num &amp;gt;= 10 { numList = append(numList, num%10) num /= 10 continue } numList = append(numList, num%10) break } var reverseNumList []int for i := len(numList) - 1; i &amp;gt;= 0; i-- { reverseNumList = append(reverseNumList, numList[i]) } return reverseNumList}func toNum(nums []int) int { var num int // NOTE-2: if nums start with 0, return 0 if len(nums) &amp;gt; 0 &amp;amp;&amp;amp; nums[0] == 0 { return 0 } for i := 0; i &amp;lt; len(nums); i++ { num = num*10 + nums[i] } return num}边界情况 零值，例如[0,0]应该返回0而不是00，这个在入口处理即可 截断后比较的数字中，存在零开头的数字，例如[2060, 2] [2060, 2] -&amp;gt; [060, 2]，此时注意，060应该设置为0而不是60" }, { "title": "优雅重启 分析2", "url": "/posts/gradefully-shutdown-2/", "categories": "cs", "tags": "gradefully-shotdown", "date": "2021-09-27 00:00:00 +0800", "snippet": "源码 https://github.com/peigongdh/hotstartBUG修复 https://github.com/peigongdh/hotstart/commit/c0a5de73693a08fe573a991e66af2aa8a12c81abfunc (srv *HotServer) getNetListener(addr string) (ln net.Listener, err error) { if srv.isChild { // TODO: name not effect file := os.NewFile(LISTENER_FD, &quot;&quot;) defer func() { _ = file.Close() }() ln, err = net.FileListener(file) if err != nil { err = fmt.Errorf(&quot;net.FileListener error: %v&quot;, err) return nil, err } } else { ln, err = net.Listen(&quot;tcp&quot;, addr) if err != nil { err = fmt.Errorf(&quot;net.Listen error: %v&quot;, err) return nil, err } } return ln, nil}// NewFile returns a new File with the given file descriptor and// name. The returned value will be nil if fd is not a valid file// descriptor. On Unix systems, if the file descriptor is in// non-blocking mode, NewFile will attempt to return a pollable File// (one for which the SetDeadline methods work).func NewFile(fd uintptr, name string) *File { // ...} 修复了os.NewFile后没有Close导致子进程下有未关闭的fd~ lsof -i:9999COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEdemo.bin 3539 zhangpei 3u IPv6 0x2ba97131f7043101 0t0 TCP *:distinct (LISTEN)demo.bin 3539 zhangpei 6u IPv6 0x2ba97131f7043101 0t0 TCP *:distinct (LISTEN) TODO: 在修复close前，设置NewFile的name为什么不生效？同理，在fork时，如果失败，则应该关闭getTCPListenerFile返回的filefunc (srv *HotServer) fork() (err error) { listener, err := srv.getTCPListenerFile() if err != nil { return fmt.Errorf(&quot;failed to get socket file descriptor: %v&quot;, err) } // set hotstart restart env flag env := append( os.Environ(), &quot;HOT_CONTINUE=1&quot;, ) execSpec := &amp;amp;syscall.ProcAttr{ Env: env, Files: []uintptr{os.Stdin.Fd(), os.Stdout.Fd(), os.Stderr.Fd(), listener.Fd()}, } srv.logf(&quot;HotServer do fork&quot;) _, err = syscall.ForkExec(os.Args[0], os.Args, execSpec) if err != nil { _ = listener.Close() return fmt.Errorf(&quot;Restart: Failed to launch, error: %v&quot;, err) } return}func (srv *HotServer) getTCPListenerFile() (*os.File, error) { file, err := srv.listener.(*net.TCPListener).File() if err != nil { return file, err } return file, nil}// File returns a copy of the underlying os.File.// It is the caller&#39;s responsibility to close f when finished.// Closing l does not affect f, and closing f does not affect l.//// The returned os.File&#39;s file descriptor is different from the// connection&#39;s. Attempting to change properties of the original// using this duplicate may or may not have the desired effect.func (l *TCPListener) File() (f *os.File, err error) { // ...}lsof工具lsof分析 https://zhuanlan.zhihu.com/p/36672944lsof（list open files）是一个查看当前系统文件的工具。在linux环境下，任何事物都以文件的形式存在，用户通过文件不仅可以访问常规数据，还可以访问网络连接和硬件；如传输控制协议 (TCP) 和用户数据报协议 (UDP)套接字等，系统在后台都为该应用程序分配了一个文件描述符，该文件描述符提供了大量关于此应用程序的信息。参数 -a：列出打开文件存在的进程； -c 进程名：列出指定进程所打开的文件； -g：列出GID号进程详情； -d 文件号：列出占用该文件号的进程； +d 目录：列出目录下被打开的文件； +D 目录：递归列出目录下被打开的文件； -n 目录：列出使用NFS的文件； -i 条件：列出符合条件的进程(4、6、协议、:端口、 @ip )； -p 进程号：列出指定进程号所打开的文件； -u：列出UID号进程详情；表头含义 COMMAND：进程的名称； PID：进程标识符； PPID：父进程标识符(需要指定-R参数)； USER：进程所有者； PGID：进程所属组； FD：文件描述符，应用程序通过文件描述符识别该文件。FD ①. cwd：表示current work dirctory，即：应用程序的当前工作目录，这是该应用程序启动的目录，除非它本身对这个目录进行更改； ②. txt：该类型的文件是程序代码，如应用程序二进制文件本身或共享库，如上列表中显示的 /sbin/init 程序； ③. lnn：library references (AIX)； ④. er：FD information error (see NAME column)； ⑤. jld：jail directory (FreeBSD)； ⑥. ltx：shared library text (code and data)； ⑦. mxx ：hex memory-mapped type number xx. ⑧. m86：DOS Merge mapped file； ⑨. mem：memory-mapped file； ⑩. mmap：memory-mapped device； ⑪. pd：parent directory； ⑫. rtd：root directory； ⑬. tr：kernel trace file (OpenBSD)； ⑭. v86 VP/ix mapped file； ⑮. 0：表示标准输出； ⑯. 1：表示标准输入； ⑰. 2：表示标准错误。 ①.u：表示该文件被打开并处于读取/写入模式； ②.r：表示该文件被打开并处于只读模式； ③.w：表示该文件被打开并处于只写模式； ④.空格：表示该文件的状态模式为unknow，且没有锁定； ⑤.-：表示该文件的状态模式为unknow，且被锁定。 ①. N：for a Solaris NFS lock of unknown type； ②. r：for read lock on part of the file； ③. R：for a read lock on the entire file； ④. w：for a write lock on part of the file；(文件的部分写锁) ⑤. W：for a write lock on the entire file；(整个文件的写锁) ⑥. u：for a read and write lock of any length； ⑦. U：for a lock of unknown type； ⑧. x：for an SCO OpenServer Xenix lock on part of the file； ⑨. X：for an SCO OpenServer Xenix lock on the entire file； ⑩. space：if there is no lock。TYPE ①. DIR：表示目录； ②. CHR：表示字符类型； ③. BLK：块设备类型； ④. UNIX：UNIX 域套接字； ⑤. FIFO：先进先出 (FIFO) 队列； ⑥. IPv4：网际协议 (IP) 套接字； ⑦. REG: 表示文件DEVICE指定磁盘的名称SIZE文件的大小NODE索引节点(文件在磁盘上的标识)NAME打开文件的确切名称例子以hotStart项目为例：重启前：COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEdemo.bin 7484 zhangpei cwd DIR 1,4 320 31521754 /Users/zhangpei/mine/hotstartdemo.bin 7484 zhangpei txt REG 1,4 7430060 32092172 /Users/zhangpei/mine/hotstart/demo.bindemo.bin 7484 zhangpei txt REG 1,4 973872 22900055 /usr/lib/dylddemo.bin 7484 zhangpei 0u CHR 16,3 0t60598 641 /dev/ttys003demo.bin 7484 zhangpei 1u CHR 16,3 0t60598 641 /dev/ttys003demo.bin 7484 zhangpei 2u CHR 16,3 0t60598 641 /dev/ttys003demo.bin 7484 zhangpei 4 PIPE 0x2ba97131f4332cc1 16384 -&amp;gt;0x2ba97131f4331dc1demo.bin 7484 zhangpei 5 PIPE 0x2ba97131f4331dc1 16384 -&amp;gt;0x2ba97131f4332cc1demo.bin 7484 zhangpei 6u IPv6 0x2ba97131f70436c1 0t0 TCP *:distinct (LISTEN)demo.bin 7484 zhangpei 7u KQUEUE count=0, state=0xa使用curl 127.0.0.1:9999/hello发送一个20s的请求使用kill -SIGUSR2 7195信号中断进程重启过程中：COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEdemo.bin 7484 zhangpei cwd DIR 1,4 320 31521754 /Users/zhangpei/mine/hotstartdemo.bin 7484 zhangpei txt REG 1,4 7430060 32092172 /Users/zhangpei/mine/hotstart/demo.bindemo.bin 7484 zhangpei txt REG 1,4 973872 22900055 /usr/lib/dylddemo.bin 7484 zhangpei 0u CHR 16,3 0t61117 641 /dev/ttys003demo.bin 7484 zhangpei 1u CHR 16,3 0t61117 641 /dev/ttys003demo.bin 7484 zhangpei 2u CHR 16,3 0t61117 641 /dev/ttys003demo.bin 7484 zhangpei 3u IPv6 0x2ba97131f7043101 0t0 TCP localhost:distinct-&amp;gt;localhost:56744 (ESTABLISHED)demo.bin 7484 zhangpei 4 PIPE 0x2ba97131f4332cc1 16384 -&amp;gt;0x2ba97131f4331dc1demo.bin 7484 zhangpei 5 PIPE 0x2ba97131f4331dc1 16384 -&amp;gt;0x2ba97131f4332cc1demo.bin 7484 zhangpei 7u KQUEUE count=0, state=0xademo.bin 7484 zhangpei 8u IPv6 0x2ba97131f70436c1 0t0 TCP *:distinct (LISTEN)demo.bin 7524 zhangpei cwd DIR 1,4 320 31521754 /Users/zhangpei/mine/hotstartdemo.bin 7524 zhangpei txt REG 1,4 7430060 32092172 /Users/zhangpei/mine/hotstart/demo.bindemo.bin 7524 zhangpei txt REG 1,4 973872 22900055 /usr/lib/dylddemo.bin 7524 zhangpei 0u CHR 16,3 0t61117 641 /dev/ttys003demo.bin 7524 zhangpei 1u CHR 16,3 0t61117 641 /dev/ttys003demo.bin 7524 zhangpei 2u CHR 16,3 0t61117 641 /dev/ttys003demo.bin 7524 zhangpei 4 PIPE 0x2ba97131f4330f81 16384 -&amp;gt;0x2ba97131f4331b81demo.bin 7524 zhangpei 5 PIPE 0x2ba97131f4331b81 16384 -&amp;gt;0x2ba97131f4330f81demo.bin 7524 zhangpei 6u IPv6 0x2ba97131f70436c1 0t0 TCP *:distinct (LISTEN)demo.bin 7524 zhangpei 7u KQUEUE count=0, state=0xa127.0.0.1:9999/hello返回，原有进程终止重启后：COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEdemo.bin 7524 zhangpei cwd DIR 1,4 320 31521754 /Users/zhangpei/mine/hotstartdemo.bin 7524 zhangpei txt REG 1,4 7430060 32092172 /Users/zhangpei/mine/hotstart/demo.bindemo.bin 7524 zhangpei txt REG 1,4 973872 22900055 /usr/lib/dylddemo.bin 7524 zhangpei 0u CHR 16,3 0t61311 641 /dev/ttys003demo.bin 7524 zhangpei 1u CHR 16,3 0t61311 641 /dev/ttys003demo.bin 7524 zhangpei 2u CHR 16,3 0t61311 641 /dev/ttys003demo.bin 7524 zhangpei 4 PIPE 0x2ba97131f4330f81 16384 -&amp;gt;0x2ba97131f4331b81demo.bin 7524 zhangpei 5 PIPE 0x2ba97131f4331b81 16384 -&amp;gt;0x2ba97131f4330f81demo.bin 7524 zhangpei 6u IPv6 0x2ba97131f70436c1 0t0 TCP *:distinct (LISTEN)demo.bin 7524 zhangpei 7u KQUEUE count=0, state=0xa重点关注重启中的参数，老进程的网络套接字文件描述符：demo.bin 7484 zhangpei 6u IPv6 0x2ba97131f70436c1 0t0 TCP *:distinct (LISTEN)被替换为：demo.bin 7484 zhangpei 8u IPv6 0x2ba97131f70436c1 0t0 TCP *:distinct (LISTEN)新进程的打开的网络套接字文件描述符：demo.bin 7524 zhangpei 6u IPv6 0x2ba97131f70436c1 0t0 TCP *:distinct (LISTEN)NOTE: 需要注意，每个进程中的文件描述符都是独立的，不要混淆TODO1: 思考，新的连接在新老进程中都会被accept吗？经过测试，发现新的连接确实没有被老进程accept:原因，Shutdown设置了DoneChan的信号，老进程Serve()退出不再Accept新的连接// Serve accepts incoming connections on the Listener l, creating a// new service goroutine for each. The service goroutines read requests and// then call srv.Handler to reply to them.//// HTTP/2 support is only enabled if the Listener returns *tls.Conn// connections and they were configured with &quot;h2&quot; in the TLS// Config.NextProtos.//// Serve always returns a non-nil error and closes l.// After Shutdown or Close, the returned error is ErrServerClosed.func (srv *Server) Serve(l net.Listener) error { // ... for { rw, e := l.Accept() if e != nil { select { case &amp;lt;-srv.getDoneChan(): return ErrServerClosed default: } } // ... }}TODO2: 思考，为什么老的网络套接字文件描述符从原来的6变成了8？参考代码：// start new process to handle HTTP Connectionfunc (srv *HotServer) fork() (err error) { listener, err := srv.getTCPListenerFile() if err != nil { return fmt.Errorf(&quot;failed to get socket file descriptor: %v&quot;, err) } // set hotstart restart env flag env := append( os.Environ(), &quot;HOT_CONTINUE=1&quot;, ) execSpec := &amp;amp;syscall.ProcAttr{ Env: env, Files: []uintptr{os.Stdin.Fd(), os.Stdout.Fd(), os.Stderr.Fd(), listener.Fd()}, } srv.logf(&quot;HotServer do fork&quot;) _, err = syscall.ForkExec(os.Args[0], os.Args, execSpec) if err != nil { _ = listener.Close() return fmt.Errorf(&quot;Restart: Failed to launch, error: %v&quot;, err) } return}func (srv *HotServer) getTCPListenerFile() (*os.File, error) { file, err := srv.listener.(*net.TCPListener).File() if err != nil { return file, err } return file, nil}// File returns a copy of the underlying os.File.// It is the caller&#39;s responsibility to close f when finished.// Closing l does not affect f, and closing f does not affect l.//// The returned os.File&#39;s file descriptor is different from the// connection&#39;s. Attempting to change properties of the original// using this duplicate may or may not have the desired effect.func (l *TCPListener) File() (f *os.File, err error) { if !l.ok() { return nil, syscall.EINVAL } f, err = l.file() if err != nil { return nil, &amp;amp;OpError{Op: &quot;file&quot;, Net: l.fd.net, Source: nil, Addr: l.fd.laddr, Err: err} } return}getTCPListenerFile返回了原来listener文件描述符的拷贝所以会观察到这个现象：即原来的老的listener文件描述符在Shutdown中已经关闭了，但是拷贝的listener文件描述符仍然存在TODO3: 子进程如何继承父进程的fd？ https://juejin.im/post/6844903956431241230所以在exec后fd会被系统关闭，但是我们可以直接通过os.Command来实现。这里有些人可能有点疑惑了不是FD_CLOEXEC标志的设置，新起的子进程继承的fd会被关闭。事实是os.Command启动的子进程可以继承父进程的fd并且使用, 阅读源码我们可以知道os.Command中通过Stdout,Stdin,Stderr以及ExtraFiles 传递的描述符默认会被Golang清除FD_CLOEXEC标志,通过Start方法追溯进去我们可以确认我们的想法。// dup2(i, i) won&#39;t clear close-on-exec flag on Linux,// probably not elsewhere either._, _, err1 = rawSyscall(funcPC(libc_fcntl_trampoline), uintptr(fd[i]), F_SETFD, 0)if err1 != 0 { goto childerror}对比HotStart的源码// start new process to handle HTTP Connectionfunc (srv *HotServer) fork() (err error) { listener, err := srv.getTCPListenerFile() if err != nil { return fmt.Errorf(&quot;failed to get socket file descriptor: %v&quot;, err) } // set hotstart restart env flag env := append( os.Environ(), &quot;HOT_CONTINUE=1&quot;, ) execSpec := &amp;amp;syscall.ProcAttr{ Env: env, Files: []uintptr{os.Stdin.Fd(), os.Stdout.Fd(), os.Stderr.Fd(), listener.Fd()}, } srv.logf(&quot;HotServer do fork&quot;) _, err = syscall.ForkExec(os.Args[0], os.Args, execSpec) if err != nil { _ = listener.Close() return fmt.Errorf(&quot;Restart: Failed to launch, error: %v&quot;, err) } return}func forkAndExecInChild(argv0 *byte, argv, envv []*byte, chroot, dir *byte, attr *ProcAttr, sys *SysProcAttr, pipe int) (pid int, err Errno) { // ... // guard against side effects of shuffling fds below. // Make sure that nextfd is beyond any currently open files so // that we can&#39;t run the risk of overwriting any of them. fd := make([]int, len(attr.Files)) nextfd = len(attr.Files) for i, ufd := range attr.Files { if nextfd &amp;lt; int(ufd) { nextfd = int(ufd) } fd[i] = int(ufd) } nextfd++ // ... // Pass 2: dup fd[i] down onto i. for i = 0; i &amp;lt; len(fd); i++ { if fd[i] == -1 { rawSyscall(funcPC(libc_close_trampoline), uintptr(i), 0, 0) continue } if fd[i] == int(i) { // dup2(i, i) won&#39;t clear close-on-exec flag on Linux, // probably not elsewhere either. _, _, err1 = rawSyscall(funcPC(libc_fcntl_trampoline), uintptr(fd[i]), F_SETFD, 0) if err1 != 0 { goto childerror } continue } // The new fd is created NOT close-on-exec, // which is exactly what we want. _, _, err1 = rawSyscall(funcPC(libc_dup2_trampoline), uintptr(fd[i]), uintptr(i), 0) if err1 != 0 { goto childerror } } // ... }" }, { "title": "优雅重启 分析1", "url": "/posts/gradefully-shutdown-1/", "categories": "cs", "tags": "gradefully-shotdown", "date": "2021-09-27 00:00:00 +0800", "snippet": "如何实现 实现目的 不关闭现有连接（正在运行中的程序）。 新的进程启动并替代旧进程。 新的进程接管新的连接。 连接要随时响应用户的请求。当用户仍在请求旧进程时要保持连接，新用户应请求新进程，不可以出现拒绝请求的情况。 实现原理 监听 SIGHUP 信号; 收到信号后将服务监听的文件描述符传递给新的子进程; 此时新老进程同时接收请求; 父进程停止接收新请求, 等待旧请求完成(或超时); 父进程退出. 代码分析go http库shutdown// Shutdown gracefully shuts down the server without interrupting any// active connections. Shutdown works by first closing all open// listeners, then closing all idle connections, and then waiting// indefinitely for connections to return to idle and then shut down.// If the provided context expires before the shutdown is complete,// Shutdown returns the context&#39;s error, otherwise it returns any// error returned from closing the Server&#39;s underlying Listener(s).//// When Shutdown is called, Serve, ListenAndServe, and// ListenAndServeTLS immediately return ErrServerClosed. Make sure the// program doesn&#39;t exit and waits instead for Shutdown to return.//// Shutdown does not attempt to close nor wait for hijacked// connections such as WebSockets. The caller of Shutdown should// separately notify such long-lived connections of shutdown and wait// for them to close, if desired. See RegisterOnShutdown for a way to// register shutdown notification functions.//// Once Shutdown has been called on a server, it may not be reused;// future calls to methods such as Serve will return ErrServerClosed.func (srv *Server) Shutdown(ctx context.Context) error { // 在原子的存储某个值的过程中，任何cpu都不会进行针对进行同一个值的读或写操作。 // 如果我们把所有针对此值的写操作都改为原子操作，那么就不会出现针对此值的读操作读操作因被并发的进行而读到修改了一半的情况。 atomic.StoreInt32(&amp;amp;srv.inShutdown, 1) srv.mu.Lock() lnerr := srv.closeListenersLocked() srv.closeDoneChanLocked() for _, f := range srv.onShutdown { go f() } srv.mu.Unlock() ticker := time.NewTicker(shutdownPollInterval) defer ticker.Stop() for { if srv.closeIdleConns() { return lnerr } select { case &amp;lt;-ctx.Done(): return ctx.Err() case &amp;lt;-ticker.C: } }}go在关闭http服务的流程如下： 先关闭所有的打开的listener 然后关闭空闲的connections 最后无限期等待连接转为空闲后再做关闭 也就是说，如果在此期间有在处理的连接是不会关闭的对比直接close的方法：// Close immediately closes all active net.Listeners and any// connections in state StateNew, StateActive, or StateIdle. For a// graceful shutdown, use Shutdown.//// Close does not attempt to close (and does not even know about)// any hijacked connections, such as WebSockets.//// Close returns any error returned from closing the Server&#39;s// underlying Listener(s).func (srv *Server) Close() error { atomic.StoreInt32(&amp;amp;srv.inShutdown, 1) srv.mu.Lock() defer srv.mu.Unlock() srv.closeDoneChanLocked() err := srv.closeListenersLocked() for c := range srv.activeConn { c.rwc.Close() delete(srv.activeConn, c) } return err}可以发现差异主要在于Close是直接遍历activeConn然后直接Close掉，而Shutdown则是设定定时器去关闭空闲链接closeIdleConns遍历所有连接，尝试关闭，如果所有的连接都关闭了则返回true// closeIdleConns closes all idle connections and reports whether the// server is quiescent.func (s *Server) closeIdleConns() bool { s.mu.Lock() defer s.mu.Unlock() quiescent := true for c := range s.activeConn { st, unixSec := c.getState() // Issue 22682: treat StateNew connections as if // they&#39;re idle if we haven&#39;t read the first request&#39;s // header in over 5 seconds. if st == StateNew &amp;amp;&amp;amp; unixSec &amp;lt; time.Now().Unix()-5 { st = StateIdle } if st != StateIdle || unixSec == 0 { // Assume unixSec == 0 means it&#39;s a very new // connection, without state set yet. quiescent = false continue } c.rwc.Close() delete(s.activeConn, c) } return quiescent}此处涉及到ConnState的概念var stateName = map[ConnState]string{ StateNew: &quot;new&quot;, StateActive: &quot;active&quot;, StateIdle: &quot;idle&quot;, StateHijacked: &quot;hijacked&quot;, StateClosed: &quot;closed&quot;,}const ( // StateNew represents a new connection that is expected to // send a request immediately. Connections begin at this // state and then transition to either StateActive or // StateClosed. StateNew ConnState = iota // StateActive represents a connection that has read 1 or more // bytes of a request. The Server.ConnState hook for // StateActive fires before the request has entered a handler // and doesn&#39;t fire again until the request has been // handled. After the request is handled, the state // transitions to StateClosed, StateHijacked, or StateIdle. // For HTTP/2, StateActive fires on the transition from zero // to one active request, and only transitions away once all // active requests are complete. That means that ConnState // cannot be used to do per-request work; ConnState only notes // the overall state of the connection. StateActive // StateIdle represents a connection that has finished // handling a request and is in the keep-alive state, waiting // for a new request. Connections transition from StateIdle // to either StateActive or StateClosed. StateIdle // StateHijacked represents a hijacked connection. // This is a terminal state. It does not transition to StateClosed. StateHijacked // StateClosed represents a closed connection. // This is a terminal state. Hijacked connections do not // transition to StateClosed. StateClosed)StateHijacked：被劫持，例如websocket参考 https://juejin.im/post/6867074626427502600" }, { "title": "youtube系统设计", "url": "/posts/youtube-design/", "categories": "cs, system-design", "tags": "youtube", "date": "2021-09-26 00:00:00 +0800", "snippet": "design youtube clarify the requirements capacity estimation（容量估计） system apis high-level system design data storage scalabilityclarify the requirements upload view share like/dislike comment search recommendNon-Functional requirementconsistency 每次请求都可以获取最新的feed或者错误 sacrifice（牺牲）达到最终一致性availability 每次请求都有响应，尽量少的错误 scalable：性能：低延迟partition tolerance 少部分机器或者网络down掉不影响整体服务capacity estimationassumptions: 2 billion total users, 150 million daily active user each content creator: 1 video pre week, 1% among all users are content creators each content consumer: avg watching time: 50 mins each video: avg length: 10 mins; avg upload resolution 1080p; bandwidth requirement for 1080p playback: 5Mbpsstorage estimation:new videos:2B * 1% * 1 video / 7 days == 33 videos / sfile size per minute:1080p + … = 100MB/mindaily write size:33 videos / s * 86400 s /day * 10min * 100MB / min = 2.7PB / dayreplication:redundancy: same-region replication * 3availabliity: cross-regin replication * 3bandwidth esimation:daily updaload bandwidth: 33 videos/s * (10*60s/video) * 5Mps = 99Gbpsdaily download/ongoing bandwidth: concurrent users: 150M DAY * 50 mins / (24 * 60mins /day) = 5.2M users bandwidth: 5.2M * 5Mbps = 26Tbps read / write ratio: 26 Tbps / 99Gbps = 263:1根据上行下行带宽比值得出，此类视频网站是读多写少的平台system apis uploadVideo streamVideo(videoId, offSet, codec, resolution) offSet可以约定从什么时候开始播放hight-level system designuploadVideouser -&amp;gt; load balancer -&amp;gt; upload service -&amp;gt; processing queue -&amp;gt; video processing service -&amp;gt; distributed media storeage -&amp;gt; database (metadata user)video processing service (async) -&amp;gt; completion queue -&amp;gt; video distributing service -&amp;gt; CDNvideo processing service breakdown video into chunks transcoding(multiple codec &amp;amp; resolutions) decode&amp;amp;encode generate thumbnails and previews video understanding MLCDN热门视频process给用户，冷门视频直接由源传给用户watchVideouser -&amp;gt; load balancer -&amp;gt; video playback service -&amp;gt; distributed media storage -&amp;gt; database (metadata/user) -&amp;gt; host identify service -&amp;gt; CDNdata storageuser tablevideo metadata tablecomment table sql: relational database for user table, video metadata nosql: store unstructured data for thumbnails in big table file system / blob storage: videofile system: HDFS / GlusterFSblob storage: netflix used amazon S3scalabilitybottlenecks &amp;amp; solutionsread heavy system distribute/replicate data across servers/regions data sharding data replicationlatency caching CDNoptimization 1: data sharding sharding by video_id via consistent hashing hot video can make shards experience very traffic: solution: replicate the hot videos to more serversoptimization 2: data replication primary-secondary configuration: write to primary and then propagate to all secondaries read from secondaryoptimization 3: caching CDN server &amp;lt;-&amp;gt; database how many to cache: cache 20% of daily read videos how to scale: consistent hashingoptimization 4: CDN predict locations where people would prefer to watch a video参考 https://www.bilibili.com/video/BV1s5411p7dR" }, { "title": "type-head系统设计", "url": "/posts/typeahead-design/", "categories": "cs, system-design", "tags": "type-head", "date": "2021-09-26 00:00:00 +0800", "snippet": "设计比较粗暴的方式实时log，记录所有单词的出现频率，然后用SQL抓取Top多少的词with a prefixkeyword hit_count“amazon” 20bSELECT * FROM hit_statsWHERE keyword LIKE ‘${key}%’ORDER BY hit_count DESCLIMIT 10缺陷：like查询较慢，是一种range query优化思路：使用Trie树联想到在Gin中使用的前缀路由匹配 https://geektutu.com/post/gee-day3.htmltype node struct { pattern string // 待匹配路由，例如 /p/:lang part string // 路由中的一部分，例如 :lang children []*node // 子节点，例如 [doc, tutorial, intro] isWild bool // 是否精确匹配，part 含有 : 或 * 时为true}class TrieNode { Map&amp;lt;Character, TrieNode&amp;gt; children; List topWords;}如何做sharding，使用一致哈希，这样机器增多时，还是会map的原来的key如何reduce log file按照1/10000等频率采样时间和空间复杂度是多少？trie多久更新一次？ 具体如何更新trie?trie通常的实现有用数组的，有用hashmap的， 这里用哪种？在leetcode里面我们经常使用数组， 但是这里用户搜索输入词范围肯定是unicode， 那一定非常稀疏， 所以必须要用hashmap单机装不下一个完整的trie怎么办？搜索词的prefix分布很不均衡怎么办？如何结合trending query?当前发生热门突发事件， 用户的搜索日志还没有来得及处理这些搜索词， 但是需要加入结果，怎么办？如何做用户个性化的结果？存储trie的节点如果挂了怎么办？ 如何提高可用性？整个trie是直接放在内存里面嘛？ 还是放在文件里面？ 还是放在其他类似redis或者memcache的系统里面？online的service需要做cache嘛?参考 https://www.jianshu.com/p/c7fc9092d9fehttp://www.noteanddata.com/interview-problems-system-design-learning-notes-autocomplete.htmlhttps://www.youtube.com/watch?v=uIqvbYVBiCI&amp;amp;t=1s" }, { "title": "推特系统设计", "url": "/posts/twitter-design/", "categories": "cs, system-design", "tags": "twitter", "date": "2021-09-26 00:00:00 +0800", "snippet": "Design twitter clarify the requirements capacity estimation（容量估计） system apis high-level system design data storage scalabilityclarify the requirements post tweets timelineNon-Functional requirementconsistency 每次请求都可以获取最新的feed或者错误 sacrifice（牺牲）达到最终一致性availability 每次请求都有响应，尽量少的错误 scalable：性能：低延迟partition tolerance 少部分机器或者网络down掉不影响整体服务capacity estimation 200million DAU, 100million new tweets each user visit home timeline 5 times; other user timeline 3 times each timeline/page has 20 tweets each tweet has size 280 bytes, metadata 30 bytes（20% tweets hava images, 10% tweets have video, 30% videos will be watched）storage estimate:write size daily:text: 100M(280 + 30) = 31GB/dayimage: 100M20%200kb = 4TB/dayvideo: 100M10%2M = 20TB/daybandwidth estimate200M*(5 home visit+3 uer visit) * 20 tweets/page = 32Btext: 32B * 280 bytes / 86400 = 100MB/simage: 32B * 20% * 200 / 86400 = 14GB/svideo: 32B * 10% * 30% * 2MB / 86400 = 20GB/ssystem apispost tweetdelete tweetlike or unlike tweetread home timeline (pagesize, opt pageToken) pageToken 实现翻页read user timeline (pagesize, opt pageToken) pageToken 实现翻页user -&amp;gt; loadBalancer -&amp;gt; tweet writer (post tweet) -&amp;gt; DB/CACHE -&amp;gt; loadBalancer -&amp;gt; timeline service -&amp;gt; CACHEvisit home timeline: tweet writer -&amp;gt; (fan out on write) -&amp;gt; get follower info -&amp;gt; update timeline cache of all followershome timeline naive solution: pull modefetch tweets from N followers from DB, merge and returnpros: write is fast: O(1)cons: read is slow: O(N) DB readsbetter solution: push modemaintain（维持） a feed list in cache from each userfanout on writepros: read is fast: O(1) from the feed list in cachecons: write need more efforts: O(N) write for each new tweetasync tasks: delay is showing latest tweets(eventual consistency)fan out on write (局限性) not efficient for users with huge amount of followershybrid solution not-hot users:fan out on write(push)do not fanout on non-active users hot users:fan in on read(pull): read during timeline request from tweets cachedata storageuser tabletweet tablefollower tablesql database: user tablenosql database: timelinefile system media filescalabilityidentify potential bottlenecksdiscussion solutions, focusing on tradeoff data sharding: datastore, cache load balancing: user &amp;lt;-&amp;gt; application server; applcation server &amp;lt;-&amp;gt; cache server; cache server &amp;lt;-&amp;gt; db data caching: read heavyshardinghow: break large tables into smaller shards on multiple serverspros: horizointal shardscons: complexity(distributed query, resharding)option 1: shard by tweets creation timepros: limited shards to querycons: hot/cold data issue; new shard fill up quicklyhot/cold data issue: 在tweets这种时效性比较高的场景下，大部分访问都是最近一天甚至一小时的数据，那么大量的压力集中在了最近的表上，资源分配不均，存在浪费，应该避免option 2: shard by hash：store all the data of a user on a single shardpros: simple; query user timelien is straightforwardcons: home timeline still needs to query multiple shards non-uniform distribution of storeage: user data might not be able to fit into a single shard hot users availabilityoption 3: shard by hash(tweetId)pros: uniform distribution（均匀分布）; high availabilitycons: need to query all shard in order to generate user / home timelinecachingtimeline service: user timeline : user_id -&amp;gt; {tweet_id} home timeline : user_id -&amp;gt; {tweet_id} tweets: tweet_id -&amp;gt; tweettweets并不能支持翻页到最后一页的设计，目前通过前端限制了翻页方式，只能往下不停翻页topics: caching policy: FIFO(First In First Out) LRU(Least Recently Used) LFU(Least Frequently Used) sharding performance参考 https://www.bilibili.com/video/BV1Sf4y1e7wc" }, { "title": "tcp拥塞控制 笔记", "url": "/posts/tcp-congestion-control-note/", "categories": "cs, network", "tags": "tcp", "date": "2021-09-26 00:00:00 +0800", "snippet": "概述TCP拥塞控制不仅仅是网络层的概念，可以将其归属于控制论的范畴。在TCP的演进过程中，出现了很多优秀的思想和算法，以实现网络传输过程中，在公平竞争性的前提下，尽可能地利用带宽资源。1、公平性公平性是在发生拥塞时各源端（或同一源端建立的不同TCP连接或UDP数据报）能公平地共享同一网络资源（如带宽、缓存等）。处于相同级别的源端应该得到相同数量的网络资源。产生公平性的根本原因在于拥塞发生必然导致数据包丢失，而数据包丢失会导致各数据流之间为争抢有限的网络资源发生竞争，争抢能力弱的数据流将受到更多损害。因此，没有拥塞，也就没有公平性问题。TCP层上的公平性问题表现在两方面：（1）面向连接的TCP和无连接的UDP在拥塞发生时对拥塞指示的不同反应和处理，导致对网络资源的不公平使用问题。在拥塞发生时，有拥塞控制机制的TCP会按拥塞控制步骤进入拥塞避免阶段，从而主动减小发送到网络的数据量。但对无连接的数据报UDP，由于没有端到端的拥塞控制机制，即使网络出现了拥塞，也不会减少向网络发送的数据量。结果遵守拥塞控制的TCP数据流得到的网络资源越来越少，没有拥塞控制的UDP则会得到越来越多的网络资源。（2）TCP连接之间也存在公平性问题。产生问题的原因在于使用了不同的拥塞控制算法，一些TCP在拥塞前使用了大窗口尺寸，或者它们的RTT较小，或者数据包比其他TCP大，这样它们也会多占带宽。2、拥塞控制过程拥塞控制主要包括四个过程：1）慢启动；2）拥塞避免；3）拥塞发生；4）快速恢复。3、拥塞控制经典概念RTT：数据包从发出去到收到对它的ack的来回时间，采用平滑方式计算RTTRTO：重传超时。简单的如RTO=n*RTT, n=3（或其他RTO计算方法）SACK：TCP Option携带多组ACK信息FR：Fast Retransmission，收到3个dup ack后，即可认为发生了丢包。不需要等待RTO超时即可重传丢失的包。ER：Early Retransmission，无法产生足够的dupack和没有新的数据包可以发送进入网络的情况下，减少触发FR的dup ack数量，以达到触发FR的目的。TLP：如果发生了尾丢包，由于尾包后面没有更多的数据包，也就没有办法触发任何的dupack。实际上，Google统计超过70%的RTO是尾丢包导致没有任何dup ack。TLP算法是通过发送一个loss probe包，来产生足够的SACK/FACK的信息以触发RF。Pacing：控制发送速率，防止bursting流控：Flow control站在单条TCP连接的维度，目的是让发送方发包的速度，不超过接收方收包的能力。所以流控解决的问题是，如何在接收方可承受的范围内，让单条 TCP 连接的速度最大化。通过滑动窗口机制实现。拥塞控制：Congestion control站在整个互联网的维度，让网络里所有TCP连接最大化共享网络通道的同时，尽可能的少出现网络拥塞现象，让网络世界里的每一个参与者既公平又高效。cwnd：发送窗口，拥塞窗口；在拥塞控制过程中窗口大小值变化。rwnd：接收窗口，通知发送者能够发送的数据大小。sliding window：滑动窗口，只是一种抽象机制概念；在发送请求及收到ack的过程中滑动。二、TCP拥塞控制算法分类历史上出现的各种TCP拥塞控制算法，其本质是针对拥塞控制的四个过程做策略调整。按照算法依据的因素，可以简单的分为以下类型：1、基于丢包的拥塞控制：Tahoe、Reno、New Reno因为Reno等算法是后续算法的基础，这里详细的描述下Reno算法的过程。（1）慢热启动算法 – Slow Start连接建好的开始先初始化cwnd = 1，表明可以传一个MSS大小的数据。每当收到一个ACK，cwnd++; 呈线性上升。每当过了一个RTT，cwnd = cwnd*2; 呈指数让升。还有一个ssthresh（slow start threshold），是一个上限，当cwnd &amp;gt;= ssthresh时，就会进入“拥塞避免算法”。（2）拥塞避免算法 – Congestion Avoidance当cwnd &amp;gt;= ssthresh时，就会进入“拥塞避免算法”。算法如下：收到一个ACK时，cwnd = cwnd + 1/cwnd当每过一个RTT时，cwnd = cwnd + 1（3）拥塞状态算法 – Fast RetransmitTahoe是等RTO超时，FR是在收到3个duplicate ACK时就开启重传，而不用等到RTO超时。拥塞发生时：cwnd = cwnd /2sshthresh = cwnd（4）快速恢复 – Fast Recoverycwnd = sshthresh + 3 * MSS （3的意思是确认有3个数据包被收到了）重传Duplicated ACKs指定的数据包如果再收到 duplicated Acks，那么cwnd = cwnd +1如果收到了新的Ack，那么，cwnd = sshthresh ，然后进入拥塞避免算法。Reno算法以其简单、有效和鲁棒性，应用最广泛。该算法所包含的慢启动、拥塞避免和快速重传、快速恢复机制，是现有的众多算法的基础。从Reno运行机制中很容易看出，为了维持一个动态平衡，必须周期性地产生一定量的丢失，再加上AIMD机制–减少快，增长慢，尤其是在大窗口环境下，由于一个数据报的丢失所带来的窗口缩小要花费很长的时间来恢复，这样，带宽利用率不可能很高且随着网络的链路带宽不断提升，这种弊端将越来越明显。另外，丢包并不一定是网络拥塞，可能是网络常态，但是基于丢包的拥塞控制并不能区分。2、基于时延RTT的带宽预测：vegasvegas通过对RTT的非常重的监控来计算一个基准RTT。然后通过这个基准RTT来估计当前的网络实际带宽，如果实际带宽比我们的期望的带宽要小或是要多的活，那么就开始线性地减少或增加cwnd的大小。中间路由器缓存数据导致RTT变大，认为发生拥塞；RTT不公平性，当不同的数据流对网络瓶颈带宽进行竞争时，具有较小RTT的TCP数据流的拥塞窗口增加速率将会快于具有大RTT的TCP数据流，从而将会占有更多的网络带宽资源。3、基于丢包和RTT：westwood在发送端做带宽估计，当探测到丢包时，根据带宽值来设置拥塞窗口、慢启动阈值。 那么，这个算法是怎么测量带宽的？每个RTT时间，会测量一次带宽，测量带宽的公式很简单，就是这段RTT内成功被ACK了多少字节。Westwood会根据RTT变化来判断丢包是否是网络拥塞造成的，还是网络常态的丢包。如果时延变化不明显，就认为是非网络拥塞，此时cwnd减少的比较小。4、二分搜索最佳cwnd：BIC-TCPBIC-TCP是Linux 2.6.18默认拥塞控制算法，依赖丢包条件触发。BIC-TCP认为TCP拥塞窗口调整的本质就是找到最适合当前网络的一个发送窗口，为了找到这个窗口值，TCP采取的方式是(拥塞避免阶段)每RTT加1，缓慢上升，丢包时下降一半，接着再来慢慢上升。BIC-TCP的提出者们看穿了事情的本质，其实这就是一个搜索的过程，而TCP的搜索方式类似于逐个遍历搜索方法，可以认为这个值是在1和一个比较大的数(large_window)之间，既然在这个区间内需要搜索一个最佳值，那么显然最好的方式就是二分搜索思想。BIC-TCP就是基于这样一个二分思想的：当出现丢包的时候，说明最佳窗口值应该比这个值小，那么BIC就把此时的cwnd设置为max_win，把乘法减小后的值设置为min_win，然后BIC就开始在这两者之间执行二分思想–每次跳到max_win和min_win的中点。BIC也具备RTT的不公平性。RTT小的连接，窗口调整发生的速度越快，因此可能更快的抢占带宽。5、连续拥塞间隔：CUBICCUBIC在设计上简化了BIC-TCP的窗口调整算法，在BIC-TCP的窗口调整中会出现一个凹和凸(这里的凹和凸指的是数学意义上的凹和凸，凹函数/凸函数)的增长曲线，CUBIC使用了一个三次函数(即一个立方函数)，在三次函数曲线中同样存在一个凹和凸的部分，该曲线形状和BIC-TCP的曲线图十分相似，于是该部分取代BIC-TCP的增长曲线。另外，CUBIC中最关键的点在于它的窗口增长函数仅仅取决于连续的两次拥塞事件的时间间隔值，从而窗口增长完全独立于网络的时延RTT，使得连接之间保持良好的RRTT公平性。来看下具体细节：当某次拥塞事件发生时，Wmax设置为此时发生拥塞时的窗口值，然后把窗口进行乘法减小，乘法减小因子设为β，当从快速恢复阶段退出然后进入到拥塞避免阶段，此时CUBIC的窗口增长开始按照“凹”式增长曲线进行增长，该过程一直持续直到窗口再次增长到Wmax，紧接着，该函数转入“凸”式增长阶段。该方式的增长可以使得窗口一直维持在Wmax附近，从而可以达到网络带宽的高利用率和协议本身的稳定性。CUBIC窗口的增长函数：W(t) = C * (t-K)3 + Wmax, 其中C和β为常量。t为当前时间距上一次窗口减小的时间差，而K就代表该函数从W增长到Wmax的时间周期。通俗一点讲，假如我们知道了Wmax，那么CUBIC的核心思想就是需要在连续两次拥塞期间执行完上面的三次函数增长曲线，6、基于精准带宽计算：BBRBBR通过实时计算带宽和最小RTT来决定发送速率pacing rate和窗口大小cwnd。完全摒弃丢包作为拥塞控制的直接反馈因素。三、BBR算法原理分析传统的拥塞控制算法是计算cwnd值来规定当前可以发送多少数据，但是并不关注以什么样的速度发送数据。如果简单而粗暴地将窗口大小（send.cwnd、recv.cwnd的最小值）数据全部突发出去，这往往会造成路由器的排队，在深队列的情况下，会测量出rtt剧烈地抖动。bbr在计算cwnd的同时，还计算了一个与之适配的pacing rate，该pacing rate规定cwnd指示的一窗数据的数据包之间，以多大的时间间隔发送出去。1、基础概念BtlBW：最大带宽RtProp：物理链路延迟BDP：管道容量，BDP=BtlBW * RtProp我们知道，网络工作的最优点是在物理链路延迟状态下，以最大速率传输数据。传统的拥塞控制算法思想是根据数据传输及ACK来确定RTT，但是这个RTT并不是物理链路延时，可能包含了路由器缓存耗时，也可能是拥塞状态下的耗时。传统的带宽计算也是在不断的试探逼近最优发送窗口，并在RTT或者统计周期内计算带宽。这种情况下，RTT并不是真正的物理链路延迟，带宽也有可能是在有路由缓存或丢包状况下计算得到，那么必然得到的不是精准的值。BBR摒弃了丢包和实时RTT作为拥塞控制因素。引入BDP管道容量来衡量链路传输水平。BBR追求的是在链路最小RTT（物理链路延迟）的状态下，找到最大带宽。2、BBR算法理论基础首先我们认为网络最优点是可以达到的。下面描述RTT及收包速率与数据包投递速率的关系。图中上半部分的过程可以描述为：随着数据包投递速率增加，如果没有超过最优带宽，则RTT不会变化，此时的RTT是物理链路延迟。随着投递速率继续增加，这时中间路由节点可能出现需要缓存数据包的情况，这会导致RTT变大。如果投递速率继续增加，超过路由缓存能力，则可能出现丢包。图中下半部分的过程可以描述为：随着数据包投递速率增加，如果没有超过最优带宽，则发送方确认接收端收到的数据速率增加。随着投递速率继续增加，因为数据包缓存在中间路由，这些包并不能及时得到ACK，因此发送方得到的ACK速率，即发送发确认接收方收到数据的速率会维持不变。如果投递速率继续增加，超过路由缓存能力，则可能出现丢包。3、带宽计算1）应答了多少数据，记为delivered；2）应答1）中的delivered这么多数据所用的时间，记为interval_us。将上述二者相除，就能得到带宽：bw = delivered/interval_us；该计算方法不关注数据包ack及顺序，是纯粹的标量。我们可以根据图示很容易算出从Delivered为7时的数据包被确认到X被确认为止，一共有12-7=5个数据包被确认，即这段时间网络上清空了5个数据包。我们便很容易算出带宽值了。4、ProbeBW：探测最大可用带宽BtlBW * Pacing_Gain，每次增加码率是之前的25% per RTT10s探测一次带宽5、ProbeRTT：探测最小RTT当10s内没有发现最小RTTProp时，就要进入ProbeRTT状态。在ProbeRTT状态，仅发4MSS/RTT(接近停止发送)，从而排空链路上的数据包，测量真实的RTTProp。这里带来的一个问题是，在一个RTT时间内以4MSS速率发送可能会造成抖动，特别是长RTT场景。具体的参考willko文章《GBN手札-BBR实时大数据传输之痛》。6、BBR不止于此BBR具体的实现，例如ProbBw和ProbRTT的状态机维护等内容，在此就不展开讲述。感兴趣的同学可以阅读BBR官方文档。参考 https://cloud.tencent.com/developer/article/1369617" }, { "title": "select/poll/epoll详解 笔记", "url": "/posts/select-poll-epoll-note/", "categories": "cs, os", "tags": "select, poll, epoll", "date": "2021-09-26 00:00:00 +0800", "snippet": "概念说明 用户空间和内核空间 进程切换 进程的阻塞 文件描述符 缓存 I/O用户空间与内核空间现在操作系统都是采用虚拟存储器，那么对32位操作系统而言，它的寻址空间（虚拟存储空间）为4G（2的32次方）。操作系统的核心是内核，独立于普通的应用程序，可以访问受保护的内存空间，也有访问底层硬件设备的所有权限。为了保证用户进程不能直接操作内核（kernel），保证内核的安全，操心系统将虚拟空间划分为两部分，一部分为内核空间，一部分为用户空间。针对linux操作系统而言，将最高的1G字节（从虚拟地址0xC0000000到0xFFFFFFFF），供内核使用，称为内核空间，而将较低的3G字节（从虚拟地址0x00000000到0xBFFFFFFF），供各个进程使用，称为用户空间。进程切换为了控制进程的执行，内核必须有能力挂起正在CPU上运行的进程，并恢复以前挂起的某个进程的执行。这种行为被称为进程切换。因此可以说，任何进程都是在操作系统内核的支持下运行的，是与内核紧密相关的。从一个进程的运行转到另一个进程上运行，这个过程中经过下面这些变化： 保存处理机上下文，包括程序计数器和其他寄存器。 更新PCB信息。 把进程的PCB移入相应的队列，如就绪、在某事件阻塞等队列。 选择另一个进程执行，并更新其PCB。 更新内存管理的数据结构。 恢复处理机上下文。TODO: 从linux的角度理解PCB（task_struct）进程的阻塞正在执行的进程，由于期待的某些事件未发生，如请求系统资源失败、等待某种操作的完成、新数据尚未到达或无新工作做等，则由系统自动执行阻塞原语(Block)，使自己由运行状态变为阻塞状态。可见，进程的阻塞是进程自身的一种主动行为，也因此只有处于运行态的进程（获得CPU），才可能将其转为阻塞状态。当进程进入阻塞状态，是不占用CPU资源的。文件描述符fd文件描述符（File descriptor）是计算机科学中的一个术语，是一个用于表述指向文件的引用的抽象化概念。文件描述符在形式上是一个非负整数。实际上，它是一个索引值，指向内核为每一个进程所维护的该进程打开文件的记录表。当程序打开一个现有文件或者创建一个新文件时，内核向进程返回一个文件描述符。在程序设计中，一些涉及底层的程序编写往往会围绕着文件描述符展开。但是文件描述符这一概念往往只适用于UNIX、Linux这样的操作系统。缓存 I/O缓存 I/O 又被称作标准 I/O，大多数文件系统的默认 I/O 操作都是缓存 I/O。在 Linux 的缓存 I/O 机制中，操作系统会将 I/O 的数据缓存在文件系统的页缓存（ page cache ）中，也就是说，数据会先被拷贝到操作系统内核的缓冲区中，然后才会从操作系统内核的缓冲区拷贝到应用程序的地址空间。缓存 I/O 的缺点：数据在传输过程中需要在应用程序地址空间和内核进行多次数据拷贝操作，这些数据拷贝操作所带来的 CPU 以及内存开销是非常大的。IO模式刚才说了，对于一次IO访问（以read举例），数据会先被拷贝到操作系统内核的缓冲区中，然后才会从操作系统内核的缓冲区拷贝到应用程序的地址空间。所以说，当一个read操作发生时，它会经历两个阶段： 等待数据准备 (Waiting for the data to be ready) 将数据从内核拷贝到进程中 (Copying the data from the kernel to the process)正式因为这两个阶段，linux系统产生了下面五种网络模式的方案。 阻塞 I/O（blocking IO） 非阻塞 I/O（nonblocking IO） I/O 多路复用（ IO multiplexing） 信号驱动 I/O（ signal driven IO） 异步 I/O（asynchronous IO）注：由于signal driven IO在实际中并不常用，所以我这只提及剩下的四种IO Model。阻塞IO（blocking IO）在linux中，默认情况下所有的socket都是blocking，一个典型的读操作流程大概是这样：当用户进程调用了recvfrom这个系统调用，kernel就开始了IO的第一个阶段：准备数据（对于网络IO来说，很多时候数据在一开始还没有到达。比如，还没有收到一个完整的UDP包。这个时候kernel就要等待足够的数据到来）。这个过程需要等待，也就是说数据被拷贝到操作系统内核的缓冲区中是需要一个过程的。而在用户进程这边，整个进程会被阻塞（当然，是进程自己选择的阻塞）。当kernel一直等到数据准备好了，它就会将数据从kernel中拷贝到用户内存，然后kernel返回结果，用户进程才解除block的状态，重新运行起来。所以，blocking IO的特点就是在IO执行的两个阶段（wait for data / copy data from kernel to user）都被block了。非阻塞 I/O（nonblocking IO）linux下，可以通过设置socket使其变为non-blocking。当对一个non-blocking socket执行读操作时，流程是这个样子：当用户进程发出read操作时，如果kernel中的数据还没有准备好，那么它并不会block用户进程，而是立刻返回一个error。从用户进程角度讲 ，它发起一个read操作后，并不需要等待，而是马上就得到了一个结果。用户进程判断结果是一个error时，它就知道数据还没有准备好，于是它可以再次发送read操作。一旦kernel中的数据准备好了，并且又再次收到了用户进程的system call，那么它马上就将数据拷贝到了用户内存，然后返回。所以，nonblocking IO的特点是用户进程需要不断的主动询问kernel数据好了没有。I/O 多路复用（IO multiplexing）IO multiplexing就是我们说的select，poll，epoll，有些地方也称这种IO方式为event driven IO。select/epoll的好处就在于单个process就可以同时处理多个网络连接的IO。它的基本原理就是select，poll，epoll这个function会不断的轮询所负责的所有socket，当某个socket有数据到达了，就通知用户进程。当用户进程调用了select，那么整个进程会被block，而同时，kernel会“监视”所有select负责的socket，当任何一个socket中的数据准备好了，select就会返回。这个时候用户进程再调用read操作，将数据从kernel拷贝到用户进程。所以，I/O 多路复用的特点是通过一种机制一个进程能同时等待多个文件描述符，而这些文件描述符（套接字描述符）其中的任意一个进入读就绪状态，select()函数就可以返回。这个图和blocking IO的图其实并没有太大的不同，事实上，还更差一些。因为这里需要使用两个system call (select 和 recvfrom)，而blocking IO只调用了一个system call (recvfrom)。但是，用select的优势在于它可以同时处理多个connection。所以，如果处理的连接数不是很高的话，使用select/epoll的web server不一定比使用multi-threading + blocking IO的web server性能更好，可能延迟还更大。select/epoll的优势并不是对于单个连接能处理得更快，而是在于能处理更多的连接。）在IO multiplexing Model中，实际中，对于每一个socket，一般都设置成为non-blocking，但是，如上图所示，整个用户的process其实是一直被block的。只不过process是被select这个函数block，而不是被socket IO给block。异步 I/O（asynchronous IO）inux下的asynchronous IO其实用得很少。先看一下它的流程：用户进程发起read操作之后，立刻就可以开始去做其它的事。而另一方面，从kernel的角度，当它受到一个asynchronous read之后，首先它会立刻返回，所以不会对用户进程产生任何block。然后，kernel会等待数据准备完成，然后将数据拷贝到用户内存，当这一切都完成之后，kernel会给用户进程发送一个signal，告诉它read操作完成了。总结blocking和non-blocking的区别调用blocking IO会一直block住对应的进程直到操作完成，而non-blocking IO在kernel还准备数据的情况下会立刻返回。synchronous IO和asynchronous IO的区别在说明synchronous IO和asynchronous IO的区别之前，需要先给出两者的定义。POSIX的定义是这样子的： A synchronous I/O operation causes the requesting process to be blocked until that I/O operation completes; An asynchronous I/O operation does not cause the requesting process to be blocked;两者的区别就在于synchronous IO做”IO operation”的时候会将process阻塞。按照这个定义，之前所述的blocking IO，non-blocking IO，IO multiplexing都属于synchronous IO。有人会说，non-blocking IO并没有被block啊。这里有个非常“狡猾”的地方，定义中所指的”IO operation”是指真实的IO操作，就是例子中的recvfrom这个system call。non-blocking IO在执行recvfrom这个system call的时候，如果kernel的数据没有准备好，这时候不会block进程。但是，当kernel中数据准备好的时候，recvfrom会将数据从kernel拷贝到用户内存中，这个时候进程是被block了，在这段时间内，进程是被block的。而asynchronous IO则不一样，当进程发起IO 操作之后，就直接返回再也不理睬了，直到kernel发送一个信号，告诉进程说IO完成。在这整个过程中，进程完全没有被block。I/O 多路复用之select、poll、epoll详解select，poll，epoll都是IO多路复用的机制。I/O多路复用就是通过一种机制，一个进程可以监视多个描述符，一旦某个描述符就绪（一般是读就绪或者写就绪），能够通知程序进行相应的读写操作。但select，poll，epoll本质上都是同步I/O，因为他们都需要在读写事件就绪后自己负责进行读写，也就是说这个读写过程是阻塞的，而异步I/O则无需自己负责进行读写，异步I/O的实现会负责把数据从内核拷贝到用户空间。（这里啰嗦下）SELECTint select (int n, fd_set *readfds, fd_set *writefds, fd_set *exceptfds, struct timeval *timeout);select 函数监视的文件描述符分3类，分别是writefds、readfds、和exceptfds。调用后select函数会阻塞，直到有描述副就绪（有数据 可读、可写、或者有except），或者超时（timeout指定等待时间，如果立即返回设为null即可），函数返回。当select函数返回后，可以 通过遍历fdset，来找到就绪的描述符。select目前几乎在所有的平台上支持，其良好跨平台支持也是它的一个优点。select的一 个缺点在于单个进程能够监视的文件描述符的数量存在最大限制，在Linux上一般为1024，可以通过修改宏定义甚至重新编译内核的方式提升这一限制，但 是这样也会造成效率的降低。POLLint poll (struct pollfd *fds, unsigned int nfds, int timeout);struct pollfd { int fd; /* file descriptor */ short events; /* requested events to watch */ short revents; /* returned events witnessed */};不同与select使用三个位图来表示三个fdset的方式，poll使用一个 pollfd的指针实现。pollfd结构包含了要监视的event和发生的event，不再使用select“参数-值”传递的方式。同时，pollfd并没有最大数量限制（但是数量过大后性能也是会下降）。 和select函数一样，poll返回后，需要轮询pollfd来获取就绪的描述符。从上面看，select和poll都需要在返回后，通过遍历文件描述符来获取已经就绪的socket。事实上，同时连接的大量客户端在一时刻可能只有很少的处于就绪状态，因此随着监视的描述符数量的增长，其效率也会线性下降。EPOLLepoll是在2.6内核中提出的，是之前的select和poll的增强版本。相对于select和poll来说，epoll更加灵活，没有描述符限制。epoll使用一个文件描述符管理多个描述符，将用户关系的文件描述符的事件存放到内核的一个事件表中，这样在用户空间和内核空间的copy只需一次。操作过程epoll操作过程需要三个接口，分别如下：int epoll_create(int size)；//创建一个epoll的句柄，size用来告诉内核这个监听的数目一共有多大int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event)；int epoll_wait(int epfd, struct epoll_event * events, int maxevents, int timeout); int epoll_create(int size);创建一个epoll的句柄，size用来告诉内核这个监听的数目一共有多大，这个参数不同于select()中的第一个参数，给出最大监听的fd+1的值，参数size并不是限制了epoll所能监听的描述符最大个数，只是对内核初始分配内部数据结构的一个建议。当创建好epoll句柄后，它就会占用一个fd值，在linux下如果查看/proc/进程id/fd/，是能够看到这个fd的，所以在使用完epoll后，必须调用close()关闭，否则可能导致fd被耗尽。 int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event)；函数是对指定描述符fd执行op操作。 epfd：是epoll_create()的返回值。 op：表示op操作，用三个宏来表示：添加EPOLL_CTL_ADD，删除EPOLL_CTL_DEL，修改EPOLL_CTL_MOD。分别添加、删除和修改对fd的监听事件。 fd：是需要监听的fd（文件描述符） epoll_event：是告诉内核需要监听什么事，struct epoll_event结构如下： struct epoll_event { __uint32_t events; /* Epoll events */ epoll_data_t data; /* User data variable */};//events可以是以下几个宏的集合：EPOLLIN ：表示对应的文件描述符可以读（包括对端SOCKET正常关闭）；EPOLLOUT：表示对应的文件描述符可以写；EPOLLPRI：表示对应的文件描述符有紧急的数据可读（这里应该表示有带外数据到来）；EPOLLERR：表示对应的文件描述符发生错误；EPOLLHUP：表示对应的文件描述符被挂断；EPOLLET： 将EPOLL设为边缘触发(Edge Triggered)模式，这是相对于水平触发(Level Triggered)来说的。EPOLLONESHOT：只监听一次事件，当监听完这次事件之后，如果还需要继续监听这个socket的话，需要再次把这个socket加入到EPOLL队列里 int epoll_wait(int epfd, struct epoll_event * events, int maxevents, int timeout);等待epfd上的io事件，最多返回maxevents个事件。参数events用来从内核得到事件的集合，maxevents告之内核这个events有多大，这个maxevents的值不能大于创建epoll_create()时的size，参数timeout是超时时间（毫秒，0会立即返回，-1将不确定，也有说法说是永久阻塞）。该函数返回需要处理的事件数目，如返回0表示已超时。工作模式epoll对文件描述符的操作有两种模式：LT（level trigger）和ET（edge trigger）。LT模式是默认模式，LT模式与ET模式的区别如下：　　LT模式：当epoll_wait检测到描述符事件发生并将此事件通知应用程序，应用程序可以不立即处理该事件。下次调用epoll_wait时，会再次响应应用程序并通知此事件。　　ET模式：当epoll_wait检测到描述符事件发生并将此事件通知应用程序，应用程序必须立即处理该事件。如果不处理，下次调用epoll_wait时，不会再次响应应用程序并通知此事件。 LT模式LT(level triggered)是缺省的工作方式，并且同时支持block和no-block socket.在这种做法中，内核告诉你一个文件描述符是否就绪了，然后你可以对这个就绪的fd进行IO操作。如果你不作任何操作，内核还是会继续通知你的。 ET模式ET(edge-triggered)是高速工作方式，只支持no-block socket。在这种模式下，当描述符从未就绪变为就绪时，内核通过epoll告诉你。然后它会假设你知道文件描述符已经就绪，并且不会再为那个文件描述符发送更多的就绪通知，直到你做了某些操作导致那个文件描述符不再为就绪状态了(比如，你在发送，接收或者接收请求，或者发送接收的数据少于一定量时导致了一个EWOULDBLOCK 错误）。但是请注意，如果一直不对这个fd作IO操作(从而导致它再次变成未就绪)，内核不会发送更多的通知(only once) ET模式在很大程度上减少了epoll事件被重复触发的次数，因此效率要比LT模式高。epoll工作在ET模式的时候，必须使用非阻塞套接口，以避免由于一个文件句柄的阻塞读/阻塞写操作把处理多个文件描述符的任务饿死。while(rs){ buflen = recv(activeevents[i].data.fd, buf, sizeof(buf), 0); if(buflen &amp;lt; 0){ // 由于是非阻塞的模式,所以当errno为EAGAIN时,表示当前缓冲区已无数据可读 // 在这里就当作是该次事件已处理处. if(errno == EAGAIN){ break; } else{ return; } } else if(buflen == 0){ // 这里表示对端的socket已正常关闭. } if(buflen == sizeof(buf){ rs = 1; // 需要再次读取 } else{ rs = 0; }}Linux中的EAGAIN含义Linux环境下开发经常会碰到很多错误(设置errno)，其中EAGAIN是其中比较常见的一个错误(比如用在非阻塞操作中)。从字面上来看，是提示再试一次。这个错误经常出现在当应用程序进行一些非阻塞(non-blocking)操作(对文件或socket)的时候。例如，以 O_NONBLOCK的标志打开文件/socket/FIFO，如果你连续做read操作而没有数据可读。此时程序不会阻塞起来等待数据准备就绪返回，read函数会返回一个错误EAGAIN，提示你的应用程序现在没有数据可读请稍后再试。又例如，当一个系统调用(比如fork)因为没有足够的资源(比如虚拟内存)而执行失败，返回EAGAIN提示其再调用一次(也许下次就能成功)。代码延时#define IPADDRESS &quot;127.0.0.1&quot;#define PORT 8787#define MAXSIZE 1024#define LISTENQ 5#define FDSIZE 1000#define EPOLLEVENTS 100listenfd = socket_bind(IPADDRESS,PORT);struct epoll_event events[EPOLLEVENTS];//创建一个描述符epollfd = epoll_create(FDSIZE);//添加监听描述符事件add_event(epollfd,listenfd,EPOLLIN);//循环等待for ( ; ; ){ //该函数返回已经准备好的描述符事件数目 ret = epoll_wait(epollfd,events,EPOLLEVENTS,-1); //处理接收到的连接 handle_events(epollfd,events,ret,listenfd,buf);}//事件处理函数static void handle_events(int epollfd,struct epoll_event *events,int num,int listenfd,char *buf){ int i; int fd; //进行遍历;这里只要遍历已经准备好的io事件。num并不是当初epoll_create时的FDSIZE。 for (i = 0;i &amp;lt; num;i++) { fd = events[i].data.fd; //根据描述符的类型和事件类型进行处理 if ((fd == listenfd) &amp;amp;&amp;amp;(events[i].events &amp;amp; EPOLLIN)) handle_accpet(epollfd,listenfd); else if (events[i].events &amp;amp; EPOLLIN) do_read(epollfd,fd,buf); else if (events[i].events &amp;amp; EPOLLOUT) do_write(epollfd,fd,buf); }}//添加事件static void add_event(int epollfd,int fd,int state){ struct epoll_event ev; ev.events = state; ev.data.fd = fd; epoll_ctl(epollfd,EPOLL_CTL_ADD,fd,&amp;amp;ev);}//处理接收到的连接static void handle_accpet(int epollfd,int listenfd){ int clifd; struct sockaddr_in cliaddr; socklen_t cliaddrlen; clifd = accept(listenfd,(struct sockaddr*)&amp;amp;cliaddr,&amp;amp;cliaddrlen); if (clifd == -1) perror(&quot;accpet error:&quot;); else { printf(&quot;accept a new client: %s:%d\\n&quot;,inet_ntoa(cliaddr.sin_addr),cliaddr.sin_port); //添加一个客户描述符和事件 add_event(epollfd,clifd,EPOLLIN); } }//读处理static void do_read(int epollfd,int fd,char *buf){ int nread; nread = read(fd,buf,MAXSIZE); if (nread == -1) { perror(&quot;read error:&quot;); close(fd); //记住close fd delete_event(epollfd,fd,EPOLLIN); //删除监听 } else if (nread == 0) { fprintf(stderr,&quot;client close.\\n&quot;); close(fd); //记住close fd delete_event(epollfd,fd,EPOLLIN); //删除监听 } else { printf(&quot;read message is : %s&quot;,buf); //修改描述符对应的事件，由读改为写 modify_event(epollfd,fd,EPOLLOUT); } }//写处理static void do_write(int epollfd,int fd,char *buf) { int nwrite; nwrite = write(fd,buf,strlen(buf)); if (nwrite == -1){ perror(&quot;write error:&quot;); close(fd); //记住close fd delete_event(epollfd,fd,EPOLLOUT); //删除监听 }else{ modify_event(epollfd,fd,EPOLLIN); } memset(buf,0,MAXSIZE); }//删除事件static void delete_event(int epollfd,int fd,int state) { struct epoll_event ev; ev.events = state; ev.data.fd = fd; epoll_ctl(epollfd,EPOLL_CTL_DEL,fd,&amp;amp;ev);}//修改事件static void modify_event(int epollfd,int fd,int state){ struct epoll_event ev; ev.events = state; ev.data.fd = fd; epoll_ctl(epollfd,EPOLL_CTL_MOD,fd,&amp;amp;ev);}//注：另外一端我就省了参考 https://segmentfault.com/a/1190000003063859" }, { "title": "秒杀系统设计", "url": "/posts/seckill-design/", "categories": "cs, system-design", "tags": "seckill", "date": "2021-09-26 00:00:00 +0800", "snippet": "表结构设计商品表 commodity_info活动表 seckill_info库存表（使用单独字段lock表示下单完毕的订单数）stock_info订单表 order_info商家：select commodity_infoinsert seckill_infoinsert stock_info用户：select seckill_info &amp;amp; commodity_info &amp;amp; stock_infoinsert order_infoupdate stock_info扣减库存：START TRANSACTION;SELECT stock FROM stock_infoWHERE commodity_id = xx AND sexkill_id = yy FOR UPDATE;UPDATE stock_info SET stock = stock -1WHERE commodity_id = xx AND seckill_id = yy AND stock &amp;gt; 0;TRANSACTION COMMIT;使用事务解决订单表，update操作（for update）使用stack &amp;gt; 0的where条件解决库存超卖的问题redis设计使用redis缓存库存数使用lua脚本解决cas（check and set）问题，即原子操作库存减少如果此时达到mysql的压力仍然很大，考虑使用消息队列削峰消息队列使用如果消息队列投递失败，如何解决？一个可行的解决方案：Redis中的库存比实际库存大一些，比如1.5倍到2倍在下单过程中double check库存数库存扣减时机下单后锁定库存，支付成功后，减库存如何限购使用Redis数据校验使用Redis提供的集合数据结构，将扣减Redis库存的用户id写入SADD KEY UID1SISMEMBNER KEY UID1Redis校验完毕后，再在Mysql中做订单uid&amp;amp;活动id的唯一校验付款和减库存的数据一致性-分布式事务三阶段提交，有超时机制保证多个存在不同数据库的数据操作，要么同时成功，要么同时失败，主要用于强一致性的保证。Scale扩展当库存扣减完毕后，可以直接拒绝后面的请求前端资源静态化（CDN）前端限流，点击一次后，按钮短时间置灰部分请求直接跳转到繁忙页未开始抢购时，禁用抢购按钮如何计算倒计时？前端轮训（Poll）服务器的时间，并获取时差秒杀服务挂掉，怎么办？尽量不要影响其他服务，尤其是非秒杀商品的正常购买服务雪崩，服务A挂掉导致后面依赖的服务都不可用？服务熔断，熔断机制是应对雪崩效应的一种微服务链路保护机制，快速返回错误响应Netflix HystrixAlibaba Sentinel防止恶意刷请求或者爬虫请求验证码机制限流机制 是否来自同一个ip地址，是否来自同一个用户id黑名单机制 黑名单ip 黑名单用户id参考 https://www.jiuzhang.com/course/77/dialog/#chapter-612_1" }, { "title": "基于redis的限流系统设计", "url": "/posts/rate-limit-design/", "categories": "cs, system-design", "tags": "rate-limit", "date": "2021-09-26 00:00:00 +0800", "snippet": "概念限流是对系统的出入流量进行控制，防止大流量出入，导致资源不足，系统不稳定。限流系统是对资源访问的控制组件，控制主要的两个功能：限流策略和熔断策略，对于熔断策略，不同的系统有不同的熔断策略诉求，有的系统希望直接拒绝、有的系统希望排队等待、有的系统希望服务降级、有的系统会定制自己的熔断策略，很难一一列举，所以本文只针对限流策略这个功能做详细的设计。针对找出超出速率阈值的请求这个功能，限流系统中有两个基础概念：资源和策略。资源 ：或者叫稀缺资源，被流量控制的对象；比如写接口、外部商户接口、大流量下的读接口策略 ：限流策略由限流算法和可调节的参数两部分组成限流算法限制瞬时并发数限制时间窗最大请求数令牌桶限制瞬时并发数定义：瞬时并发数，系统同时处理的请求/事务数量优点：这个算法能够实现控制并发数的效果缺点：使用场景比较单一，一般用来对入流量进行控制限制时间窗最大请求数定义：时间窗最大请求数，指定的时间范围内允许的最大请求数优点：这个算法能够满足绝大多数的流控需求，通过时间窗最大请求数可以直接换算出最大的QPS（QPS = 请求数/时间窗）缺点：这种方式可能会出现流量不平滑的情况，时间窗内一小段流量占比特别大令牌桶算法描述假如用户配置的平均发送速率为r，则每隔1/r秒一个令牌被加入到桶中假设桶中最多可以存放b个令牌。如果令牌到达时令牌桶已经满了，那么这个令牌会被丢弃当流量以速率v进入，从桶中以速率v取令牌，拿到令牌的流量通过，拿不到令牌流量不通过，执行熔断逻辑属性长期来看，符合流量的速率是受到令牌添加速率的影响，被稳定为：r因为令牌桶有一定的存储量，可以抵挡一定的流量突发情况 M是以字节/秒为单位的最大可能传输速率。 M&amp;gt;rT max = b/(M-r) 承受最大传输速率的时间B max = T max * M 承受最大传输速率的时间内传输的流量优点：流量比较平滑，并且可以抵挡一定的流量突发情况因为我们限流系统的实现就是基于令牌桶这个算法，具体的代码实现参考下文。参考 https://my.oschina.net/lyyjason/blog/1608213" }, { "title": "运算符优先级", "url": "/posts/operator-precedence/", "categories": "cs", "tags": "operator-precedence", "date": "2021-09-26 00:00:00 +0800", "snippet": "写下这篇文章缘于我最近在使用Lua时，有一次在表达式中无意识地使用括号，这种对括号的泛滥使用是一种偷懒，我尝试回忆起学生时代中在程序设计语言的课程上的一个重要主题，即运算符优先级。本文针对一个简单的语句，在两个相似的程序设计语言中——一个高级语言Lua一个低级语言C——展开讨论，通过查询官方手册，我们很容易找到答案。Lua中的运算符优先级在Lua中写如下表达式if not a or not b它应该等价于下列语句中的哪一种if (not a) or (not b)if (not (a or (not b))通过参考文档：Operator precedence in Lua follows the table below, from lower to higher priority:orand&amp;lt; &amp;gt; &amp;lt;= &amp;gt;= ~= ==|~&amp;amp;&amp;lt;&amp;lt; &amp;gt;&amp;gt;..+ -* / // %unary operators (not # - ~)As usual, you can use parentheses to change the precedences of an expression. The concatenation (&#39;..&#39;) and exponentiation (&#39;^&#39;) operators are right associative. All other binary operators are left associative.https://www.lua.org/manual/5.3/manual.html显然if not a or not b等价于if (not a) or (not b)C中的运算符优先级if (! a || ! b)它应该等价于下列语句中的哪一种if (! (a || !b)if ((! a) || (! b))C 运算符的优先级和结合性将影响表达式中操作数的分组和计算。 仅当存在优先级较高或较低的其他运算符时，运算符的优先级才有意义。 首先计算带优先级较高的运算符的表达式。 也可以通过“绑定”一词描述优先级。优先级较高的运算符被认为具有更严格的绑定。 下表总结了 C 运算符的优先级和结合性（计算操作数的顺序），并按照从最高优先级到最低优先级的顺序将其列出。 如果几个运算符一起出现，则其具有相同的优先级并且将根据其结合性对其进行计算。 以后缀运算符开头的部分描述了表中的运算符。 此部分的其余部分提供了有关优先级和结合性的常规信息。C 运算符的优先级和结合性Symbol1运算类型结合性[ ] ( ) .–&amp;gt; 后缀 ++ 和后缀 ––表达式从左到右前缀 ++ 和前缀 –– sizeof &amp;amp; * + – ~ !一元从右到左typecasts一元从右到左* / %乘法从左到右+ –加法从左到右&amp;lt;&amp;lt; &amp;gt;&amp;gt;按位移动从左到右&amp;lt; &amp;gt; &amp;lt;= &amp;gt;=关系从左到右== !=相等从左到右&amp;amp;按位“与”从左到右^按位“异或”从左到右|按位“与或”从左到右&amp;amp;&amp;amp;逻辑“与”从左到右||逻辑“或”从左到右?:条件表达式从右到左= *= /= %= += –= &amp;lt;&amp;lt;= &amp;gt;&amp;gt;=&amp;amp;= ^= |=简单和复合 assignment2从右到左,顺序计算从左到右 运算符按优先级的降序顺序列出。 如果多个运算符出现在同一行或一个组中，则它们具有相同的优先级。 所有简单的和复合的赋值运算符都有相同的优先级。https://msdn.microsoft.com/zh-cn/library/2bxt6kc4.aspx因为 ! 运算符优先级比 || 高所以if (! a || ! b)等价于if ((! a) || (! b))" }, { "title": "nsq 笔记", "url": "/posts/nsq-learning/", "categories": "cs", "tags": "nsq", "date": "2021-09-26 00:00:00 +0800", "snippet": "NSQ源码分析1 http://zhengjianglong.cn/2018/02/18/nsq/NSQ%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90(1)–%E8%AE%BE%E8%AE%A1%E7%90%86%E5%BF%B5%E6%B5%85%E6%9E%90/在设计的时候为了考虑合理的权衡，NSQ有以下特点： 支持消息内存队列的大小设置，默认完全持久化（值为0），消息即可持久到磁盘也可以保存在内存中。 保证消息至少传递一次,以确保消息可以最终成功发送。 kafka则提供了三种事务选择。 收到的消息是无序的, 实现了松散订购. kafka保证了消息有序性 发现服务nsqlookupd具有最终一致性,消息最终能够找到所有Topic生产者 没有复制 ——不像其他的队列组件，NSQ并没有提供任何形式的复制和集群，也正是这点让它能够如此简单地运行，但它确实对于一些高保证性高可靠性的消息发布没有足够的保证。 没有严格的顺序 ——虽然Kafka由一个有序的日志构成，但NSQ不是。消息可以在任何时间以任何顺序进入队列。消息传递担保NSQ保证消息将交付至少一次，虽然消息可能是重复的。这个担保是作为协议和工作流的一部分，工作原理如下： 客户表示已经准备好接收消息 NSQ 发送一条消息，并暂时将数据存储在本地（在 re-queue 或 timeout） 客户端回复 FIN（结束）或 REQ（重新排队）分别指示成功或失败。如果客户端没有回复, NSQ 会在设定的时间超时，自动重新排队消息。这确保了消息丢失唯一可能的情况是不正常结束 nsqd 进程，在内存中的任何信息（或任何缓冲未刷新到磁盘）都将丢失。一种解决方案是构成冗余 nsqd对（在不同的主机上）接收消息的相同部分的副本。Topic 和Channel一个Topic（话题）对应多个Channel(通道)每个通道都接收到一个话题中所有消息的拷贝。Topic和Chanel都不是预先配置的，话题由第一次发布消息到命名的Topic或第一次通过订阅一个命名Topic来创建。Channel被第一次订阅到指定的通道创建。一个通道一般会有多个客户端连接。假设所有已连接的客户端处于准备接收消息的状态，每个消息将被传递到一个随机的客户端.消除单点故障NSQ采用push的方式将消息数据推送到客户端，而不是像Kafka一样等待客户端的拉取，这样实现的好处就是最大限度地提高性能和吞吐量的。这个概念，称之为RDY状态，基本上是客户端流量控制的一种形式。客户端的消息消费进度由NSQ来控制，通过RDY状态记录客户端消息读取进度： 当客户端连接到 nsqd 和并订阅到一个通道时，它被放置在一个 RDY 为 0 状态(未发送信息)。 当客户端已准备好接收消息发送，更新它的命令RDY状态到它准备处理的数量，比如100。当 100 条消息可用时，将被传递到客户端。 服务器端为那个客户端每次递减 RDY 计数。心跳和超时NSQ采用push方式将消息传递给消费者，因此需要保证客户端在线。NSQ通过心跳机制和超时检查观察客户端情况，每隔一段时间，nsqd将想消费者发送一个心跳线连接。当检测到一个致命错误，客户端连接被强制关闭。在传输中的消息会超时而重新排队等待传递到另一个消费者。最后，错误会被记录并累计到各种内部指标。NSQ源码分析2 http://zhengjianglong.cn/2018/02/20/nsq/NSQ%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90(2)-nsqlookup%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3/疑问 为什么NSQlookupd提供了查询producers的接口 GET /lookup （返回某个topic下所有的producers），但是NSQadmin没有暴露出来呢？ producers究竟指的是nsqd还是nsqd上的连接，从NSQadmin上的限制来看，是指的nsqd 一个topic可以对应多个producer探索 go-svc是一个开源组件：svc采用了模板设计模式在负责初始化、启动、关闭进程。 这些初始化Init、启动Start和关闭Stop方法通过接口定义，交给具体进程去实现 尝试使用go-svc来实现 nsqlookupd的http服务路由使用的是开源框架httprouter；httprouter路由使用radix树来存储路由信息，路由查找上效率高，同时提供一些其他优秀特性，因此很受欢迎，gin web框架使用的就是httprouter路由；在这个函数中定义了每个接口，以及这个接口对应处理函数。扩展阅读kafka消息队列 http://zhengjianglong.cn/2018/04/14/kafka/kafka%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90(1)–%E6%A6%82%E8%BF%B0/分布式实时消息平台NSQ https://zhuanlan.zhihu.com/p/37081073" }, { "title": "jwt 笔记", "url": "/posts/jwt-note/", "categories": "cs", "tags": "jwt", "date": "2021-09-26 00:00:00 +0800", "snippet": "跨域认证的问题互联网服务离不开用户认证。一般流程是下面这样。 1、用户向服务器发送用户名和密码。 2、服务器验证通过后，在当前对话（session）里面保存相关数据，比如用户角色、登录时间等等。 3、服务器向用户返回一个 session_id，写入用户的 Cookie。 4、用户随后的每一次请求，都会通过 Cookie，将 session_id 传回服务器。 5、服务器收到 session_id，找到前期保存的数据，由此得知用户的身份。这种模式的问题在于，扩展性（scaling）不好。单机当然没有问题，如果是服务器集群，或者是跨域的服务导向架构，就要求 session 数据共享，每台服务器都能够读取 session。举例来说，A 网站和 B 网站是同一家公司的关联服务。现在要求，用户只要在其中一个网站登录，再访问另一个网站就会自动登录，请问怎么实现？一种解决方案是 session 数据持久化，写入数据库或别的持久层。各种服务收到请求后，都向持久层请求数据。这种方案的优点是架构清晰，缺点是工程量比较大。另外，持久层万一挂了，就会单点失败。另一种方案是服务器索性不保存 session 数据了，所有数据都保存在客户端，每次请求都发回服务器。JWT 就是这种方案的一个代表。JWT 的原理JWT 的原理是，服务器认证以后，生成一个 JSON 对象，发回给用户，就像下面这样。{ “姓名”: “张三”, “角色”: “管理员”, “到期时间”: “2018年7月1日0点0分”}以后，用户与服务端通信的时候，都要发回这个 JSON 对象。服务器完全只靠这个对象认定用户身份。为了防止用户篡改数据，服务器在生成这个对象的时候，会加上签名（详见后文）。服务器就不保存任何 session 数据了，也就是说，服务器变成无状态了，从而比较容易实现扩展。对比实际应用中，JWT的缺陷较大一个JWT大概长这样：base64(header).base64(json payload).signatureheader部分描述一些基本信息，比如这个token是用什么算法签名的，是什么版本的等等。payload就是一个json object。你可以任意放置你想要的信息，只要符合json的格式即可。标准中已经规定好了有一些字段的意思，比如iat表示issue at，token签发的时间；exp表示token过期的时间等等。根据这些约定就可以实现一些小的代码库来检查比如token是不是过期了等等。但是请注意，很多人误解，认为JWT是加了密的，但其实payload是明文的。signature是一个签名。服务器端可以自行选择一个算法和一个secret，与payload拼接上，得到一个签名。secret并不会在网络中传输，所以客户端无法伪造一个JWT。这样，一旦一个签名生成，再传回给服务器，服务器就可以知道这个token是不是它当初生成的。通过这样的机制，JWT中可以存储一些认证必要的信息。给定一个JWT，服务器只要验证：这个JWT的签名是对的这个JWT还在生效（即当前时间在JWT生效时刻之后，在失效时刻之前）之后服务器就可以信任这个JWT中包含的信息，包括user id、包含的权限等等。服务器不需要自己再去查询一遍这个用户的信息，以及这个用户的权限信息，就可以对请求作出相应。不用session了，无状态大法好！然而，需要泼一下冷水的是：使用了JWT，无法实现在服务器端对用户请求进行管理——管理员没法统计多少个人登录了，一个人登录了多少次，登陆了什么设备；同时，也无法强行“踢”掉一个用户的登录——JWT一旦生成，在失效之前，总是有效的。如果实现了一个token黑名单之类的功能，就等价于实现了Session机制，无状态带来的好处就无从谈起。这个限制对于任何一个要认真做用户风险控制的网站来说都是不可能接受的。使用了JWT，无法很好的控制payload的数据量。尽管规范表示，应该只把认证的相关信息放到payload里。但实际上，开发人员往往会误用，把几乎所有和user相关的数据都放到payload里。而payload的尺寸过大，比如达到数KB，就会极大的损耗带宽和IO性能。要记得，为了达成“无状态”，每个请求都必须把全量的JWT都带着……这两个严重的缺陷限定了JWT只能用到一些不太认真的场景。而对于真正的社交、金融、游戏等认真一点的服务，还是要选择基于Session的认证。当然，token中的签名还是有好处的，签名可以确保token的确是服务器产生的，不会被篡改。如果token中包含了user id，那么还可以实现简单的前端错误上报；如果token中还有session id，就可以在服务器端实现基于Session的认证。因此，你可以将user id、session id、token过期时间等几个关键数据放到payload里——只放这几个，不放其他的数据，得到一个用来做Session认证的JWT。更进一步，如果你把JWT的规范稍微小改一下，比如payload不用JSON，而是更紧凑的格式；定死了签名算法，即可省略JWT的header了；最后再优化一下编码格式，就能得到一个你自己的token。但，无论用session还是token，还是什么其他的名字，这些都不重要。重要的是服务器这边必须实现session机制，以便于对用户登录信息进行有效的管理。安全XSS使用基于Token认证的开发人员很喜欢使用Header + Local Storage。因为这样可以有效防止CSRF （下一小节专门讲）。但是使用Local Storage，反而会增加中招XSS（Crossing Site Script）的机会。一旦中招XSS，攻击者可以轻易的拿到认证信息，并且传回自己的接受网址而不被用户察觉。这样一来攻击者能够轻易的代替用户登录了。整个浏览器中，只有一种资源是脚本无法访问到的。这就是被设置为HttpOnly的cookie。这是非常理想的放置认证token/session id的地方。设置这种token只需要在Set Cookie时这么写： Set-Cookie: access_token=xxxxxxxxxxxxxxxxxx; HttpOnly; Secure; Same-Site=strict; Path=/;(Secure和Same-Site是什么？下文会解释)XSS攻击者没有任何办法从HttpOnly的Cookie中拿到你的认证信息，除非他能在你登录网站后，直接进入你的电脑，打开浏览器的开发者工具并人肉复制粘贴CSRFCSRF代表Crossing Site Recource Forge。大致的触发流程是：用户登录了站点A，并且在Cookie中留下了A站点的认证信息用户进入了站点B，而站点B用一些方式（比如一个提交行为是到A站点某关键接口的表单）引诱用户去点击。当用户点击时，会发出到A站点的请求。而浏览器会给这个请求附带上A站点的认证信息，从而让这个请求能够执行。这种行为可能是，但不限于，给某个A站点的某个其他用户提权/转账/发文辱骂等等。上文中提到了，很多人用JWT+Local Storage的本心是为了防护CRSF。这样做的原因是——因为Cookie的发送是完全由浏览器控制的，不受网页本身的控制。所以最简单直接的办法，就是不用Cookie，不让自动发送认证信息成为可能。问题在于，这么干是有XSS风险的。从上文中可以看到，为了避免XSS，就必须用HttpOnlyCookie。XSS对比CSRF参考 http://www.ruanyifeng.com/blog/2018/07/json_web_token-tutorial.html https://blog.nowcoder.net/n/7a6a4758eef4444ea7ea301640cb46db" }, { "title": "http3 笔记1", "url": "/posts/http3-note/", "categories": "cs", "tags": "http, http2, http3", "date": "2021-09-26 00:00:00 +0800", "snippet": "HTTP1.x和HTTP2HTTPS通信时间总和 = TCP连接时间 + TLS 连接时间 + HTTP交易时间 = 1.5 RTT + 1.5 RTT + 1 RTT = 4 RTTQUICIP / UDP / QUIC这个就是Google开发QUIC协议，QUIC协议集成了TCP可靠传输机制、TLS安全加密、HTTP /2 流量复用技术，其页面的加载时间为2.5 RTT时间。此外，完成QUIC交易的连接的Session ID会缓存在浏览器内存里，如果用户再次打开该页面，无需建立TLS连接，直接使用缓存Session ID 对应的加密参数，服务器可以根据Session ID在缓存里查找对应的加密参数，并完成加密。换句话说，重连TLS连接是一个0 RTT 事件，用户所要等待的页面加载事件 = HTTP交易事件 = 1 RTT。HTTP /3这一次IETF又觉得QUIC是一个好东西，但是希望QUIC不仅可以运输HTTP，还可以运输其它协议，把QUIC与HTTP分离，最终各合伙人的占位如下所示：IP / UDP / QUIC / HTTP这样整体的页面加载时间为2 RTT。TLS 1.3IETF的QUIC标准集成了TLS 1.3版本，1.3版本更简练，建立TLS连接不再需要1.5 RTT，而只需要1 RTT，是因为浏览器第一次就把自己的密钥交换的素材发给服务器，这样就节省了第三次消息，少了0.5个RTT时间。页面的整体加载时间 = TLS 1.3连接时间 + HTTP交易时间 = 1RTT + 1RTT = 2 RTT重连页面的加载时间 = HTTP交易时间 = 1 RTT上文协议的进化过程就是人类与RTT斗争史，目标是减少用户等待页面加载时间、同时保证用户看到的页面安全，没有在传输过程中被偷窥、篡改。HTTP /3所带来的挑战HTTP /3所带来的挑战99%+以上的手机移动终端、电脑终端，都使用私有IP，都需要NAT设备来完成私有IP与全球IP的转换。这意味着NAT设备通常会记忆用户的通信状态，一旦用户完成了通信，NAT设备会释放这些记忆。对于基于TCP的HTTP、HTTPS传输，NAT设备可以根据TCP报文头的SYN / FIN状态位，知道通信什么时候开始，什么时候结束，对应记忆的开始、记忆的结束。但是基于UDP传输的HTTP/3，NAT设备收到流量会知道连接什么时候开始，但是却无法知道流量什么时候结束。NAT设备的记忆如果短于用户会话时间，则用户会话会中断。NAT设备的记忆如果大大长于用户会话时间，则意味着NAT设备的端口资源会白白被占用！最直接的解决方案是，在QUIC的头部模仿TCP的SYN/FIN状态，让沿途的NAT设备知道会话什么时候开始、什么时候结束。但这需要升级全球所有的NAT设备的软件！另外一个可行的方案是，让QUIC周期性地发送Keepalive消息，刷新NAT设备的记忆，避免NAT设备释放自己的记忆。参考 https://zhuanlan.zhihu.com/p/143464334https://www.zhihu.com/question/302412059/answer/533223530" }, { "title": "http2 笔记1", "url": "/posts/http2-note/", "categories": "cs, network", "tags": "http2", "date": "2021-09-26 00:00:00 +0800", "snippet": "header压缩我们平常听到的GZIP压缩仅仅是针对HTTP请求的Body部分进行的，这只能算是半压缩。对于很多API服务来说，返回的内容体其实并不大，这个时候请求头就占据了大部分流量。HTTP2将魔爪伸到了HTTP头部，这回是彻底的对整个请求都进行压缩了。HTTP2的头压缩原理完全不同于HTTP1.1，它将常用的HEADER键值对映射到一个静态表里面的索引值，于是很多头部的键值对使用一个位置索引来表示就可以了。这样便大大节省了头部消息的长度。对于那些不常用的自定义的头部会使用一个动态表来维护，具体原理有一定复杂度，这里就不再啰嗦了。server pushServer Push不同于Websocket，Server Push一般是指服务器主动向客户端推送数据，这是一种单向的主动推送，而WebSocket是双向的，这两种技术不是竞争关系。Server Push可以用在服务器主动向客户端推送静态资源，比如浏览器请求index.html时，服务器除了返回网页内容外，还会将index.html页面里面的各种css和js一起推送到浏览器缓存起来，当浏览器分析了网页内容发现静态资源时，不需要再去服务器请求一次，它只需要从缓存里直接拿就可以了。不过现代的网站的静态资源大多都是CDN架构的，静态资源都在第三方服务器，Server Push在这方面作用并不大。Server Push还可以用在推送通知消息，比如谁关注了你，谁给你点了赞等，这个可以替代古老的Comet技术和近几年Google推广的SPDY协议，它需要服务器维持当前的TCP通道不关闭，需要持续占用服务器资源。pipelinePipeline是指后一个HTTP请求无需等待前一个HTTP请求返回结果就可以提前发起。HTTP1.1也有Pipeline支持，但是有所不足，并行的还不够彻底。它可以提前发起请求，但是却限定了返回结果必须和收到请求的顺序保持一致而不能乱序。如果第一个请求服务器处理慢了，那么后续的返回结果客户端无法立即收到，而必须等到第一个结果全部返回了才行。HTTP2则解决了这个问题，它支持乱序返回，甚至不同请求的返回结果的分块【HTTP Chunk】也可以交叉返回而不会混乱，这种技术称之为Multiplexing【多路复用】。多路复用HTTP2协议是二进制协议，不同于HTTP1.1的文本协议。文本协议是以特殊的符号结尾【换行回车符】来分割消息的，而二进制协议是通过字节长度来分割消息。二进制协议虽然直观性不如文本协议，但是在实现的时候要简单直接一些。HTTP2为支持多路复用，在同一条TCP通道上支持发送多个资源／请求，将每条资源／请求定义为一个Stream【流】，同一个TCP通道可以传输多个Stream。同时为了支持多个资源的并行交错发送，将Stream再次分割为多个Frame【帧】，帧与帧之间可以交错发送。接收端通过流ID将这些帧组装起来，通一个流ID的帧属于同一个资源／请求。因为TCP协议已经可以保证消息包是有序的，所以接收端不必担心乱序问题。以下内容参考： https://blog.fundebug.com/2019/03/07/understand-http2-and-http3/流：流是连接中的一个虚拟信道，可以承载双向的消息；每个流都有一个唯一的整数标识符（1、2…N）；消息：是指逻辑上的 HTTP 消息，比如请求、响应等，由一或多个帧组成。帧：HTTP 2.0 通信的最小单位，每个帧包含帧首部，至少也会标识出当前帧所属的流，承载着特定类型的数据，如 HTTP 首部、负荷，等等HTTP/2 中，同域名下所有通信都在单个连接上完成，该连接可以承载任意数量的双向数据流。每个数据流都以消息的形式发送，而消息又由一个或多个帧组成。多个帧之间可以乱序发送，根据帧首部的流标识可以重新组装。在 HTTP/2 中，有了二进制分帧之后，HTTP /2 不再依赖 TCP 链接去实现多流并行了，在 HTTP/2 中：同域名下所有通信都在单个连接上完成。单个连接可以承载任意数量的双向数据流。数据流以消息的形式发送，而消息又由一个或多个帧组成，多个帧之间可以乱序发送，因为根据帧首部的流标识可以重新组装。这一特性，使性能有了极大提升：同个域名只需要占用一个 TCP 连接，使用一个连接并行发送多个请求和响应,消除了因多个 TCP 连接而带来的延时和内存消耗。并行交错地发送多个请求，请求之间互不影响。并行交错地发送多个响应，响应之间互不干扰。在 HTTP/2 中，每个请求都可以带一个 31bit 的优先值，0 表示最高优先级， 数值越大优先级越低。有了这个优先值，客户端和服务器就可以在处理不同的流时采取不同的策略，以最优的方式发送流、消息和帧。参考 https://zhuanlan.zhihu.com/p/33769711" }, { "title": "gorm 笔记", "url": "/posts/gorm-learning/", "categories": "cs", "tags": "gorm", "date": "2021-09-26 00:00:00 +0800", "snippet": "gorm在事务中避免条件污染使用Session.WithConditions = false避免tx = tx.Table(new(official.TermLevel).TableName()).Session(&amp;amp;gorm.Session{WithConditions: false})注意：此处存在bug，session会过滤所有条件，在链式中应该前置，以上面为例，假设存在分表Table无法工作tx = tx.Session(&amp;amp;gorm.Session{WithConditions: false}).Table(new(official.TermLevel).TableName())Model应该配合Update使用，待验证gorm中使用外键preloadtype Post struct { ID int64 `gorm:&quot;column:id&quot; json:&quot;id&quot; form:&quot;id&quot;` AppId string `gorm:&quot;column:app_id&quot; json:&quot;app_id&quot; form:&quot;app_id&quot;` PostAuthorName string `gorm:&quot;column:post_author_name&quot; json:&quot;post_author_name&quot; form:&quot;post_author_name&quot;` PostDate time.Time `gorm:&quot;column:post_date&quot; json:&quot;post_date&quot; form:&quot;post_date&quot;` PostStatus string `gorm:&quot;column:post_status&quot; json:&quot;post_status&quot; form:&quot;post_status&quot;` Icon string `gorm:&quot;column:icon&quot; json:&quot;icon&quot; form:&quot;icon&quot;` Desc string `gorm:&quot;column:desc&quot; json:&quot;desc&quot; form:&quot;desc&quot;` PostTitle string `gorm:&quot;column:post_title&quot; json:&quot;post_title&quot; form:&quot;post_title&quot;` PostContent string `gorm:&quot;column:post_content&quot; json:&quot;post_content&quot; form:&quot;post_content&quot;` TermRelationship []TermRelationship `gorm:&quot;ForeignKey:ObjectId;AssociationForeignKey:ID&quot;` CreatedAt time.Time `gorm:&quot;column:created_at&quot; json:&quot;created_at&quot; form:&quot;created_at&quot;` UpdatedAt time.Time `gorm:&quot;column:updated_at&quot; json:&quot;updated_at&quot; form:&quot;updated_at&quot;`}type TermRelationship struct { ID int64 `gorm:&quot;column:id&quot; json:&quot;id&quot; form:&quot;id&quot;` ObjectId int64 `gorm:&quot;column:object_id&quot; json:&quot;object_id&quot; form:&quot;object_id&quot;` TermId int64 `gorm:&quot;column:term_id&quot; json:&quot;term_id&quot; form:&quot;term_id&quot;` OrderType string `gorm:&quot;column:order_type&quot; json:&quot;order_type&quot; form:&quot;order_type&quot;` Term Term `gorm:&quot;ForeignKey:TermId;AssociationForeignKey:ID&quot;` Post Post `gorm:&quot;ForeignKey:ObjectId;AssociationForeignKey:ID&quot;` CreatedAt time.Time `gorm:&quot;column:created_at&quot; json:&quot;created_at&quot; form:&quot;created_at&quot;` UpdatedAt time.Time `gorm:&quot;column:updated_at&quot; json:&quot;updated_at&quot; form:&quot;updated_at&quot;`}type Term struct { ID int64 `gorm:&quot;column:id&quot; json:&quot;id&quot; form:&quot;id&quot;` AppId string `gorm:&quot;column:app_id&quot; json:&quot;app_id&quot; form:&quot;app_id&quot;` Taxonomy string `gorm:&quot;column:taxonomy&quot; json:&quot;taxonomy&quot; form:&quot;taxonomy&quot;` Level int64 `gorm:&quot;column:level&quot; json:&quot;level&quot; form:&quot;level&quot;` Name string `gorm:&quot;column:name&quot; json:&quot;name&quot; form:&quot;name&quot;` Icon string `gorm:&quot;column:icon&quot; json:&quot;icon&quot; form:&quot;icon&quot;` Desc string `gorm:&quot;column:desc&quot; json:&quot;desc&quot; form:&quot;desc&quot;` CreatedAt time.Time `gorm:&quot;column:created_at&quot; json:&quot;created_at&quot; form:&quot;created_at&quot;` UpdatedAt time.Time `gorm:&quot;column:updated_at&quot; json:&quot;updated_at&quot; form:&quot;updated_at&quot;`}在Post中post和term的关系存储在term_relationship中，所以应该指定外键为objectId：gorm:”ForeignKey:ObjectId;AssociationForeignKey:ID”在TermRelationship中自己的字段TermId，ObjectId也是外键type TermLevel struct { ID int64 `gorm:&quot;column:id&quot; json:&quot;id&quot; form:&quot;id&quot;` TermId int64 `gorm:&quot;column:term_id&quot; json:&quot;term_id&quot; form:&quot;term_id&quot;` ParentId int64 `gorm:&quot;column:parent_id&quot; json:&quot;parent_id&quot; form:&quot;parent_id&quot;` TermById Term `gorm:&quot;ForeignKey:TermId;AssociationForeignKey:ID&quot;` TermByParentId Term `gorm:&quot;ForeignKey:ParentId;AssociationForeignKey:ID&quot;` CreatedAt time.Time `gorm:&quot;column:created_at&quot; json:&quot;created_at&quot; form:&quot;created_at&quot;` UpdatedAt time.Time `gorm:&quot;column:updated_at&quot; json:&quot;updated_at&quot; form:&quot;updated_at&quot;`}// Preload(&quot;TermById&quot;)// Preload(&quot;TermByParentId&quot;)同时可以类似上面为TermId和ParentId指定同一个表的外键，只是需要在preload时分别加载动态表名Gorm建议在动态表名使用Scopes实现：func UserTable(user User) func (tx *gorm.DB) *gorm.DB { return func (tx *gorm.DB) *gorm.DB { if user.Admin { return tx.Table(&quot;admin_users&quot;) } return tx.Table(&quot;users&quot;) }}db.Scopes(UserTable(user)).Create(&amp;amp;user)" }, { "title": "go-svc 笔记", "url": "/posts/go-svc-learning/", "categories": "cs", "tags": "go-svc", "date": "2021-09-26 00:00:00 +0800", "snippet": "核心代码type Service interface { // Init is called before the program/service is started and after it&#39;s // determined if the program is running as a Windows Service. This method must // be non-blocking. Init(Environment) error // Start is called after Init. This method must be non-blocking. Start() error // Stop is called in response to syscall.SIGINT, syscall.SIGTERM, or when a // Windows Service is stopped. Stop() error}// Run runs your Service.//// Run will block until one of the signals specified in sig is received.// If sig is empty syscall.SIGINT and syscall.SIGTERM are used by default.func Run(service Service, sig ...os.Signal) error { env := environment{} if err := service.Init(env); err != nil { return err } if err := service.Start(); err != nil { return err } if len(sig) == 0 { sig = []os.Signal{syscall.SIGINT, syscall.SIGTERM} } signalChan := make(chan os.Signal, 1) signalNotify(signalChan, sig...) &amp;lt;-signalChan return service.Stop()}信号 2) SIGINT 程序终止(interrupt)信号, 在用户键入INTR字符(通常是Ctrl-C)时发出，用于通知前台进程组终止进程。 15) SIGTERM 程序结束(terminate)信号, 与SIGKILL不同的是该信号可以被阻塞和处理。通常用来要求程序自己正常退出，shell命令kill缺省产生这个信号。如果进程终止不了，我们才会尝试SIGKILL。分析signalChan := make(chan os.Signal, 1)signalNotify(signalChan, sig...)&amp;lt;-signalChan &amp;lt;-ch用来从channel ch中接收数据，这个表达式会一直被block，直到有数据可以接收。扩展阅读go-svc源码 https://github.com/judwhite/go-svcgo channel 详解 https://colobu.com/2016/04/14/Golang-Channels/" }, { "title": "http in go 笔记2", "url": "/posts/go-http-note-2/", "categories": "cs, network", "tags": "http, go", "date": "2021-09-26 00:00:00 +0800", "snippet": "问题一个 TCP 连接可以发多少个 HTTP 请求要搞懂这个问题，我们需要先解决下面五个问题： 0、现代浏览器在与服务器建立了一个 TCP 连接后是否会在一个 HTTP 请求完成后断开？什么情况下会断开？ 1、一个 TCP 连接可以对应几个 HTTP 请求？ 2、一个 TCP 连接中 HTTP 请求发送可以一起发送么（比如一起发三个请求，再三个响应一起接收）？ 3、为什么有的时候刷新页面不需要重新建立 SSL 连接？ 4、浏览器对同一 Host 建立 TCP 连接到数量有没有限制？问题0现代浏览器在与服务器建立了一个 TCP 连接后是否会在一个 HTTP 请求完成后断开？什么情况下会断开？ 在 HTTP/1.0 中，一个服务器在发送完一个 HTTP 响应后，会断开 TCP 链接。但是这样每次请求都会重新建立和断开 TCP 连接，代价过大。 所以虽然标准中没有设定，某些服务器对 Connection: keep-alive 的 Header 进行了支持。 意思是说，完成这个 HTTP 请求之后，不要断开 HTTP 请求使用的 TCP 连接。 这样的好处是连接可以被重新使用，之后发送 HTTP 请求的时候不需要重新建立 TCP 连接。 另外，如果维持连接，那么 SSL 的开销也可以避免。 初始化连接和 SSL 开销消失了，说明使用的是同一个 TCP 连接 持久连接：既然维持 TCP 连接好处这么多，HTTP/1.1 就把 Connection 头写进标准，并且默认开启持久连接 除非请求中写明 Connection: close，那么浏览器和服务器之间是会维持一段时间的 TCP 连接，不会一个请求结束就断掉。 所以第一个问题的答案是：默认情况下建立 TCP 连接不会断开，只有在请求报头中声明 Connection: close 才会在请求完成后关闭连接。问题1一个 TCP 连接可以对应几个 HTTP 请求？ 了解了第一个问题之后，其实这个问题已经有了答案，如果维持连接，一个 TCP 连接是可以发送多个 HTTP 请求的。问题2一个 TCP 连接中 HTTP 请求发送可以一起发送么（比如一起发三个请求，再三个响应一起接收）？ HTTP/1.1 存在一个问题，单个 TCP 连接在同一时刻只能处理一个请求 它的意思是说：两个请求的生命周期不能重叠，任意两个 HTTP 请求从开始到结束的时间在同一个 TCP 连接里不能重叠。 虽然 HTTP/1.1 规范中规定了 Pipelining 来试图解决这个问题，但是这个功能在浏览器中默认是关闭的。 先来看一下 Pipelining 是什么，RFC 2616 中规定了： A client that supports persistent connections MAY “pipeline” its requests (i.e., send multiple requests without waiting for each response). A server MUST send its responses to those requests in the same order that the requests were received. 一个支- 持持久连接的客户端可以在一个连接中发送多个请求（不需要等待任意请求的响应）。收到请求的服务器必须按照请求收到的顺序发送响应。 至于标准为什么这么设定，我们可以大概推测一个原因： 由于 HTTP/1.1 是个文本协议，同时返回的内容也并不能区分对应于哪个发送的请求，所以顺序必须维持一致。 比如你向服务器发送了两个请求 GET/query?q=A 和 GET/query?q=B，服务器返回了两个结果，浏览器是没有办法根据响应结果来判断响应对应于哪一个请求的。 Pipelining 这种设想看起来比较美好，但是在实践中会出现许多问题： 一些代理服务器不能正确的处理 HTTP Pipelining。 正确的流水线实现是复杂的。 Head-of-line Blocking 连接头阻塞：在建立起一个 TCP 连接之后，假设客户端在这个连接连续向服务器发送了几个请求，按照标准，服务器应该按照收到请求的顺序返回结果 假设服务器在处理首个请求时花费了大量时间，那么后面所有的请求都需要等着首个请求结束才能响应。 所以现代浏览器默认是不开启 HTTP Pipelining 的。 但是，HTTP2 提供了 Multiplexing 多路传输特性，可以在一个 TCP 连接中同时完成多个 HTTP 请求。 TODO: Multiplexing 具体如何实现？可以参考go http中的注释：// HTTP cannot have multiple simultaneous active requests.[*]// Until the server replies to this request, it can&#39;t read another,// so we might as well run the handler in this goroutine.// [*] Not strictly true: HTTP pipelining. We could let them all process// in parallel even if their responses need to be serialized.// But we&#39;re not going to implement HTTP pipelining because it// was never deployed in the wild and the answer is HTTP/2. 所以这个问题也有了答案：在 HTTP/1.1 存在 Pipelining 技术可以完成这个多个请求同时发送，但是由于浏- 览器默认关闭，所以可以认为这是不可行的。 在 HTTP2 中由于 Multiplexing 特点的存在，多个 HTTP 请求可以在同一个 TCP 连接中并行进行。 那么在 HTTP/1.1 时代，浏览器是如何提高页面加载效率的呢？主要有下面两点： 1、维持和服务器已经建立的 TCP 连接，在同一连接上顺序处理多个请求。 2、和服务器建立多个 TCP 连接。问题3为什么有的时候刷新页面不需要重新建立 SSL 连接？ 在第一个问题的讨论中已经有了答案：TCP 连接有的时候会被浏览器和服务端维持一段时间。TCP 不需要重新建立，SSL 自然也会用之前的。问题4浏览器对同一 Host 建立 TCP 连接到数量有没有限制？ 假设我们还处在 HTTP/1.1 时代，那个时候没有多路传输，当浏览器拿到一个有几十张图片的网页该怎么办呢？ 肯定不能只开一个 TCP 连接顺序下载，那样用户肯定等的很难受 但是如果每个图片都开一个 TCP 连接发 HTTP 请求，那电脑或者服务器都可能受不了 要是有 1000 张图片的话总不能开 1000 个TCP 连接吧，你的电脑同意 NAT 也不一定会同意。 所以答案是：有。Chrome 最多允许对同一个 Host 建立六个 TCP 连接。不同的浏览器有一些区别。 https://developers.google.com/web/tools/chrome-devtools/network/issues#queued-or-stalled-requestsdevelopers.google.com参考 https://blog.csdn.net/qq_22343483/article/details/101523185" }, { "title": "http in go 笔记1", "url": "/posts/go-http-note-1/", "categories": "cs, network", "tags": "http, go", "date": "2021-09-26 00:00:00 +0800", "snippet": "http in go从入口开始看// ListenAndServe listens on the TCP network address addr and then calls// Serve with handler to handle requests on incoming connections.// Accepted connections are configured to enable TCP keep-alives.//// The handler is typically nil, in which case the DefaultServeMux is used.//// ListenAndServe always returns a non-nil error.func ListenAndServe(addr string, handler Handler) error { server := &amp;amp;Server{Addr: addr, Handler: handler} return server.ListenAndServe()}// ListenAndServe listens on the TCP network address srv.Addr and then// calls Serve to handle requests on incoming connections.// Accepted connections are configured to enable TCP keep-alives.//// If srv.Addr is blank, &quot;:http&quot; is used.//// ListenAndServe always returns a non-nil error. After Shutdown or Close,// the returned error is ErrServerClosed.func (srv *Server) ListenAndServe() error { if srv.shuttingDown() { return ErrServerClosed } addr := srv.Addr if addr == &quot;&quot; { addr = &quot;:http&quot; } ln, err := net.Listen(&quot;tcp&quot;, addr) if err != nil { return err } return srv.Serve(ln)}// Serve accepts incoming connections on the Listener l, creating a// new service goroutine for each. The service goroutines read requests and// then call srv.Handler to reply to them.//// HTTP/2 support is only enabled if the Listener returns *tls.Conn// connections and they were configured with &quot;h2&quot; in the TLS// Config.NextProtos.//// Serve always returns a non-nil error and closes l.// After Shutdown or Close, the returned error is ErrServerClosed.func (srv *Server) Serve(l net.Listener) error { if fn := testHookServerServe; fn != nil { fn(srv, l) // call hook with unwrapped listener } origListener := l l = &amp;amp;onceCloseListener{Listener: l} defer l.Close() if err := srv.setupHTTP2_Serve(); err != nil { return err } if !srv.trackListener(&amp;amp;l, true) { return ErrServerClosed } defer srv.trackListener(&amp;amp;l, false) var tempDelay time.Duration // how long to sleep on accept failure baseCtx := context.Background() if srv.BaseContext != nil { baseCtx = srv.BaseContext(origListener) if baseCtx == nil { panic(&quot;BaseContext returned a nil context&quot;) } } ctx := context.WithValue(baseCtx, ServerContextKey, srv) for { rw, e := l.Accept() if e != nil { select { case &amp;lt;-srv.getDoneChan(): return ErrServerClosed default: } if ne, ok := e.(net.Error); ok &amp;amp;&amp;amp; ne.Temporary() { if tempDelay == 0 { tempDelay = 5 * time.Millisecond } else { tempDelay *= 2 } if max := 1 * time.Second; tempDelay &amp;gt; max { tempDelay = max } srv.logf(&quot;http: Accept error: %v; retrying in %v&quot;, e, tempDelay) time.Sleep(tempDelay) continue } return e } if cc := srv.ConnContext; cc != nil { ctx = cc(ctx, rw) if ctx == nil { panic(&quot;ConnContext returned nil&quot;) } } tempDelay = 0 c := srv.newConn(rw) c.setState(c.rwc, StateNew) // before Serve can return go c.serve(ctx) }}todo:trackListener这个方法的作用是干什么没有弄明白// Serve a new connection.func (c *conn) serve(ctx context.Context) { c.remoteAddr = c.rwc.RemoteAddr().String() ctx = context.WithValue(ctx, LocalAddrContextKey, c.rwc.LocalAddr()) defer func() { if err := recover(); err != nil &amp;amp;&amp;amp; err != ErrAbortHandler { const size = 64 &amp;lt;&amp;lt; 10 buf := make([]byte, size) buf = buf[:runtime.Stack(buf, false)] c.server.logf(&quot;http: panic serving %v: %v\\n%s&quot;, c.remoteAddr, err, buf) } if !c.hijacked() { c.close() c.setState(c.rwc, StateClosed) } }() if tlsConn, ok := c.rwc.(*tls.Conn); ok { if d := c.server.ReadTimeout; d != 0 { c.rwc.SetReadDeadline(time.Now().Add(d)) } if d := c.server.WriteTimeout; d != 0 { c.rwc.SetWriteDeadline(time.Now().Add(d)) } if err := tlsConn.Handshake(); err != nil { // If the handshake failed due to the client not speaking // TLS, assume they&#39;re speaking plaintext HTTP and write a // 400 response on the TLS conn&#39;s underlying net.Conn. if re, ok := err.(tls.RecordHeaderError); ok &amp;amp;&amp;amp; re.Conn != nil &amp;amp;&amp;amp; tlsRecordHeaderLooksLikeHTTP(re.RecordHeader) { io.WriteString(re.Conn, &quot;HTTP/1.0 400 Bad Request\\r\\n\\r\\nClient sent an HTTP request to an HTTPS server.\\n&quot;) re.Conn.Close() return } c.server.logf(&quot;http: TLS handshake error from %s: %v&quot;, c.rwc.RemoteAddr(), err) return } c.tlsState = new(tls.ConnectionState) *c.tlsState = tlsConn.ConnectionState() if proto := c.tlsState.NegotiatedProtocol; validNPN(proto) { if fn := c.server.TLSNextProto[proto]; fn != nil { h := initNPNRequest{ctx, tlsConn, serverHandler{c.server}} fn(c.server, tlsConn, h) } return } } // HTTP/1.x from here on. ctx, cancelCtx := context.WithCancel(ctx) c.cancelCtx = cancelCtx defer cancelCtx() c.r = &amp;amp;connReader{conn: c} c.bufr = newBufioReader(c.r) c.bufw = newBufioWriterSize(checkConnErrorWriter{c}, 4&amp;lt;&amp;lt;10) for { w, err := c.readRequest(ctx) if c.r.remain != c.server.initialReadLimitSize() { // If we read any bytes off the wire, we&#39;re active. c.setState(c.rwc, StateActive) } if err != nil { const errorHeaders = &quot;\\r\\nContent-Type: text/plain; charset=utf-8\\r\\nConnection: close\\r\\n\\r\\n&quot; switch { case err == errTooLarge: // Their HTTP client may or may not be // able to read this if we&#39;re // responding to them and hanging up // while they&#39;re still writing their // request. Undefined behavior. const publicErr = &quot;431 Request Header Fields Too Large&quot; fmt.Fprintf(c.rwc, &quot;HTTP/1.1 &quot;+publicErr+errorHeaders+publicErr) c.closeWriteAndWait() return case isUnsupportedTEError(err): // Respond as per RFC 7230 Section 3.3.1 which says, // A server that receives a request message with a // transfer coding it does not understand SHOULD // respond with 501 (Unimplemented). code := StatusNotImplemented // We purposefully aren&#39;t echoing back the transfer-encoding&#39;s value, // so as to mitigate the risk of cross side scripting by an attacker. fmt.Fprintf(c.rwc, &quot;HTTP/1.1 %d %s%sUnsupported transfer encoding&quot;, code, StatusText(code), errorHeaders) return case isCommonNetReadError(err): return // don&#39;t reply default: publicErr := &quot;400 Bad Request&quot; if v, ok := err.(badRequestError); ok { publicErr = publicErr + &quot;: &quot; + string(v) } fmt.Fprintf(c.rwc, &quot;HTTP/1.1 &quot;+publicErr+errorHeaders+publicErr) return } } // Expect 100 Continue support req := w.req if req.expectsContinue() { if req.ProtoAtLeast(1, 1) &amp;amp;&amp;amp; req.ContentLength != 0 { // Wrap the Body reader with one that replies on the connection req.Body = &amp;amp;expectContinueReader{readCloser: req.Body, resp: w} } } else if req.Header.get(&quot;Expect&quot;) != &quot;&quot; { w.sendExpectationFailed() return } c.curReq.Store(w) if requestBodyRemains(req.Body) { registerOnHitEOF(req.Body, w.conn.r.startBackgroundRead) } else { w.conn.r.startBackgroundRead() } // HTTP cannot have multiple simultaneous active requests.[*] // Until the server replies to this request, it can&#39;t read another, // so we might as well run the handler in this goroutine. // [*] Not strictly true: HTTP pipelining. We could let them all process // in parallel even if their responses need to be serialized. // But we&#39;re not going to implement HTTP pipelining because it // was never deployed in the wild and the answer is HTTP/2. serverHandler{c.server}.ServeHTTP(w, w.req) w.cancelCtx() if c.hijacked() { return } w.finishRequest() if !w.shouldReuseConnection() { if w.requestBodyLimitHit || w.closedRequestBodyEarly() { c.closeWriteAndWait() } return } c.setState(c.rwc, StateIdle) c.curReq.Store((*response)(nil)) if !w.conn.server.doKeepAlives() { // We&#39;re in shutdown mode. We might&#39;ve replied // to the user without &quot;Connection: close&quot; and // they might think they can send another // request, but such is life with HTTP/1.1. return } if d := c.server.idleTimeout(); d != 0 { c.rwc.SetReadDeadline(time.Now().Add(d)) if _, err := c.bufr.Peek(4); err != nil { return } } c.rwc.SetReadDeadline(time.Time{}) }}// Read next request from connection.func (c *conn) readRequest(ctx context.Context) (w *response, err error) { if c.hijacked() { return nil, ErrHijacked } var ( wholeReqDeadline time.Time // or zero if none hdrDeadline time.Time // or zero if none ) t0 := time.Now() if d := c.server.readHeaderTimeout(); d != 0 { hdrDeadline = t0.Add(d) } if d := c.server.ReadTimeout; d != 0 { wholeReqDeadline = t0.Add(d) } c.rwc.SetReadDeadline(hdrDeadline) if d := c.server.WriteTimeout; d != 0 { defer func() { c.rwc.SetWriteDeadline(time.Now().Add(d)) }() } c.r.setReadLimit(c.server.initialReadLimitSize()) if c.lastMethod == &quot;POST&quot; { // RFC 7230 section 3 tolerance for old buggy clients. peek, _ := c.bufr.Peek(4) // ReadRequest will get err below c.bufr.Discard(numLeadingCRorLF(peek)) } req, err := readRequest(c.bufr, keepHostHeader) if err != nil { if c.r.hitReadLimit() { return nil, errTooLarge } return nil, err } if !http1ServerSupportsRequest(req) { return nil, badRequestError(&quot;unsupported protocol version&quot;) } c.lastMethod = req.Method c.r.setInfiniteReadLimit() hosts, haveHost := req.Header[&quot;Host&quot;] isH2Upgrade := req.isH2Upgrade() if req.ProtoAtLeast(1, 1) &amp;amp;&amp;amp; (!haveHost || len(hosts) == 0) &amp;amp;&amp;amp; !isH2Upgrade &amp;amp;&amp;amp; req.Method != &quot;CONNECT&quot; { return nil, badRequestError(&quot;missing required Host header&quot;) } if len(hosts) &amp;gt; 1 { return nil, badRequestError(&quot;too many Host headers&quot;) } if len(hosts) == 1 &amp;amp;&amp;amp; !httpguts.ValidHostHeader(hosts[0]) { return nil, badRequestError(&quot;malformed Host header&quot;) } for k, vv := range req.Header { if !httpguts.ValidHeaderFieldName(k) { return nil, badRequestError(&quot;invalid header name&quot;) } for _, v := range vv { if !httpguts.ValidHeaderFieldValue(v) { return nil, badRequestError(&quot;invalid header value&quot;) } } } delete(req.Header, &quot;Host&quot;) ctx, cancelCtx := context.WithCancel(ctx) req.ctx = ctx req.RemoteAddr = c.remoteAddr req.TLS = c.tlsState if body, ok := req.Body.(*body); ok { body.doEarlyClose = true } // Adjust the read deadline if necessary. if !hdrDeadline.Equal(wholeReqDeadline) { c.rwc.SetReadDeadline(wholeReqDeadline) } w = &amp;amp;response{ conn: c, cancelCtx: cancelCtx, req: req, reqBody: req.Body, handlerHeader: make(Header), contentLength: -1, closeNotifyCh: make(chan bool, 1), // We populate these ahead of time so we&#39;re not // reading from req.Header after their Handler starts // and maybe mutates it (Issue 14940) wants10KeepAlive: req.wantsHttp10KeepAlive(), wantsClose: req.wantsClose(), } if isH2Upgrade { w.closeAfterReply = true } w.cw.res = w w.w = newBufioWriterSize(&amp;amp;w.cw, bufferBeforeChunkingSize) return w, nil}func (sh serverHandler) ServeHTTP(rw ResponseWriter, req *Request) { handler := sh.srv.Handler if handler == nil { handler = DefaultServeMux } if req.RequestURI == &quot;*&quot; &amp;amp;&amp;amp; req.Method == &quot;OPTIONS&quot; { handler = globalOptionsHandler{} } handler.ServeHTTP(rw, req)}参考handler接口定义type Handler interface { ServeHTTP(ResponseWriter, *Request)}辅助参考文档DefaultServeMux： https://www.lagou.com/lgeduarticle/71243.htmlvar DefaultServeMux = &amp;amp;defaultServeMux使用了全局ServeMux来注册路由以实现http.HandleFunctype ServeMux struct { mu sync.RWMutex m map[string]muxEntry es []muxEntry // slice of entries sorted from longest to shortest. hosts bool // whether any patterns contain hostnames}type muxEntry struct { h Handler pattern string}ServeMux主要暴露了这几个方法func (mux *ServeMux) Handle(pattern string, handler Handler)func (mux *ServeMux) HandleFunc(pattern string, handler func(ResponseWriter, *Request))func (mux *ServeMux) Handler(r *Request) (h Handler, pattern string)func (mux *ServeMux) ServeHTTP(w ResponseWriter, r *Request)Handle注册handle// Handle registers the handler for the given pattern.// If a handler already exists for pattern, Handle panics.func (mux *ServeMux) Handle(pattern string, handler Handler) { mux.mu.Lock() defer mux.mu.Unlock() if pattern == &quot;&quot; { panic(&quot;http: invalid pattern&quot;) } if handler == nil { panic(&quot;http: nil handler&quot;) } if _, exist := mux.m[pattern]; exist { panic(&quot;http: multiple registrations for &quot; + pattern) } if mux.m == nil { mux.m = make(map[string]muxEntry) } e := muxEntry{h: handler, pattern: pattern} mux.m[pattern] = e if pattern[len(pattern)-1] == &#39;/&#39; { mux.es = appendSorted(mux.es, e) } if pattern[0] != &#39;/&#39; { mux.hosts = true }}HandleFunc包装function为handler// HandleFunc registers the handler function for the given pattern.func (mux *ServeMux) HandleFunc(pattern string, handler func(ResponseWriter, *Request)) { if handler == nil { panic(&quot;http: nil handler&quot;) } mux.Handle(pattern, HandlerFunc(handler))}handler// handler is the main implementation of Handler.// The path is known to be in canonical form, except for CONNECT methods.func (mux *ServeMux) handler(host, path string) (h Handler, pattern string) { mux.mu.RLock() defer mux.mu.RUnlock() // Host-specific pattern takes precedence over generic ones if mux.hosts { h, pattern = mux.match(host + path) } if h == nil { h, pattern = mux.match(path) } if h == nil { h, pattern = NotFoundHandler(), &quot;&quot; } return}// Handler returns the handler to use for the given request,// consulting r.Method, r.Host, and r.URL.Path. It always returns// a non-nil handler. If the path is not in its canonical form, the// handler will be an internally-generated handler that redirects// to the canonical path. If the host contains a port, it is ignored// when matching handlers.//// The path and host are used unchanged for CONNECT requests.//// Handler also returns the registered pattern that matches the// request or, in the case of internally-generated redirects,// the pattern that will match after following the redirect.//// If there is no registered handler that applies to the request,// Handler returns a ``page not found&#39;&#39; handler and an empty pattern.func (mux *ServeMux) Handler(r *Request) (h Handler, pattern string) { // CONNECT requests are not canonicalized. if r.Method == &quot;CONNECT&quot; { // If r.URL.Path is /tree and its handler is not registered, // the /tree -&amp;gt; /tree/ redirect applies to CONNECT requests // but the path canonicalization does not. if u, ok := mux.redirectToPathSlash(r.URL.Host, r.URL.Path, r.URL); ok { return RedirectHandler(u.String(), StatusMovedPermanently), u.Path } return mux.handler(r.Host, r.URL.Path) } // All other requests have any port stripped and path cleaned // before passing to mux.handler. host := stripHostPort(r.Host) path := cleanPath(r.URL.Path) // If the given path is /tree and its handler is not registered, // redirect for /tree/. if u, ok := mux.redirectToPathSlash(host, path, r.URL); ok { return RedirectHandler(u.String(), StatusMovedPermanently), u.Path } if path != r.URL.Path { _, pattern = mux.handler(host, path) url := *r.URL url.Path = path return RedirectHandler(url.String(), StatusMovedPermanently), pattern } return mux.handler(host, r.URL.Path)}match会匹配最长的路径，因为mux.es按照长度递减排序例如如下path：/a/b/a/当url为/a/b/c会优先匹配/a/b// Find a handler on a handler map given a path string.// Most-specific (longest) pattern wins.func (mux *ServeMux) match(path string) (h Handler, pattern string) { // Check for exact match first. v, ok := mux.m[path] if ok { return v.h, v.pattern } // Check for longest valid match. mux.es contains all patterns // that end in / sorted from longest to shortest. for _, e := range mux.es { if strings.HasPrefix(path, e.pattern) { return e.h, e.pattern } } return nil, &quot;&quot;}实现了ServeHttp接口func (mux *ServeMux) ServeHTTP(w ResponseWriter, r *Request) { if r.RequestURI == &quot;*&quot; { if r.ProtoAtLeast(1, 1) { w.Header().Set(&quot;Connection&quot;, &quot;close&quot;) } w.WriteHeader(StatusBadRequest) return } h, _ := mux.Handler(r) h.ServeHTTP(w, r)}总结： Handle 和 HandleFunc 方法用来将路由路径与处理函数的映射通过一个 map 记录到当前的 mux 实例里；Handler 方法将接收的请求中的 URL 预处理后拿去和记录的映射匹配，若匹配到，就返回该路由的处理函数和路径；ServeHTTP 方法将请求派遣给匹配到的处理函数处理。参考 https://www.codeleading.com/article/61262896573/" }, { "title": "geecache 笔记", "url": "/posts/geecache-learning/", "categories": "cs", "tags": "geecache", "date": "2021-09-26 00:00:00 +0800", "snippet": "背景本文为geecache的阅读笔记 https://geektutu.com/post/geecache.html回调Getter// A Getter loads data for a key.type Getter interface { Get(key string) ([]byte, error)}// A GetterFunc implements Getter with a function.type GetterFunc func(key string) ([]byte, error)// Get implements Getter interface functionfunc (f GetterFunc) Get(key string) ([]byte, error) { return f(key)}func TestGetter(t *testing.T) { var f Getter = GetterFunc(func(key string) ([]byte, error) { return []byte(key), nil }) expect := []byte(&quot;key&quot;) if v, _ := f.Get(&quot;key&quot;); !reflect.DeepEqual(v, expect) { t.Errorf(&quot;callback failed&quot;) }} 定义一个函数类型 F，并且实现接口 A 的方法，然后在这个方法中调用自己。这是 Go 语言中将其他函数（参数返回值定义与 F 一致）转换为接口 A 的常用技巧。 其实可以反过来想一想，如果不提供这个把函数转换为接口的函数，调用时就需要创建一个struct，然后实现对应的接口，创建一个实例作为参数，相比这种方式就麻烦得多了。 http包中的HandlerFunc类型就是这种做法，本身这个类型实现了ServeHTTP方法。然后用作对其他普通函数的包装一致hashTODO:扩展该一致哈希算法，缓存击穿 缓存雪崩：缓存在同一时刻全部失效，造成瞬时DB请求量大、压力骤增，引起雪崩。缓存雪崩通常因为缓存服务器宕机、缓存的 key 设置了相同的过期时间等引起。 缓存击穿：一个存在的key，在缓存过期的一刻，同时有大量的请求，这些请求都会击穿到 DB ，造成瞬时DB请求量大、压力骤增。 缓存穿透：查询一个不存在的数据，因为不存在则不会写到缓存中，所以每次都会去请求 DB，如果瞬间流量过大，穿透到 DB，导致宕机。singleflightpackage singleflightimport &quot;sync&quot;type call struct { wg sync.WaitGroup val interface{} err error}type Group struct { mu sync.Mutex // protects m m map[string]*call}func (g *Group) Do(key string, fn func() (interface{}, error)) (interface{}, error) { if c, ok := g.m[key]; ok { c.wg.Wait() // 如果请求正在进行中，则等待 return c.val, c.err // 请求结束，返回结果 } c := new(call) c.wg.Add(1) // 发起请求前加锁 g.m[key] = c // 添加到 g.m，表明 key 已经有对应的请求在处理 c.val, c.err = fn() // 调用 fn，发起请求 c.wg.Done() // 请求结束 delete(g.m, key) // 更新 g.m return c.val, c.err // 返回结果} 在一瞬间有大量请求get(key)，而且key未被缓存或者未被缓存在当前节点 如果不用singleflight，那么这些请求都会发送远端节点或者从本地数据库读取，会造成远端节点或本地数据库压力猛增。使用singleflight，第一个get(key)请求到来时，singleflight会记录当前key正在被处理，后续的请求只需要等待第一个请求处理完成，取返回值即可。" }, { "title": "爬虫系统设计", "url": "/posts/crawler-design/", "categories": "cs", "tags": "crawler", "date": "2021-09-26 00:00:00 +0800", "snippet": "设计将原先队列的方式优化为使用数据库的方式表结构:urlstatuspriorityavailable_time问题 慢查询，sharding数据库 链接错误， available_time 设置翻倍的间隔时间 如果网页频繁变化，我们把abailable_time 设置减倍的间隔时间 巨型网页吃掉了大量的资源，设置分配的比例限制" }, { "title": "后端架构技术整理", "url": "/posts/system-design-note/", "categories": "cs", "tags": "backend", "date": "2021-09-26 00:00:00 +0800", "snippet": "bitset 位集Java平台的BitSet用于存放一个位序列，如果要高效的存放一个位序列，就可以使用位集(BitSet)。由于位集将位包装在字节里，所以使用位集比使用Boolean对象的List更加高效和更加节省存储空间。BitSet是位操作的对象，值只有0或1即false和true，内部维护了一个long数组，初始只有一个long，所以BitSet最小的size是64，当随着存储的元素越来越多，BitSet内部会动态扩充，一次扩充64位，最终内部是由N个long来存储。默认情况下，BitSet的所有位都是false即0。应用场景： 统计一组大数据中没有出现过的数；将这组数据映射到BitSet，然后遍历BitSet，对应位为0的数表示没有出现过的数据。 对大数据进行排序；将数据映射到BitSet，遍历BitSet得到的就是有序数据。 在内存对大数据进行压缩存储等等。一个GB的内存空间可以存储85亿多个数，可以有效实现数据的压缩存储，节省内存空间开销。注： 当使用大数据排序等场景时bitset的bit数应大于等于最大数 排序无法解决数据重复的问题树二叉树每个节点最多有两个叶子节点。完全二叉树叶节点只能出现在最下层和次下层，并且最下面一层的结点都集中在该层最左边的若干位置的二叉树。平衡二叉树左右两个子树的高度差的绝对值不超过1，并且左右两个子树都是一棵平衡二叉树。二叉查找树（BST）二叉查找树（Binary Search Tree），也称有序二叉树（ordered binary tree）,排序二叉树（sorted binary tree）。红黑树添加阶段后，左旋或者右旋从而再次达到平衡。B，B+，B*树MySQL是基于B+树聚集索引组织表B+树的叶子节点链表结构相比于 B-树便于扫库，和范围检索。LSM 树LSM（Log-Structured Merge-Trees）和 B+ 树相比，是牺牲了部分读的性能来换取写的性能(通过批量写入)，实现读写之间的平衡。 Hbase、LevelDB、Tair（Long DB）、nessDB 采用 LSM 树的结构。LSM可以快速建立索引。TODO: B树等概念的巩固布隆过滤器常用于大数据的排重，比如email，url 等。 核心原理：将每条数据通过计算产生一个指纹（一个字节或多个字节，但一定比原始数据要少很多），其中每一位都是通过随机计算获得，在将指纹映射到一个大的按位存储的空间中。注意：会有一定的错误率。 优点：空间和时间效率都很高。 缺点：随着存入的元素数量增加，误算率随之增加。事务 ACID 特性事务的隔离级别 未提交读：一个事务可以读取另一个未提交的数据，容易出现脏读的情况。 读提交：一个事务等另外一个事务提交之后才可以读取数据，但会出现不可重复读的情况（多次读取的数据不一致），读取过程中出现UPDATE操作，会多。（大多数数据库默认级别是RC，比如SQL Server，Oracle），读取的时候不可以修改。 可重复读： 同一个事务里确保每次读取的时候，获得的是同样的数据，但不保障原始数据被其他事务更新（幻读），Mysql InnoDB 就是这个级别。 序列化：所有事物串行处理（牺牲了效率）区分读提交和可重复读： https://blog.csdn.net/tolcf/article/details/49311035读提交：务A事先读取了数据，事务B紧接了更新了数据，并提交了事务，而事务A再次读取该数据时，数据已经发生了改变。造成了不可重复读（虚读）。可重复读：事务A读取与搜索条件相匹配的若干行。事务B以插入或删除行等方式来修改事务A的结果集，然后再提交。事务A再读取时，却发现数据发生了变化。造成了幻读。很多人都容易混淆不可重复读和幻读的概念，当然，本人也是纠结了好久，下面就说一下我的理解。不可重复读真正含义应该包含虚读和幻读。 所谓的虚读，也就是大家经常说的不可重复读，是指在数据库访问中，一个事务范围内两个相同的查询却返回了不同数据。这是由于查询时系统中其他事务修改的提交而引起的。比如事务T1读取某一数据，事务T2读取并修改了该数据，T1为了对读取值进行检验而再次读取该数据，便得到了不同的结果。一种更易理解的说法是：在一个事务内，多次读同一个数据。在这个事务还没有结束时，另一个事务也访问该同一数据。那么，在第一个事务的两次读数据之间。由于第二个事务的修改，那么第一个事务读到的数据可能不一样，这样就发生了在一个事务内两次读到的数据是不一样的，因此称为不可重复读，即原始读取不可重复。 所谓幻读，是指事务A读取与搜索条件相匹配的若干行。事务B以插入或删除行等方式来修改事务A的结果集，然后再提交。幻读是指当事务不是独立执行时发生的一种现象，例如第一个事务对一个表中的数据进行了修改，比如这种修改涉及到表中的“全部数据行”。同时，第二个事务也修改这个表中的数据，这种修改是向表中插入“一行新数据”。那么，以后就会发生操作第一个事务的用户发现表中还有没有修改的数据行，就好象发生了幻觉一样.一般解决幻读的方法是增加范围锁RangeS，锁定检锁范围为只读，这样就避免了幻读。MVCC innodb 中 MVCC 用在 Repeatable-Read 隔离级别。 MVCC 会产生幻读问题（更新时异常。） 通过隐藏版本列来实现 MVCC 控制，一列记录创建时间、一列记录删除时间，这里的时间每次只操作比当前版本小（或等于）的 行。锁重点公平锁 &amp;amp; 非公平锁公平锁的作用就是严格按照线程启动的顺序来执行的，不允许其他线程插队执行的；而非公平锁是允许插队的。默认情况下 ReentrantLock 和 synchronized 都是非公平锁。ReentrantLock 可以设置成公平锁。乐观锁&amp;amp;悲观锁 悲观锁悲观锁如果使用不当（锁的条数过多），会引起服务大面积等待。推荐优先使用乐观锁+重试。 乐观锁的方式：版本号+重试方式悲观锁：通过 select … for update 进行行锁(不可读、不可写，share 锁可读不可写)。 https://www.cnblogs.com/zhiqian-ali/p/6200874.html悲观锁（Pessimistic Lock）悲观锁的特点是先获取锁，再进行业务操作，即“悲观”的认为获取锁是非常有可能失败的，因此要先确保获取锁成功再进行业务操作。通常所说的“一锁二查三更新”即指的是使用悲观锁。通常来讲在数据库上的悲观锁需要数据库本身提供支持，即通过常用的select … for update操作来实现悲观锁。当数据库执行select for update时会获取被select中的数据行的行锁，因此其他并发执行的select for update如果试图选中同一行则会发生排斥（需要等待行锁被释放），因此达到锁的效果。select for update获取的行锁会在当前事务结束时自动释放，因此必须在事务中使用。这里需要注意的一点是不同的数据库对select for update的实现和支持都是有所区别的，例如oracle支持select for update no wait，表示如果拿不到锁立刻报错，而不是等待，mysql就没有no wait这个选项。另外mysql还有个问题是select for update语句执行中所有扫描过的行都会被锁上，这一点很容易造成问题。因此如果在mysql中用悲观锁务必要确定走了索引，而不是全表扫描。乐观锁（Optimistic Lock）乐观锁的特点先进行业务操作，不到万不得已不去拿锁。即“乐观”的认为拿锁多半是会成功的，因此在进行完业务操作需要实际更新数据的最后一步再去拿一下锁就好。乐观锁在数据库上的实现完全是逻辑的，不需要数据库提供特殊的支持。一般的做法是在需要锁的数据上增加一个版本号，或者时间戳，然后按照如下方式实现：1. SELECT data AS old_data, version AS old_version FROM …;2. 根据获取的数据进行业务操作，得到new_data和new_version3. UPDATE SET data = new_data, version = new_version WHERE version = old_versionif (updated row &amp;gt; 0) { // 乐观锁获取成功，操作完成} else { // 乐观锁获取失败，回滚并重试}乐观锁是否在事务中其实都是无所谓的，其底层机制是这样：在数据库内部update同一行的时候是不允许并发的，即数据库每次执行一条update语句时会获取被update行的写锁，直到这一行被成功更新后才释放。因此在业务操作进行前获取需要锁的数据的当前版本号，然后实际更新数据时再次对比版本号确认与之前获取的相同，并更新版本号，即可确认这之间没有发生并发的修改。如果更新失败即可认为老版本的数据已经被并发修改掉而不存在了，此时认为获取锁失败，需要回滚整个业务操作并可根据需要重试整个过程。总结： 乐观锁在不发生取锁失败的情况下开销比悲观锁小，但是一旦发生失败回滚开销则比较大，因此适合用在取锁失败概率比较小的场景，可以提升系统并发性能 乐观锁还适用于一些比较特殊的场景，例如在业务操作过程中无法和数据库保持连接等悲观锁无法适用的地方mysql并发死锁mysql的innodb存储引擎实务锁虽然是锁行，但它内部是锁索引的。锁相同数据的不同索引条件可能会引起死锁。 https://www.cnblogs.com/Lawson/p/5008741.htmlhttps://www.cnblogs.com/zejin2008/p/5262751.html乐观锁 &amp;amp; CAS和MySQL乐观锁方式相似，只不过是通过和原值进行比较。 https://blog.csdn.net/u011514810/article/details/76895723/CopyOnWriteCopyOnWrite容器可以对CopyOnWrite容器进行并发的读，而不需要加锁。CopyOnWrite并发容器用于读多写少的并发场景。比如白名单，黑名单，商品类目的访问和更新场景，不适合需要数据强一致性的场景。实现读写分离，读取发生在原始数据上，写入发生在副本上。不用加锁，通过最终一致实现一致性。 https://www.cnblogs.com/hapjin/p/4840107.html1，什么是写时复制(Copy-On-Write)容器？写时复制是指：在并发访问的情景下，当需要修改JAVA中Containers的元素时，不直接修改该容器，而是先复制一份副本，在副本上进行修改。修改完成之后，将指向原来容器的引用指向新的容器(副本容器)。2，写时复制带来的影响①由于不会修改原始容器，只修改副本容器。因此，可以对原始容器进行并发地读。其次，实现了读操作与写操作的分离，读操作发生在原始容器上，写操作发生在副本容器上。②数据一致性问题：读操作的线程可能不会立即读取到新修改的数据，因为修改操作发生在副本上。但最终修改操作会完成并更新容器，因此这是最终一致性。RingBuffer在程序设计中，我们有时会遇到这样的情况，一个线程将数据写到一个buffer中，另外一个线程从中读数据。所以这里就有多线程竞争的问题。通常的解决办法是对竞争资源加锁。但是，一般加锁的损耗较高。其实，对于这样的一个线程写，一个线程读的特殊情况，可以以一种简单的无锁RingBuffer来实现。这样代码的运行效率很高。如图所示，假定buffer的长度是bufferSize. 我们设置两个指针。head指向的是下一次读的位置，而tail指向的是下一次写的位置。由于这里是环形buffer (ring buffer)，这里就有一个问题，怎样判断buffer是满或者空。这里采用的规则是，buffer的最后一个单元不存储数据。所以，如果head == tail，那么说明buffer为空。如果 head == tail + 1 (mod bufferSize)，那么说明buffer满了。接下来就是最重要的内容了：怎样以无锁的方式进行线程安全的buffer的读写操作。基本原理是这样的。在进行读操作的时候，我们只修改head的值，而在写操作的时候我们只修改tail的值。在写操作时，我们在写入内容到buffer之后才修改tail的值；而在进行读操作的时候，我们会读取tail的值并将其赋值给copyTail。赋值操作是原子操作。所以在读到copyTail之后，从head到copyTail之间一定是有数据可以读的，不会出现数据没有写入就进行读操作的情况。同样的，读操作完成之后，才会修改head的数值；而在写操作之前会读取head的值判断是否有空间可以用来写数据。所以，这时候tail到head - 1之间一定是有空间可以写数据的，而不会出现一个位置的数据还没有读出就被写操作覆盖的情况。这样就保证了RingBuffer的线程安全性。可重入锁 &amp;amp; 不可重入锁通过简单代码举例说明可重入锁和不可重入锁。可重入锁指同一个线程可以再次获得之前已经获得的锁。可重入锁可以用户避免死锁。Java中的可重入锁：synchronized 和 java.util.concurrent.locks.ReentrantLocksynchronized 使用方便，编译器来加锁，是非公平锁。ReenTrantLock 使用灵活，锁的公平性可以定制。相同加锁场景下，推荐使用 synchronized。互斥锁 &amp;amp; 共享锁互斥锁：同时只能有一个线程获得锁。比如，ReentrantLock 是互斥锁，ReadWriteLock 中的写锁是互斥锁。共享锁：可以有多个线程同时或的锁。比如，Semaphore、CountDownLatch 是共享锁，ReadWriteLock 中的读锁是共享锁。死锁互斥、持有、不可剥夺、环形等待。DevOps参考最近阅读的小册： https://juejin.cn/book/6897616008173846543中间件NginxNginx 通过异步非阻塞的事件处理机制实现高并发。Apache 每个请求独占一个线程，非常消耗系统资源。事件驱动适合于IO密集型服务(Nginx)，多进程或线程适合于CPU密集型服务(Apache)，所以Nginx适合做反向代理，而非web服务器使用。Jetty&amp;amp;Tomcat架构比较:Jetty的架构比Tomcat的更为简单。性能比较：Jetty和Tomcat性能方面差异不大，Jetty默认采用NIO结束在处理I/O请求上更占优势，Tomcat默认采用BIO处理I/O请求，Tomcat适合处理少数非常繁忙的链接，处理静态资源时性能较差。其他方面：Jetty的应用更加快速，修改简单，对新的Servlet规范的支持较好;Tomcat 对JEE和Servlet 支持更加全面。Cachegee-cache可以参考曾经阅读的实践代码：gee-cachehttps://github.com/peigongdh/7days-golangmemcached采用多路复用技术提高并发性。slab分配算法： memcached给Slab分配内存空间，默认是1MB。分配给Slab之后 把slab的切分成大小相同的chunk，Chunk是用于缓存记录的内存空间，Chunk 的大小默认按照1.25倍的速度递增。好处是不会频繁申请内存，提高IO效率，坏处是会有一定的内存浪费。Redis使用 ziplist 存储链表，ziplist是一种压缩链表，它的好处是更能节省内存空间，因为它所存储的内容都是在连续的内存区域当中的。使用 skiplist(跳跃表)来存储有序集合对象、查找上先从高Level查起、时间复杂度和红黑树相当，实现容易，无锁、并发性好。RDB方式：定期备份快照，常用于灾难恢复。优点：通过fork出的进程进行备份，不影响主进程、RDB 在恢复大数据集时的速度比 AOF 的恢复速度要快。缺点：会丢数据。AOF方式：保存操作日志方式。优点：恢复时数据丢失少，缺点：文件大，回复慢。也可以两者结合使用。TODO: 回顾Redis设计的数据结构 https://blog.csdn.net/wcf373722432/article/details/78678504Tair特点：可以配置备份节点数目，通过异步同步到备份节点一致性Hash算法。架构：和Hadoop 的设计思想类似，有Configserver，DataServer，Configserver 通过心跳来检测，Configserver也有主备关系。几种存储引擎:MDB，完全内存性，可以用来存储Session等数据。Rdb（类似于Redis），轻量化，去除了aof之类的操作，支持Restfull操作LDB（LevelDB存储引擎），持久化存储，LDB 作为rdb的持久化，google实现，比较高效，理论基础是LSM(Log-Structured-Merge Tree)算法，现在内存中修改数据，达到一定量时（和内存汇总的旧数据一同写入磁盘）再写入磁盘，存储更加高效，县比喻Hash算法。Tair采用共享内存来存储数据，如果服务挂掉（非服务器），重启服务之后，数据亦然还在。消息队列RabbitMQ 消费者默认是推模式（也支持拉模式）。Kafka 默认是拉模式。Push方式：优点是可以尽可能快地将消息发送给消费者，缺点是如果消费者处理能力跟不上，消费者的缓冲区可能会溢出。Pull方式：优点是消费端可以按处理能力进行拉去，缺点是会增加消息延迟。曾经读过的文章，消息队列的设计： https://tech.meituan.com/2016/07/01/mq-design.htmlRabbitMQ支持事务，推拉模式都是支持、适合需要可靠性消息传输的场景。RocketMQJava实现，推拉模式都是支持，吞吐量逊于Kafka。可以保证消息顺序。ActiveMQ纯Java实现，兼容JMS，可以内嵌于Java应用中。Kafka高吞吐量、采用拉模式。适合高IO场景，比如日志同步。Redis 消息推送生产者、消费者模式完全是客户端行为，list 和 拉模式实现，阻塞等待采用 blpop 指令。定时调度单机定时调度fork 进程 + sleep 轮询定时调度在 QuartzSchedulerThread 代码中，while()无限循环，每次循环取出时间将到的trigger，触发对应的job，直到调度器线程被关闭。分布式定时调度opencron、LTS、XXL-JOB、Elastic-Job、Uncode-Schedule、AntaresQuartz集群中，独立的Quartz节点并不与另一其的节点或是管理节点通信，而是通过相同的数据库表来感知到另一Quartz应用的RPCDubboThriftgRPC配置中心TODO: 对比QTT的全局配置平台实现apolloApollo（阿波罗）是携程框架部门研发的开源配置管理中心，能够集中化管理应用不同环境、不同集群的配置，配置修改后能够实时推送到应用端，并且具备规范的权限、流程治理等特性。Apollo支持4个维度管理Key-Value格式的配置：application (应用)environment (环境)cluster (集群)namespace (命名空间)同时，Apollo基于开源模式开发，开源地址：https://github.com/ctripcorp/apolloAPI网关主要职责：请求转发、安全认证、协议转换、容灾。 https://www.infoq.cn/article/2016/07/API-background-architecture-floo/摘录：Netflix 其实并没有对 API GW 进行深入的功能实现（或者说面相业务友好的相关功能），整体上它只提供了一个技术框架、和一些标准的 filter 实例实现，相信了解过 filter chain 原理的分布式中间件工程师也能搭出这样的框架。这么做的原因，我认为很大原因是 API GW 所扮演的角色是一个业务平台，而非技术平台，将行业特征很强的业务部分开源，对于受众意义也不是特别大。另外，除了 Netflix Zuul，在商业产品上还有 apigee 公司所提供的方案，在轻量级开源实现上还有基于 Nginx 的 kong ，kong 其实提供了 19 个插件式的功能实现，涵盖的面主要在于安全、监控等领域，但缺少对报文转换的能力（为什么缺 也很显而易见——避免产生业务场景的耦合，更通用）。API GW 本身NIO 接入，异步接出流控与屏蔽秘钥交换客户端认证与报文加解密业务路由框架报文转换HTTP DNS/ Direct IPAPI GW 客户端 SDK / Library基本通信秘钥交换与 Cache身份认证与报文加解密配套的在线自助服务平台代码生成文档生成沙盒调测网络HTTP&amp;amp;HTTP2.0参考http2-node-1.md网络模型SELECT &amp;amp; POLL &amp;amp; EPOLLselect，poll，epoll本质上都是同步I/O，因为他们都需要在读写事件就绪后自己负责进行读写，也就是说这个读写过程是阻塞的。select 有打开文件描述符数量限制，默认1024（2048 for x64），100万并发，就要用1000个进程、切换开销大；poll采用链表结构，没有数量限制。select，poll “醒着”的时候要遍历整个fd集合，而epoll在“醒着”的时候只要判断一下就绪链表是否为空就行了，通过回调机制节省大量CPU时间；select，poll每次调用都要把fd集合从用户态往内核态拷贝一次，而epoll只要一次拷贝。poll会随着并发增加，性能逐渐下降，epoll采用红黑树结构，性能稳定，不会随着连接数增加而降低。在连接数少并且连接都十分活跃的情况下，select和poll的性能可能比epoll好，毕竟epoll的通知机制需要很多函数回调。序列化HessianProtobuf数据库重点基础理论第一范式：数据表中的每一列（每个字段）必须是不可拆分的最小单元，也就是确保每一列的原子性；第二范式（2NF）：满足1NF后，要求表中的所有列，都必须依赖于主键，而不能有任何一列与主键没有关系，也就是说一个表只描述一件事情；第三范式：必须先满足第二范式（2NF），要求：表中的每一列只与主键直接相关而不是间接相关，（表中的每一列只能依赖于主键）；NoSQLMongoDB优点：弱一致性（最终一致），更能保证用户的访问速度；内置GridFS，支持大容量的存储；Schema-less 数据库，不用预先定义结构；内置Sharding；相比于其他NoSQL，第三方支持丰富；性能优越；缺点：mongodb不支持事务操作；mongodb占用空间过大；MongoDB没有如MySQL那样成熟的维护工具，这对于开发和IT运营都是个值得注意的地方；Hbase空数据不存储，节省空间，且适用于并发。rowkey 按照字典顺序排列，便于批量扫描。通过散列可以避免热点。搜索引擎LuceneElasticsearchSolr大数据流式计算StormFlinkKafka Stream应用场景：广告相关实时统计；推荐系统用户画像标签实时更新；线上服务健康状况实时监测；实时榜单；实时数据统计。安全XSSCSRFSQL 注入Hash Dos脚本注入漏洞扫描工具验证码DDoS 防范用户隐私信息保护鉴权授权、认证RBACOAuth2.0单点登录(SSO)分布式设计扩展性设计总结下来，通用的套路就是分布、缓存及异步处理。水平切分+垂直切分利用中间件进行分片如，MySQL Proxy。利用分片策略进行切分，如按照ID取模。分布式服务+消息队列。稳定性 &amp;amp; 高可用可扩展：水平扩展、垂直扩展。 通过冗余部署，避免单点故障。隔离：避免单一业务占用全部资源。避免业务之间的相互影响 2. 机房隔离避免单点故障。解耦：降低维护成本，降低耦合风险。减少依赖，减少相互间的影响。限流：滑动窗口计数法、漏桶算法、令牌桶算法等算法。遇到突发流量时，保证系统稳定。降级：紧急情况下释放非核心功能的资源。牺牲非核心业务，保证核心业务的高可用。熔断：异常情况超出阈值进入熔断状态，快速失败。减少不稳定的外部依赖对核心服务的影响。自动化测试：通过完善的测试，减少发布引起的故障。灰度发布：灰度发布是速度与安全性作为妥协，能够有效减少发布故障。设计原则：数据不丢(持久化)；服务高可用(服务副本)；绝对的100%高可用很难，目标是做到尽可能多的9，如99.999%（全年累计只有5分钟）。硬件负载均衡软件负载均衡《几种负载均衡算法》 轮寻、权重、负载、最少连接、QoS《DNS负载均衡》配置简单，更新速度慢。《Nginx负载均衡》简单轻量、学习成本低；主要适用于web应用。《借助LVS+Keepalived实现负载均衡 》配置比较负载、只支持到4层，性能较高。《HAProxy用法详解 全网最详细中文文档》支持到七层（比如HTTP）、功能比较全面，性能也不错。《Haproxy+Keepalived+MySQL实现读均衡负载》主要是用户读请求的负载均衡。《rabbitmq+haproxy+keepalived实现高可用集群搭建》限流计数器：通过滑动窗口计数器，控制单位时间内的请求次数，简单粗暴。漏桶算法：固定容量的漏桶，漏桶满了就丢弃请求，比较常用。令牌桶算法：固定容量的令牌桶，按照一定速率添加令牌，处理请求前需要拿到令牌，拿不到令牌则丢弃请求，或进入丢队列，可以通过控制添加令牌的速率，来控制整体速度。Guava 中的 RateLimiter 是令牌桶的实现。Nginx 限流：通过 limit_req 等模块限制并发连接数。应用层容灾雪崩效应原因：硬件故障、硬件故障、程序Bug、重试加大流量、用户大量请求。雪崩的对策：限流、改进缓存模式(缓存预加载、同步调用改异步)、自动扩容、降级。Hystrix设计原则：资源隔离：Hystrix通过将每个依赖服务分配独立的线程池进行资源隔离, 从而避免服务雪崩。熔断开关：服务的健康状况 = 请求失败数 / 请求总数，通过阈值设定和滑动窗口控制开关。命令模式：通过继承 HystrixCommand 来包装服务调用逻辑。主要策略：失效瞬间：单机使用锁；使用分布式锁；不过期；热点数据：热点数据单独存储；使用本地缓存；分成多个子key；考虑singleFight防缓存击穿的方式有很多种，比如通过计划任务来跟新缓存使得从前端过来的所有请求都是从缓存读取等等。之前读过 groupCache的源码，发现里面有一个很有意思的库，叫singleFlight, 因为groupCache从节点上获取缓存如果未命中，则会去其他节点寻找，其他节点还没有的话再从数据源获取，所以这个步骤对于防击穿非常有必要。singleFlight使得groupCache在多个并发请求对一个失效的key进行源数据获取时，只让其中一个得到执行，其余阻塞等待到执行的那个请求完成后，将结果传递给阻塞的其他请求达到防止击穿的效果。对比学习过的gee-cache中的单飞模型缓存雪崩：缓存在同一时刻全部失效，造成瞬时DB请求量大、压力骤增，引起雪崩。缓存雪崩通常因为缓存服务器宕机、缓存的 key 设置了相同的过期时间等引起。设置固定+随机的过期时间缓存击穿：一个存在的key，在缓存过期的一刻，同时有大量的请求，这些请求都会击穿到 DB ，造成瞬时DB请求量大、压力骤增。 使用singleFight缓存穿透：查询一个不存在的数据，因为不存在则不会写到缓存中，所以每次都会去请求 DB，如果瞬间流量过大，穿透到 DB，导致宕机。为空结果集设置缓存跨机房容灾通过自研中间件进行数据同步。注意延迟问题，多次跨机房调用会将延时放大数倍。建房间专线很大概率会出现问题，做好运维和程序层面的容错。不能依赖于程序端数据双写，要有自动同步方案。数据永不在高延迟和较差网络质量下，考虑同步质量问题。核心业务和次要业务分而治之，甚至只考虑核心业务。异地多活监控部署、测试也要跟上。业务允许的情况下考虑用户分区，尤其是游戏、邮箱业务。控制跨机房消息体大小，越小越好。考虑使用docker容器虚拟化技术，提高动态调度能力。容灾演练流程常见故障画像案例：预案有效性、预案有效性、故障复现、架构容灾测试、参数调优、参数调优、故障突袭、联合演练。平滑启动平滑重启应用思路 1.端流量（如vip层）、2. flush 数据(如果有)、3, 重启应用《JVM安全退出（如何优雅的关闭java服务）》 推荐推出方式：System.exit，Kill SIGTERM；不推荐 kill-9；用 Runtime.addShutdownHook 注册钩子。《常见Java应用如何优雅关闭》 Java、Spring、Dubbo 优雅关闭方式。参考整理的笔记：hot-start-node-1.md数据库扩展分片模式中间件： 轻量级：sharding-jdbc、TSharding；重量级：Atlas、MyCAT、Vitess等。问题：事务、Join、迁移、扩容、ID、分页等。事务补偿：对数据进行对帐检查;基于日志进行比对;定期同标准数据来源进行同步等。分库策略：数值范围；取模；日期等。分库数量：通常 MySQL 单库 5千万条、Oracle 单库一亿条需要分库。分区：是MySQL内部机制，对客户端透明，数据存储在不同文件中，表面上看是同一个表。分表：物理上创建不同的表、客户端需要管理分表路由。服务发现重点客户端服务发现模式：客户端直接查询注册表，同时自己负责负载均衡。Eureka 采用这种方式。服务器端服务发现模式：客户端通过负载均衡查询服务实例。CAP支持：Consul（CA）、zookeeper（cp）、etcd（cp） 、euerka（ap）产品设计中 CAP 理论的取舍Eureka 典型的 AP,作为分布式场景下的服务发现的产品较为合适，服务发现场景的可用性优先级较高，一致性并不是特别致命。其次 CA 类型的场景 Consul,也能提供较高的可用性，并能 k-v store 服务保证一致性。 而Zookeeper,Etcd则是CP类型 牺牲可用性，在服务发现场景并没太大优势；服务路由控制原则：透明化路由负载均衡策略：随机、轮询、服务调用延迟、一致性哈希、粘滞连接本地路由优先策略：injvm(优先调用jvm内部的服务)，innative(优先使用相同物理机的服务),原则上找距离最近的服务。配置方式：统一注册表；本地配置；动态下发。CAP https://www.cnblogs.com/szlbm/p/5588543.html1、有些系统，既要快速地响应用户，同时还要保证系统的数据对于任意客户端都是真实可靠的，就像火车站售票系统2、有些系统，需要为用户保证绝对可靠的数据安全，虽然在数据一致性上存在延时，但最终务必保证严格的一致性，就像银行的转账系统3、有些系统，虽然向用户展示了一些可以说是”错误”的数据，但是在整个系统使用过程中，一定会在某一个流程上对系统数据进行准确无误的检查，从而避免用户发生不必要的损失，就像网购系统分布一致性的提出在分布式系统中要解决的一个重要问题就是数据的复制。在我们的日常开发经验中，相 信很多开发人员都遇到过这样的问题：假设客户端C1将系统中的一个值K由V1更新为V2，但客户端C2无法立即读取到K的最新值，需要在一段时间之后才能 读取到。这很正常，因为数据库复制之间存在延时。分布式系统对于数据的复制需求一般都来自于以下两个原因：1、为了增加系统的可用性，以防止单点故障引起的系统不可用2、提高系统的整体性能，通过负载均衡技术，能够让分布在不同地方的数据副本都能够为用户提供服务数据复制在可用性和性能方面给分布式系统带来的巨大好处是不言而喻的，然而数据复制所带来的一致性挑战，也是每一个系统研发人员不得不面对的。所谓分布一致性问题，是指在分布式环境中引入数据复制机制之后，不同数据节点之间 可能出现的，并无法依靠计算机应用程序自身解决的数据不一致的情况。简单讲，数据一致性就是指在对一个副本数据进行更新的时候，必须确保也能够更新其他的 副本，否则不同副本之间的数据将不一致。那么如何解决这个问题？一种思路是”既然是由于延时动作引起的问题，那我可以将写入的动作阻塞，直到数据复制完成后，才完成写入动作”。 没错，这似乎能解决问题，而且有一些系统的架构也确实直接使用了这个思路。但这个思路在解决一致性问题的同时，又带来了新的问题：写入的性能。如果你的应 用场景有非常多的写请求，那么使用这个思路之后，后续的写请求都将会阻塞在前一个请求的写操作上，导致系统整体性能急剧下降。总得来说，我们无法找到一种能够满足分布式系统所有系统属性的分布式一致性解决方案。因此，如何既保证数据的一致性，同时又不影响系统运行的性能，是每一个分布式系统都需要重点考虑和权衡的。于是，一致性级别由此诞生：1、强一致性这种一致性级别是最符合用户直觉的，它要求系统写入什么，读出来的也会是什么，用户体验好，但实现起来往往对系统的性能影响大2、弱一致性这种一致性级别约束了系统在写入成功后，不承诺立即可以读到写入的值，也不久承诺多久之后数据能够达到一致，但会尽可能地保证到某个时间级别（比如秒级别）后，数据能够达到一致状态3、最终一致性最终一致性是弱一致性的一个特例，系统会保证在一定时间内，能够达到一个数据一致的状态。这里之所以将最终一致性单独提出来，是因为它是弱一致性中非常推崇的一种一致性模型，也是业界在大型分布式系统的数据一致性上比较推崇的模型CAP理论一个经典的分布式系统理论。CAP理论告诉我们：一个分布式系统不可能同时满足一致性（C：Consistency）、可用性（A：Availability）和分区容错性（P：Partition tolerance）这三个基本需求，最多只能同时满足其中两项。1、一致性在分布式环境下，一致性是指数据在多个副本之间能否保持一致的特性。在一致性的需求下，当一个系统在数据一致的状态下执行更新操作后，应该保证系统的数据仍然处于一直的状态。对于一个将数据副本分布在不同分布式节点上的系统来说，如果对第一个节点的数据进 行了更新操作并且更新成功后，却没有使得第二个节点上的数据得到相应的更新，于是在对第二个节点的数据进行读取操作时，获取的依然是老数据（或称为脏数 据），这就是典型的分布式数据不一致的情况。在分布式系统中，如果能够做到针对一个数据项的更新操作执行成功后，所有的用户都可以读取到其最新的值，那么 这样的系统就被认为具有强一致性2、可用性可用性是指系统提供的服务必须一直处于可用的状态，对于用户的每一个操作请求总是能够在有限的时间内返回结果。这里的重点是”有限时间内”和”返回结果”。“有限时间内”是指，对于用户的一个操作请求，系统必须能够在指定的时间内返回对 应的处理结果，如果超过了这个时间范围，那么系统就被认为是不可用的。另外，”有限的时间内”是指系统设计之初就设计好的运行指标，通常不同系统之间有很 大的不同，无论如何，对于用户请求，系统必须存在一个合理的响应时间，否则用户便会对系统感到失望。“返回结果”是可用性的另一个非常重要的指标，它要求系统在完成对用户请求的处理后，返回一个正常的响应结果。正常的响应结果通常能够明确地反映出队请求的处理结果，即成功或失败，而不是一个让用户感到困惑的返回结果。3、分区容错性分区容错性约束了一个分布式系统具有如下特性：分布式系统在遇到任何网络分区故障的时候，仍然需要能够保证对外提供满足一致性和可用性的服务，除非是整个网络环境都发生了故障。网络分区是指在分布式系统中，不同的节点分布在不同的子网络（机房或异地网络） 中，由于一些特殊的原因导致这些子网络出现网络不连通的状况，但各个子网络的内部网络是正常的，从而导致整个系统的网络环境被切分成了若干个孤立的区域。 需要注意的是，组成一个分布式系统的每个节点的加入与退出都可以看作是一个特殊的网络分区。既然一个分布式系统无法同时满足一致性、可用性、分区容错性三个特点，所以我们就需要抛弃一样：CA 放弃分区容错性，加强一致性和可用性，其实就是传统的单机数据库的选择AP 放弃一致性（这里说的一致性是强一致性），追求分区容错性和可用性，这是很多分布式系统设计时的选择，例如很多NoSQL系统就是如此CP 放弃可用性，追求一致性和分区容错性，基本不会选择，网络问题会直接让整个系统不可用需要明确的一点是，对于一个分布式系统而言，分区容错性是一个最基本的要求。因为 既然是一个分布式系统，那么分布式系统中的组件必然需要被部署到不同的节点，否则也就无所谓分布式系统了，因此必然出现子网络。而对于分布式系统而言，网 络问题又是一个必定会出现的异常情况，因此分区容错性也就成为了一个分布式系统必然需要面对和解决的问题。因此系统架构师往往需要把精力花在如何根据业务 特点在C（一致性）和A（可用性）之间寻求平衡。BASE理论BASE是Basically Available（基本可用）、Soft state（软状态）和Eventually consistent（最终一致性）三个短语的缩写。BASE理论是对CAP中一致性和可用性权衡的结果，其来源于对大规模互联网系统分布式实践的总结， 是基于CAP定理逐步演化而来的。BASE理论的核心思想是：即使无法做到强一致性，但每个应用都可以根据自身业务特点，采用适当的方式来使系统达到最终一致性。接下来看一下BASE中的三要素：1、基本可用基本可用是指分布式系统在出现不可预知故障的时候，允许损失部分可用性—-注意，这绝不等价于系统不可用。比如：（1）响应时间上的损失。正常情况下，一个在线搜索引擎需要在0.5秒之内返回给用户相应的查询结果，但由于出现故障，查询结果的响应时间增加了1~2秒（2）系统功能上的损失：正常情况下，在一个电子商务网站上进行购物的时候，消费者几乎能够顺利完成每一笔订单，但是在一些节日大促购物高峰的时候，由于消费者的购物行为激增，为了保护购物系统的稳定性，部分消费者可能会被引导到一个降级页面2、软状态软状态指允许系统中的数据存在中间状态，并认为该中间状态的存在不会影响系统的整体可用性，即允许系统在不同节点的数据副本之间进行数据同步的过程存在延时3、最终一致性最终一致性强调的是所有的数据副本，在经过一段时间的同步之后，最终都能够达到一个一致的状态。因此，最终一致性的本质是需要系统保证最终数据能够达到一致，而不需要实时保证系统数据的强一致性。总的来说，BASE理论面向的是大型高可用可扩展的分布式系统，和传统的事物ACID特性是相反的，它完全不同于ACID的强一致性模型，而是通过牺牲强一致性来获得可用性，并允许数据在一段时间内是不一致的，但最终达到一致状态。但同时，在实际的分布式场景中，不同业务单元和组件对数据一致性的要求是不同的，因此在具体的分布式系统架构设计过程中，ACID特性和BASE理论往往又会结合在一起。分布式锁基于数据库的分布式锁：优点：操作简单、容易理解。缺点：存在单点问题、数据库性能够开销较大、不可重入；基于缓存的分布式锁：优点：非阻塞、性能好。缺点：操作不好容易造成锁无法释放的情况。Zookeeper 分布式锁：通过有序临时节点实现锁机制，自己对应的节点需要最小，则被认为是获得了锁。优点：集群可以透明解决单点问题，避免锁不被释放问题，同时锁可以重入。缺点：性能不如缓存方式，吞吐量会随着zk集群规模变大而下降。Redis-分布式锁基于 setnx(set if ont exists)，有则返回false，否则返回true。并支持过期时间。分布式一致性算法PAXOSRaft三种角色：Leader（领袖）、Follower（群众）、Candidate（候选人）通过随机等待的方式发出投票，得票多的获胜。通过回顾算法：https://km.qutoutiao.net/pages/viewpage.action?pageId=222068805Gossip《Gossip算法》幂等幂等特性的作用：该资源具备幂等性，请求方无需担心重复调用会产生错误。常见保证幂等的手段：MVCC（类似于乐观锁）、去重表(唯一索引)、悲观锁、一次性token、序列号方式。分布式一致方案TCC(Try/Confirm/Cancel) 柔性事务基于BASE理论：基本可用、柔性状态、最终一致。解决方案：记录日志+补偿（正向补充或者回滚）、消息重试(要求程序要幂等)；“无锁设计”、采用乐观锁机制。唯一ID 生成Twitter 方案（Snowflake 算法）：41位时间戳+10位机器标识（比如IP，服务器名称等）+12位序列号(本地计数器)Flicker 方案：MySQL自增ID + “REPLACE INTO XXX:SELECT LAST_INSERT_ID();”UUID：缺点，无序，字符串过长，占用空间，影响检索性能。MongoDB 方案：利用 ObjectId。缺点：不能自增。在数据库中创建 sequence 表，用于记录，当前已被占用的id最大值。每台客户端主机取一个id区间（比如 1000~2000）缓存在本地，并更新 sequence 表中的id最大值记录。客户端主机之间取不同的id区间，用完再取，使用乐观锁机制控制并发。一致哈希 https://coderxing.gitbooks.io/architecture-evolution/content/di-san-pian-ff1a-bu-luo/631-yi-zhi-xing-ha-xi.html​ 一致性哈希（Consistent hashing）算法是由 MIT 的Karger 等人与1997年在一篇学术论文（《Consistent hashing and random trees: distributed caching protocols for relieving hot spots on the World Wide Web》）中提出来的，用于解决分布式缓存数据分布问题。在传统的哈希算法下，每条缓存数据落在那个节点是通过哈希算法和服务器节点数量计算出来的，一旦服务器节点数量发生增加或者介绍，哈希值需要重新计算，此时几乎所有的数据和服务器节点的对应关系也会随之发生变化，进而会造成绝大多数缓存的失效。一致性哈希算法通过环形结构和虚拟节点的概念，确保了在缓存服务器节点数量发生变化时大部分数据保持原地不动，从而大幅提高了缓存的有效性。下面我们通过例子来解释一致性哈希的原理。" }, { "title": "zinx 笔记", "url": "/posts/zinx-learning/", "categories": "cs", "tags": "zinx", "date": "2021-09-25 00:00:00 +0800", "snippet": "channel关闭 Always close a channel on the producer side永远在生产方关闭一个channel参考： 如何优雅地关闭 Go 中的工作 goroutine https://ictar.xyz/2019/10/19/trans-go-worker-cancellation/Go并发编程模型：主动停止goroutine https://zhuanlan.zhihu.com/p/66659719如何解包//处理客户端请求go func(conn net.Conn) { //创建封包拆包对象dp dp := znet.NewDataPack() for { //1 先读出流中的head部分 headData := make([]byte, dp.GetHeadLen()) _, err := io.ReadFull(conn, headData) //ReadFull 会把msg填充满为止 if err != nil { fmt.Println(&quot;read head error&quot;) break } //将headData字节流 拆包到msg中 msgHead, err := dp.Unpack(headData) if err != nil { fmt.Println(&quot;server unpack err:&quot;, err) return } if msgHead.GetDataLen() &amp;gt; 0 { //msg 是有data数据的，需要再次读取data数据 msg := msgHead.(*znet.Message) msg.Data = make([]byte, msg.GetDataLen()) //根据dataLen从io中读取字节流 _, err := io.ReadFull(conn, msg.Data) if err != nil { fmt.Println(&quot;server unpack data err:&quot;, err) return } fmt.Println(&quot;==&amp;gt; Recv Msg: ID=&quot;, msg.Id, &quot;, len=&quot;, msg.DataLen, &quot;, data=&quot;, string(msg.Data)) } }}(conn)参考 https://github.com/peigongdh/zinx" }, { "title": "zero-copy 笔记", "url": "/posts/zero-copy-note/", "categories": "cs, os", "tags": "zero-copy", "date": "2021-09-25 00:00:00 +0800", "snippet": "Linux I/O 原理和 Zero-copy 技术全面揭秘 https://zhuanlan.zhihu.com/p/308054212GO和java在web框架中的速度对比 https://www.zhihu.com/question/360929863/answer/2098404402" }, { "title": "wal 笔记", "url": "/posts/wal-note/", "categories": "cs, db", "tags": "wal", "date": "2021-09-25 00:00:00 +0800", "snippet": "WAL(Write Ahead Log)预写日志WAL是数据库系统中常见的一种手段，用于保证数据操作的原子性和持久性。在计算机科学中，「预写式日志」（Write-ahead logging，缩写 WAL）是关系数据库系统中用于提供原子性和持久性（ACID 属性中的两个）的一系列技术。在使用 WAL 的系统中，所有的修改在提交之前都要先写入 log 文件中。log 文件中通常包括 redo 和 undo 信息。这样做的目的可以通过一个例子来说明。假设一个程序在执行某些操作的过程中机器掉电了。在重新启动时，程序可能需要知道当时执行的操作是成功了还是部分成功或者是失败了。如果使用了 WAL，程序就可以检查 log 文件，并对突然掉电时计划执行的操作内容跟实际上执行的操作内容进行比较。在这个比较的基础上，程序就可以决定是撤销已做的操作还是继续完成已做的操作，或者是保持原样。参考 https://zhuanlan.zhihu.com/p/137512843" }, { "title": "urlencode 笔记", "url": "/posts/urlencode-note/", "categories": "cs", "tags": "urlencode", "date": "2021-09-25 00:00:00 +0800", "snippet": "urlencode这个问题涉及到URL的定义。我们知道URL是为了 统一的命名网络中的一个资源（URL不是单单为了HTTP协议而定义的，而是网络上的所有的协议都可以使用）。所以这就要求URL有一些基本的特性：URL是可移植的。（所有的网络协议都可以使用URL）URL的完整性。（不能丢失数据，比如URL中包含二进制数据时，如何处理）URL的可阅读性。（希望人能阅读）因为一些历史的原因URL设计者使用US-ASCII字符集表示URL。（原因比如ASCII比较简单；所有的系统都支持ASCII）为了满足URL的以上特性，设计者就将转义序列移植了进去，来实现通过ASCII字符集的有限子集对任意字符或数据进行编码。URL转义表示法包含一个百分号，后面跟上两个表示字符ASCII码的十六进制数值。现在URL转义表示法比较常用的有两个：RFC 2396 - Uniform Resource Identifiers (URI): Generic SyntaxRFC 3986 - Uniform Resource Identifier (URI): Generic Syntax对比base64与urlencode对比urlencode和base64都希望能够具备一定的可读性，严格来说base64只是要求便于使用文本传输区别在于，base64是二进制的映射，基本丢失了可读性，而urlencode的英文字符则不变，具备一定的可读性Base64URLconst encodeStd = &quot;ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789+/&quot;const encodeURL = &quot;ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789-_&quot;const xxxxxxxxx = &quot;tYsvKxjwABSD9-rhp_P7Ty6ZuWLamn2cOlJGg13oCVHQb5RIqief4N8UMkF0dEXz&quot;JWT 作为一个令牌（token），有些场合可能会放到 URL（比如 api.example.com/?token=xxx）。Base64 有三个字符+、/和=，在 URL 里面有特殊含义，所以要被替换掉：=被省略、+替换成-，/替换成_ 。这就是 Base64URL 算法。参考 https://www.zhihu.com/question/19673368/answer/71537081" }, { "title": "tls 笔记", "url": "/posts/tls-note/", "categories": "cs, network", "tags": "tls", "date": "2021-09-25 00:00:00 +0800", "snippet": "作用不使用SSL/TLS的HTTP通信，就是不加密的通信。所有信息明文传播，带来了三大风险。（1） 窃听风险（eavesdropping）：第三方可以获知通信内容。（2） 篡改风险（tampering）：第三方可以修改通信内容。（3） 冒充风险（pretending）：第三方可以冒充他人身份参与通信。SSL/TLS协议是为了解决这三大风险而设计的，希望达到：（1） 所有信息都是加密传播，第三方无法窃听。（2） 具有校验机制，一旦被篡改，通信双方会立刻发现。（3） 配备身份证书，防止身份被冒充。互联网是开放环境，通信双方都是未知身份，这为协议的设计带来了很大的难度。而且，协议还必须能够经受所有匪夷所思的攻击，这使得SSL/TLS协议变得异常复杂。过程SSL/TLS协议的基本思路是采用公钥加密法，也就是说，客户端先向服务器端索要公钥，然后用公钥加密信息，服务器收到密文后，用自己的私钥解密。但是，这里有两个问题。（1）如何保证公钥不被篡改？解决方法：将公钥放在数字证书中。只要证书是可信的，公钥就是可信的。（2）公钥加密计算量太大，如何减少耗用的时间？解决方法：每一次对话（session），客户端和服务器端都生成一个”对话密钥”（session key），用它来加密信息。由于”对话密钥”是对称加密，所以运算速度非常快，而服务器公钥只用于加密”对话密钥”本身，这样就减少了加密运算的消耗时间。因此，SSL/TLS协议的基本过程是这样的：（1） 客户端向服务器端索要并验证公钥。（2） 双方协商生成”对话密钥”。（3） 双方采用”对话密钥”进行加密通信。上面过程的前两步，又称为”握手阶段”（handshake）。参考 http://www.ruanyifeng.com/blog/2014/02/ssl_tls.html" }, { "title": "tcp 笔记3", "url": "/posts/tcp-note-3/", "categories": "cs, network", "tags": "tcp", "date": "2021-09-25 00:00:00 +0800", "snippet": "握手 发送方生成一个随机数‘X’发用一个SYN包到接收方 接收方对‘X’加一，生成一随机数‘Y’，返回一个SYN/ACK包 发送方增加‘X’、‘Y’，应答一个ACK包和应用数据。 ‘X’、‘Y’用于保证TCP数据包的序列，确保没有间断流控流控制是一种回退机制，用于防止发送方压倒接收方。接收方将等待应用程序处理的传入TCP数据包存储到接收缓冲区中。无论何时接收方接收到数据包，它都会将缓冲区的大小反馈给发送方。发送方，如果它尊重协议，避免发送更多的数据，可以容纳在接收方的缓冲区。这种机制与应用程序级别的速度限制并没有太大的不同。但是，TCP在连接级别上限制速率，而不是在API或IP地址上限制速率。 发送方和接收方之间的往返时间(RTT)越低，发送方就越快地根据接收方的容量调整其输出带宽。拥塞控制TCP不仅可以保护接收方，还可以保护底层网络。 发送方如何知道底层网络的可用带宽是多少?估计它的唯一方法是通过实际测量。其思想是发送方维护一个所谓的“拥塞窗口”。“该窗口表示未完成的数据包的总数，可以发送而无需等待来自另一方的确认。”接收器窗口的大小限制了拥塞窗口的最大大小。拥塞窗口越小，在任何给定时间可以传输的字节就越少，使用的带宽也就越少。当建立新连接时，拥塞窗口的大小被设置为系统默认值。然后，对于每一个被确认的包，窗口的大小会呈指数级增长。这意味着我们不能在连接建立后立即使用网络的全部容量。同样，往返时间越短，发送方就越快开始利用底层网络的带宽。如果丢了包怎么办?当发送方通过超时检测到错过的确认时，一种称为“拥塞避免”的机制就会启动，拥塞窗口大小就会减小。从那时起，时间将使窗口大小增加一定的数量，而超时将使窗口大小减少另一个数量。如前所述，拥塞窗口的大小定义了无需等待确认即可发送的最大比特数。发送方需要等待一个完整的往返过程才能获得确认。因此，通过将拥塞窗口的大小除以往返时间，我们可以得到最大的理论带宽:Bandwidth=WinSize/RTT这个简单的等式表明带宽是延迟的函数。TCP会非常努力地优化窗口大小，因为它对往返时间无能为力。但这并不总能得到最优的配置。总而言之，拥塞控制是一种自适应机制，用于推断网络的底层带宽和拥塞。在应用程序级别也可以应用类似的模式。想象一下你在Netflix上看电影时会发生什么。它开始模糊的;然后，它稳定到一个合理的水平，直到出现一个小问题，质量再次恶化。这种应用于视频流的机制称为自适应比特率流。握手挥手其他参考 https://blog.csdn.net/qzcsu/article/details/72861891握手 TCP服务器进程先创建传输控制块TCB，时刻准备接受客户进程的连接请求，此时服务器就进入了LISTEN（监听）状态； TCP客户进程也是先创建传输控制块TCB，然后向服务器发出连接请求报文，这是报文首部中的同部位SYN=1，同时选择一个初始序列号 seq=x ，此时，TCP客户端进程进入了 SYN-SENT（同步已发送状态）状态。TCP规定，SYN报文段（SYN=1的报文段）不能携带数据，但需要消耗掉一个序号。 TCP服务器收到请求报文后，如果同意连接，则发出确认报文。确认报文中应该 ACK=1，SYN=1，确认号是ack=x+1，同时也要为自己初始化一个序列号 seq=y，此时，TCP服务器进程进入了SYN-RCVD（同步收到）状态。这个报文也不能携带数据，但是同样要消耗一个序号。 TCP客户进程收到确认后，还要向服务器给出确认。确认报文的ACK=1，ack=y+1，自己的序列号seq=x+1，此时，TCP连接建立，客户端进入ESTABLISHED（已建立连接）状态。TCP规定，ACK报文段可以携带数据，但是如果不携带数据则不消耗序号。 当服务器收到客户端的确认后也进入ESTABLISHED状态，此后双方就可以开始通信了。挥手 客户端进程发出连接释放报文，并且停止发送数据。释放数据报文首部，FIN=1，其序列号为seq=u（等于前面已经传送过来的数据的最后一个字节的序号加1），此时，客户端进入FIN-WAIT-1（终止等待1）状态。 TCP规定，FIN报文段即使不携带数据，也要消耗一个序号。 服务器收到连接释放报文，发出确认报文，ACK=1，ack=u+1，并且带上自己的序列号seq=v，此时，服务端就进入了CLOSE-WAIT（关闭等待）状态。TCP服务器通知高层的应用进程，客户端向服务器的方向就释放了，这时候处于半关闭状态，即客户端已经没有数据要发送了，但是服务器若发送数据，客户端依然要接受。这个状态还要持续一段时间，也就是整个CLOSE-WAIT状态持续的时间。 客户端收到服务器的确认请求后，此时，客户端就进入FIN-WAIT-2（终止等待2）状态，等待服务器发送连接释放报文（在这之前还需要接受服务器发送的最后的数据）。 服务器将最后的数据发送完毕后，就向客户端发送连接释放报文，FIN=1，ack=u+1，由于在半关闭状态，服务器很可能又发送了一些数据，假定此时的序列号为seq=w，此时，服务器就进入了LAST-ACK（最后确认）状态，等待客户端的确认。 客户端收到服务器的连接释放报文后，必须发出确认，ACK=1，ack=w+1，而自己的序列号是seq=u+1，此时，客户端就进入了TIME-WAIT（时间等待）状态。注意此时TCP连接还没有释放，必须经过2∗ *∗MSL（最长报文段寿命）的时间后，当客户端撤销相应的TCB后，才进入CLOSED状态。 服务器只要收到了客户端发出的确认，立即进入CLOSED状态。同样，撤销TCB后，就结束了这次的TCP连接。可以看到，服务器结束TCP连接的时间要比客户端早一些。TCP状态机https://www.jianshu.com/p/3c7a0771b67e状态 状态 描述 LISTEN 等待来自远程TCP应用程序的请求 SYN_SENT 发送连接请求后等待来自远程端点的确认。TCP第一次握手后客户端所处的状态 SYN-RECEIVED 该端点已经接收到连接请求并发送确认。该端点正在等待最终确认。TCP第二次握手后服务端所处的状态 ESTABLISHED 代表连接已经建立起来了。这是连接数据传输阶段的正常状态 FIN_WAIT_1 等待来自远程TCP的终止连接请求或终止请求的确认 FIN_WAIT_2 在此端点发送终止连接请求后，等待来自远程TCP的连接终止请求 CLOSE_WAIT 该端点已经收到来自远程端点的关闭请求，此TCP正在等待本地应用程序的连接终止请求 CLOSING 等待来自远程TCP的连接终止请求确认 LAST_ACK 等待先前发送到远程TCP的连接终止请求的确认 TIME_WAIT 等待足够的时间来确保远程TCP接收到其连接终止请求的确认 TIME_WAITTIME_WAIT状态应该是最让人疑惑的一个状态了。在上图中可以看到，执行主动断开的节点最后会进入这个状态，该节点会在此状态保存2倍的MSL（最大段生存期）。TCP的每个实现都必须为MSL选择一个值。RFC 1122推荐的值为两分钟，伯克利派的实现使用30秒。这也就是说TIME_WAIT状态会维持1到4分钟。MSL是任何IP数据报可以在网络中生存的最长时间。这个时间是有限制的，因为每个数据报都包含一个8位的跳数限制，最大值是255.虽然这是一个跳数限制而不是一个真正的时间限制，但是根据这个限制来假设数据报的最长生命周期依然是有意义的。网络中数据报丢失的原因通常是路由异常。一旦路由崩溃或者两个路由之间的链路断开，路由协议需要几秒或几分钟才能稳定，并找到一条备用路径。在这段时间内，可能发生路由回路。同时假设丢失是一个TCP数据报，则发生TCP超时，并且重新发送分组，重传的分组通过一些备用路径达到最终目的地。但是一段时间后（该时间小于MSL），路由循环被更正，在循环中丢失的数据报被发送到最终目的地。这个原始的数据报被称为丢失的副本或漫游副本。TCP协议必须处理这些数据报。维持TIME_WAIT有两个原因：可靠地实现TCP的全双工连接终止。允许旧的重复数据段在网络中过期在四次挥手中，假设最后的ACK丢失了，被动关闭方会重发FIN。主动关闭端必须维护状态，来允许被动关闭方重发最后的ACK；如果它没有维护这个状态，将会对重发FIN返回RST，被动关闭方会认为这是个错误。如果TCP正在执行彻底终止数据流的两个方向所需的所有工作（即全双工关闭），则必须正确处理这四个段中任何一个的丢失。所以执行主动关闭的一方必须在结束时保持TIME_WAIT状态：因为它可能必须重传最后的ACK。现在来聊维持TIME_WAIT状态的第二个原因。假设在主机12.106.32.254的1500端口和206.168.112.219的21端口之间有一个TCP连接。此连接关闭后，在相通的地址和端口建立了另外一个连接。由于IP地址和端口相同，所以后一种连接被称为先前连接的“化身”。TCP必须防止连接中的旧副本在稍后再次出现，并被误解为属于同一连接的新“化身”。为此，TCP将不会启动当前处于TIME_WAIT状态的连接的新“化身”。由于TIME_WAIT状态的持续时间时两倍的MSL，因此TCP允许一个方向的数据在MSL秒内丢失，也允许回复在一个MSL秒内丢失。通过强制执行此规则，可以保证当一个TCP连接成功建立时，来自先前连接的所有旧的副本在网络中已过期。参考开发视角的TCP https://zhuanlan.zhihu.com/p/141468204" }, { "title": "tcp 笔记2", "url": "/posts/tcp-note-2/", "categories": "cs, network", "tags": "tcp", "date": "2021-09-25 00:00:00 +0800", "snippet": "1.TCP 协议保证可靠传输的手段 应用数据被分割成 TCP 认为最适合发送的数据块。 TCP 给发送的每一个包进行编号，接收方对数据包进行排序，把有序数据传送给应用层。 校验和：TCP 将保持它首部和数据的检验和。这是一个端到端的检验和，目的是检测数据在传输过程中的任何变化。如果收到段的检验和有差错，TCP 将丢弃这个报文段和不确认收到此报文段。 TCP 的接收端会丢弃重复的数据。 流量控制：TCP 连接的每一方都有固定大小的缓冲空间，TCP的接收端只允许发送端发送接收端缓冲区能接纳的数据。当接收方来不及处理发送方的数据，能提示发送方降低发送的速率，防止包丢失。TCP 使用的流量控制协议是可变大小的滑动窗口协议。 （TCP 利用滑动窗口实现流量控制） 拥塞控制：当网络拥塞时，减少数据的发送。 ARQ协议：也是为了实现可靠传输的，它的基本原理就是每发完一个分组就停止发送，等待对方确认。在收到确认后再发下一个分组。 超时重传：当 TCP 发出一个段后，它启动一个定时器，等待目的端确认收到这个报文段。如果不能及时收到一个确认，将重发这个报文段。2. ARQ协议自动重传请求（Automatic Repeat-reQuest，ARQ）是OSI模型中数据链路层和传输层的错误纠正协议之一。它通过使用确认和超时这两个机制，在不可靠服务的基础上实现可靠的信息传输。如果发送方在发送后一段时间之内没有收到确认帧，它通常会重新发送。ARQ包括停止等待ARQ协议和连续ARQ协议。2.1 停止等待ARQ协议停止等待协议是为了实现可靠传输的，它的基本原理就是每发完一个分组就停止发送，等待对方确认（回复ACK）。如果过了一段时间（超时时间后），还是没有收到 ACK 确认，说明没有发送成功，需要重新发送，直到收到确认后再发下一个分组；在停止等待协议中，若接收方收到重复分组，就丢弃该分组，但同时还要发送确认；优点：简单缺点：信道利用率低，等待时间长2.1.1 无差错情况发送方发送分组,接收方在规定时间内收到,并且回复确认.发送方再次发送。2.1.2 出现差错情况（超时重传）停止等待协议中超时重传是指只要超过一段时间仍然没有收到确认，就重传前面发送过的分组（认为刚才发送过的分组丢失了）。因此每发送完一个分组需要设置一个超时计时器，其重传时间应比数据在分组传输的平均往返时间更长一些。这种自动重传方式常称为自动重传请求 ARQ。另外在停止等待协议中若收到重复分组，就丢弃该分组，但同时还要发送确认。连续 ARQ 协议可提高信道利用率。发送维持一个发送窗口，凡位于发送窗口内的分组可连续发送出去，而不需要等待对方确认。接收方一般采用累积确认，对按序到达的最后一个分组发送确认，表明到这个分组位置的所有分组都已经正确收到了。2.1.3 确认丢失和确认迟到确认丢失：确认消息在传输过程丢失。当A发送M1消息，B收到后，B向A发送了一个M1确认消息，但却在传输过程中丢失。而A并不知道，在超时计时过后，A重传M1消息，B再次收到该消息后采取以下两点措施：1. 丢弃这个重复的M1消息，不向上层交付。 2. 向A发送确认消息。（不会认为已经发送过了，就不再发送。A能重传，就证明B的确认消息丢失）。确认迟到：确认消息在传输过程中迟到。A发送M1消息，B收到并发送确认。在超时时间内没有收到确认消息，A重传M1消息，B仍然收到并继续发送确认消息（B收到了2份M1）。此时A收到了B第二次发送的确认消息。接着发送其他数据。过了一会，A收到了B第一次发送的对M1的确认消息（A也收到了2份确认消息）。处理如下：1. A收到重复的确认后，直接丢弃。2. B收到重复的M1后，也直接丢弃重复的M1。2.2 连续ARQ协议连续 ARQ 协议可提高信道利用率。发送方维持一个发送窗口，凡位于发送窗口内的分组可以连续发送出去，而不需要等待对方确认。接收方一般采用累计确认，对按序到达的最后一个分组发送确认，表明到这个分组为止的所有分组都已经正确收到了。优点：信道利用率高，容易实现，即使确认丢失，也不必重传。缺点：不能向发送方反映出接收方已经正确收到的所有分组的信息。 比如：发送方发送了 5条 消息，中间第三条丢失（3号），这时接收方只能对前两个发送确认。发送方无法知道后三个分组的下落，而只好把后三个全部重传一次。这也叫 Go-Back-N（回退 N），表示需要退回来重传已经发送过的 N 个消息。3. 滑动窗口和流量控制TCP 利用滑动窗口实现流量控制。流量控制是为了控制发送方发送速率，保证接收方来得及接收。接收方发送的确认报文中的窗口字段可以用来控制发送方窗口大小，从而影响发送方的发送速率。将窗口字段设置为 0，则发送方不能发送数据。4. 拥塞控制在某段时间，若对网络中某一资源的需求超过了该资源所能提供的可用部分，网络的性能就要变坏。这种情况就叫拥塞。拥塞控制就是为了防止过多的数据注入到网络中，这样就可以使网络中的路由器或链路不致过载。拥塞控制所要做的都有一个前提，就是网络能够承受现有的网络负荷。拥塞控制是一个全局性的过程，涉及到所有的主机，所有的路由器，以及与降低网络传输性能有关的所有因素。相反，流量控制往往是点对点通信量的控制，是个端到端的问题。流量控制所要做到的就是抑制发送端发送数据的速率，以便使接收端来得及接收。为了进行拥塞控制，TCP 发送方要维持一个拥塞窗口(cwnd)的状态变量。拥塞控制窗口的大小取决于网络的拥塞程度，并且动态变化。发送方让自己的发送窗口取为拥塞窗口和接收方的接受窗口中较小的一个。TCP的拥塞控制采用了四种算法，即慢开始、拥塞避免、快重传和快恢复。在网络层也可以使路由器采用适当的分组丢弃策略（如主动队列管理 AQM），以减少网络拥塞的发生。慢开始：慢开始算法的思路是当主机开始发送数据时，如果立即把大量数据字节注入到网络，那么可能会引起网络阻塞，因为现在还不知道网络的符合情况。经验表明，较好的方法是先探测一下，即由小到大逐渐增大发送窗口，也就是由小到大逐渐增大拥塞窗口数值。cwnd初始值为1，每经过一个传播轮次，cwnd加倍。拥塞避免：拥塞避免算法的思路是让拥塞窗口cwnd缓慢增大，即每经过一个往返时间RTT就把发送放的cwnd加1.快重传与快恢复：在 TCP/IP 中，快速重传和恢复（fast retransmit and recovery，FRR）是一种拥塞控制算法，它能快速恢复丢失的数据包。没有 FRR，如果数据包丢失了，TCP 将会使用定时器来要求传输暂停。在暂停的这段时间内，没有新的或复制的数据包被发送。有了 FRR，如果接收机接收到一个不按顺序的数据段，它会立即给发送机发送一个重复确认。如果发送机接收到三个重复确认，它会假定确认件指出的数据段丢失了，并立即重传这些丢失的数据段。有了 FRR，就不会因为重传时要求的暂停被耽误。 　当有单独的数据包丢失时，快速重传和恢复（FRR）能最有效地工作。当有多个数据信息包在某一段很短的时间内丢失时，它则不能很有效地工作。参考快速重传算法 https://blog.csdn.net/wdscq1234/article/details/52529994TCP 协议如何保证可靠传输 https://segmentfault.com/a/1190000022944999" }, { "title": "tcp 笔记1", "url": "/posts/tcp-note-1/", "categories": "cs, network", "tags": "tcp", "date": "2021-09-25 00:00:00 +0800", "snippet": "正题我想说说之前一直困扰我很久的一个问题“阻塞、非阻塞、同步、异步”。关于“阻塞、非阻塞、同步、异步”的区别，网上众说纷纭，其中很大一部分人喜欢用一个生活中的比喻来解释。但我认为，比喻虽然生动形象，但是毕竟与计算机技术的实际场景有所不同，导致很多人在阅读的过程中看懂了例子，却还是弄不明白“阻塞、非阻塞、同步、异步”。因此本文中我将不在使用比喻，而是列举“阻塞、非阻塞、同步、异步”相关的例子来解释说明。字面意思解释好，要开始了，接下来的内容请仔细阅读。首先，我们先来看看这四个词语本身的含义。先来看一张表： 词语 描述的主语 含义 阻塞 调用者的状态 代表调用者会被卡住，不会继续向下执行 非阻塞 调用者的状态 代表调用者不会被卡住，会继续向下执行 同步 两者主题之间的合作模式 两者之间串行工作 异步 两者主题之间的合作模式 两者之间并行工作 从这张表我们可以看出，阻塞与非阻塞描述的主语是一样的，同步和异步描述的主语是一样的。所以，可以得出第一个结论，本质上“阻塞和非阻塞”与“同步和异步”之间并没有必然的关系。对应的编程技术接下来，我们来聊聊“阻塞、非阻塞、同步、异步”在我们的计算机编程中所代表的东西。先说结论： 结论1：阻塞和非阻塞代表是否为打开的文件描述符设置了O_NONBLOCK属性。 结论2：同步和异步是指在对文件描述符进行IO操作时，是否使用了epoll等多路复用技术或者AIO。 如果不清楚我在说什么，建议看看网络编程相关的知识。如果看懂了之前描述“阻塞、非阻塞、同步、异步”的那张表格，也知道O_NONBLOCK属性, 多路复用, AIO，但还是对我说的两条结论不太理解的话，也别着急，听我解释。结论1首先我们知道阻塞和非阻塞的主语是调用者，而文件描述符的调用者或者说操作者是打开和操作文件描述符的程序，因此此处O_NONBLOCK描述的便是操作文件描述符的程序的状态。举个栗子，假设发生了如下事件： 程序A打开了网络连接C，默认情况下C的描述符的属性为阻塞模式，此处使用默认的阻塞模式 程序A试图在没有数据到来的情况下从C中读取数据。 有数据到达C。 在默认阻塞模式下，第2步时程序A便会卡住，直到读到数据或者读取超时。而如果我们为C的文件描述符设置了O_NONBLOCK模式，则第2步时，程序A不会被卡住，而是通过read的返回值被告知当前没有可以读取的数据。程序A拿到了这个告知后便可以选择是继续等待，还是去执行其他的任务了，而不会被一直卡住。这就是“阻塞和非阻塞”在编程时对应的东西。结论2还是先说说这两个词本身，“同步和异步”这两个词所描述的是两个主体之间的合作模式，而针对文件描述符的操作来说，这两个主体便是指操作文件描述符的程序和文件描述符所代表的资源管理者。接着用上面的例子，“同步和异步”所描述便是两个主体——程序A和连接C——之间的合作模式。在同步模式下，程序A的行为和连接C的行为是串行进行的，即在步骤2时，程序A调用read接口去读取数，此时控制权交给了连接C，连接C根据是否有数据以及是否使用了O_NONBLOCK来决定什么时候将控制权交给程序A。无论何种情况，只要C不将控制权交给A，A便无法处理其他事情，所以说两者是串行执行的。而如果使用了epoll或AIO，则两者的合作模式将会发生改变，即程序A只有在收到连接C的通知时，才会去读取或写入C，而在没有通知时，程序A可以去处理其他的事情或其他消息，因此两者之间是并行的。非阻塞与异步的“特殊”关系但是有一点要注意，虽说“阻塞和非阻塞”与“同步和异步”之间并没有必然的关系，但在使用时，部分的异步处理方式需要使用非阻塞模式来支撑。具体关系为：使用epoll时，分两种情况（如果不了解，请参考unix网络编程）.边缘触发时，必须使用非阻塞IO。由于此时每次新数据到来只会触发一次epoll事件，无论新数据有没有读取都是这样。因此需要不停地read读取所有数据，如果使用阻塞模式，肯达概率会在最后一次读取时由于无数据可读而卡住。水平触发时，阻塞和非阻塞IO都可以。该模式下，主要有可以读取的数据，便会触发epoll事件，而不管数据是新到来的还是上一次未读完的。因此一般在有epoll事件触发时才会去读取，所以不会阻塞，因为肯定能读到数据。另外，即使read指定读取的数据量大于实际收到的数据量，也不会阻塞，而是在read操作结束时返回实际read的数据量大小。所以要注意的是，一次触发epoll事件只能对应一个read或write操作，如果一次触发进行了多次read或write，则会有阻塞的风险。使用aio时，阻塞和非阻塞IO都可以。Epoll和AIO的区别最后，在简单说一下Epoll和AIO的区别：通知的含义不同。Epoll 是在连接打开、关闭、可读、可写的时候，收到通知；AIO 是在读写（通过 aio_read 或 aio_write 发起）完成之后（将要读的内容读到 application 指定的位置或将 application 指定的位置的内容写入成功）的通知。响应通知的 execution context 不同。epoll 是在调用线程中执行；而 aio 可以以 signal 、new thread 的方式来响应通知。TODO边沿触发/电平触发数字电路的概念，参考： https://www.zhihu.com/question/53447316 首先要知道触发是啥，说的简单点，就是信号来了以后，我什么时候采集它呀？最好的办法就是根据时钟来采集，因为时钟是稳定的，而且一般一组信号只有一个时钟，大家都用时钟来采集，就可以做到同步了。一个时钟可以看做是由 上升沿，高电平，下降沿，低电平 四个部分组成。下降沿触发是时钟由高变成低的这个过程，而低电平它就只是一个状态。在一般情况下，一个时钟下降沿这个过程的时间，是比低电平这个状态持续的时间短的多的。所以好处看到了吧，在下降沿这个过程采样，信号会在这个过程中变化的概率非常小；而用低电平采样，信号在低电平这个状态持续时间中变化的概率很高。如果信号有毛刺，产生波动等等，你在低电平中，都能够触发到，会对你的结果产生影响。 下降沿并不是clk等于0的状态，而是从1跳变到0的状态。必须检测到跳变才会触发。低电平触发是连续六次采样都为低就触发。 边沿触发，只会在电平变化时出现一次中断。电平触发，只要电平为触发电平，会一直触发中断，即使清零了，还会再次进入。参考 https://zhuanlan.zhihu.com/p/88728018" }, { "title": "system-design目录", "url": "/posts/system-design-2021-note/", "categories": "cs, system-design", "tags": "system-design", "date": "2021-09-25 00:00:00 +0800", "snippet": "目录 优惠券系统设计 发券接口限流保护 数据库扩展与一致哈希算法 一致哈希算法的两个版本 如何设计限流器 如何设计实时数据系统 分布式文件系统GFS master slave 的设计模式 分布式文件系统的读写流程 怎么处理分布式系统中的failure和recovery的问题 如何做replica, check sum检查 了解consistent hash和sharding的实际应用 文档协同编辑系统设计 websocket在协同编辑中的应用 了解什么是协同编辑 协同编辑的集中实现方案 如何解决编辑冲突问题 了解 OT（ Operational Transformation）原理 分布式数据库 Big Table 通过设计分布式数据库系统Bigtable了解如下内容： Big Table 的原理与实现 了解NoSQL Database如何进行读写操作的,以及相应的优化 了解如何建立index 学习Bloom Filter的实现原理 Master Slave 的设计模式 聊天系统 IM System 聊天系统中的 Pull vs Push 讲解一种特殊的 Service - Realtime Service 用 channel 优化群聊 如何限制多机登录 用户在线状态的获取与查询 Online Status 视频流系统设计 视频切分和断点续传如何实现 如何在架构设计中节省带宽 小文件存储之视频切片与缩略图存储 了解视频预加载 了解 CDN 的基本原理 基于地理位置的信息系统 系统学习LBS相关系统设计的核心要点： 地理位置信息存储与查询常用算法之 Geohash 如何设计 Uber 关键点：学会设计 Uber 以后可以轻松解决设计 Facebook Nearby 和 Yelp 分布式计算 Map Reduce 学习Map Reduce 的应用与原理 了解如何多台机器并行解决算法问题 掌握Map和Reduce的原理 通过三个题目掌握MapReduce算法实现： WordCount InvertedIndex Anagram Twitter Search 推特的海量推文数据如何存储 如何快速搜索 对搜索结果进行排名 搜索系统容错能力 资源课程 https://www.jiuzhang.com/course/77/system-design-primer https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.mdgrokking-the-system-design-interview https://www.educative.io/courses/grokking-the-system-design-interview上级课程大纲：scalability-system-designhttps://www.educative.io/path/scalability-system-design延伸推荐OOD：Grokking the Object Oriented Design Interview https://www.educative.io/courses/grokking-the-object-oriented-design-interview?aff=K7qB" }, { "title": "shadowsocks配置", "url": "/posts/shadowsocks-note/", "categories": "cs", "tags": "shadowsocks", "date": "2021-09-25 00:00:00 +0800", "snippet": "shadowsocks/usr/local/bin/ss-server -v -c /etc/shadowsocks-libev/config.json -f /var/run/shadowsocks-libev.pid# supervisorctl[program:shadowsocks]user=nobodydirectory=/usr/local/shadowsockscommand=/usr/local/bin/ss-server -v -c /etc/shadowsocks-libev/config.jsonprocess_name=%(program_name)sautostart=trueredirect_stderr=truestdout_logfile=/var/log/shadowsocks/server.logstdout_logfile_maxbytes=1MBstdout_logfile_backups=0[program:kcptun]user=kcptundirectory=/usr/local/kcptuncommand=/usr/local/kcptun/server_linux_amd64 -c &quot;/usr/local/kcptun/server-config.json&quot;process_name=%(program_name)sautostart=trueredirect_stderr=truestdout_logfile=/var/log/kcptun/server.logstdout_logfile_maxbytes=1MBstdout_logfile_backups=0kcptun操作zhangpei supervisorctlkcptun RUNNING pid 2459, uptime 655 days, 12:23:06supervisor&amp;gt; stop kcptunkcptun: stoppedsupervisor&amp;gt; start kcptunkcptun: startedsupervisor&amp;gt; statuskcptun RUNNING pid 8690, uptime 0:00:04supervisor&amp;gt; statuskcptun RUNNING pid 8690, uptime 0:00:11supervisor&amp;gt; quit参考配置参数说明： https://www.wangjunfeng.com.cn/2019/11/11/kcp-tun-config/使用脚本： https://github.com/kuoruan/shell-scripts" }, { "title": "ray-tracing 笔记", "url": "/posts/rasterization-and-ray-tracing-note/", "categories": "cs, cg", "tags": "ray-tracing", "date": "2021-09-25 00:00:00 +0800", "snippet": "光线追踪参考：什么是光线追踪，路径追踪，降噪？ https://zhuanlan.zhihu.com/p/123866941 相比光追，光栅化最大的特点之一在于，它必须要有相机。在光栅化渲染中，要把相机放在待渲染的三角面片前，使用投影矩阵，经过变换以适应屏幕，将其渲染为2D画面上的各个像素。 而光线追踪可以执行渲染所需的计算，跟相机无关。这就是光线追踪具有高反射的原因。在光栅化过程中（当然也有其他的trick），我们只有在安装了额外的相机（如镜子）的情况下，才能正确地计算反射。而光线追踪可从任何地方发射光线，所以我们能用光追计算在任意一点上的反射。视频介绍： https://zhuanlan.zhihu.com/p/165539118rust-ray-tracing https://github.com/peigongdh/rust-ray-tracing-demo https://raytracing.github.io/books/RayTracingInOneWeekend.html https://zhuanlan.zhihu.com/p/152300191演示demo https://lucifier129.github.io/rust-ray-tracing-demo/ray-tracing-web/build/" }, { "title": "oauth 笔记", "url": "/posts/oauth-note/", "categories": "cs", "tags": "oauth", "date": "2021-09-25 00:00:00 +0800", "snippet": "令牌与密码令牌（token）与密码（password）的作用是一样的，都可以进入系统，但是有三点差异。（1）令牌是短期的，到期会自动失效，用户自己无法修改。密码一般长期有效，用户不修改，就不会发生变化。（2）令牌可以被数据所有者撤销，会立即失效。以上例而言，屋主可以随时取消快递员的令牌。密码一般不允许被他人撤销。（3）令牌有权限范围（scope），比如只能进小区的二号门。对于网络服务来说，只读令牌就比读写令牌更安全。密码一般是完整权限。上面这些设计，保证了令牌既可以让第三方应用获得权限，同时又随时可控，不会危及系统安全。这就是 OAuth 2.0 的优点。注意，只要知道了令牌，就能进入系统。系统一般不会再次确认身份，所以令牌必须保密，泄漏令牌与泄漏密码的后果是一样的。 这也是为什么令牌的有效期，一般都设置得很短的原因。OAuth 2.0 对于如何颁发令牌的细节，规定得非常详细。具体来说，一共分成四种授权类型（authorization grant），即四种颁发令牌的方式，适用于不同的互联网场景。RFC 6749OAuth 2.0 的标准是 RFC 6749 文件。该文件先解释了 OAuth 是什么。OAuth 引入了一个授权层，用来分离两种不同的角色：客户端和资源所有者。……资源所有者同意以后，资源服务器可以向客户端颁发令牌。客户端通过令牌，去请求数据。这段话的意思就是，OAuth 的核心就是向第三方应用颁发令牌。然后，RFC 6749 接着写道：（由于互联网有多种场景，）本标准定义了获得令牌的四种授权方式（authorization grant ）。也就是说，OAuth 2.0 规定了四种获得令牌的流程。你可以选择最适合自己的那一种，向第三方应用颁发令牌。下面就是这四种授权方式。授权码（authorization-code）隐藏式（implicit）密码式（password）：客户端凭证（client credentials）注意，不管哪一种授权方式，第三方应用申请令牌之前，都必须先到系统备案，说明自己的身份，然后会拿到两个身份识别码：客户端 ID（client ID）和客户端密钥（client secret）。这是为了防止令牌被滥用，没有备案过的第三方应用，是不会拿到令牌的。 http://www.ruanyifeng.com/blog/2019/04/oauth-grant-types.htmlDemoGitHub OAuth 第三方登录示例教程 http://www.ruanyifeng.com/blog/2019/04/github-oauth.html" }, { "title": "句柄 笔记", "url": "/posts/handle-note/", "categories": "cs, os", "tags": "file, handler", "date": "2021-09-25 00:00:00 +0800", "snippet": "背景查看系统中文件句柄的数量：$ cat /proc/sys/fs/file-nr203136 0 19593935通过lsof计算文件描述符数量$lsof | wc -l253/proc/sys/fs/file-nr这里所展示的数据第一列-&amp;gt;已分配文件句柄的数目，第二列-&amp;gt;已使用文件句柄的数目，第三列-&amp;gt;文件句柄的最大数目，通过下面这个文件也可以看到文件句柄的最大数目$ cat /proc/sys/fs/file-max 19593935那问题来了： 什么是文件描述符，什么是句柄？ 为什么文件描述符和句柄的大小要一样？ 为什么文件描述符和句柄的大小要不一样？ 上面的问题不一定都对，但是可以带着思考往下看句柄Windows下的概念。句柄是Windows下各种对象的标识符，比如文件、资源、菜单、光标等等。文件句柄和文件描述符类似，它也是一个非负整数，也用于定位文件数据在内存中的位置。由于Linux下所有的东西都被看成文件，所以Linux下的文件描述符其实就相当于Windows下的句柄。文件句柄只是Windows下众多句柄中的一种类型而已文件描述符本质上是一个索引号（非负整数），系统用户层可以根据它找到系统内核层的文件数据。这是一个POSIX标准下的概念，常见于Linux系统。内核（kernel）利用文件描述符来访问文件。打开现存文件或新建文件时，内核会返回一个文件描述符。读写文件也需要文件描述符来指定待读写的文件。两者比较 一个描述符上可以监控很多的事件，如果监控的是read，然而write事件到来的时候也会将该描述符唤醒 lsof在用户空间，主要还是从文件描述符的角度来看文件句柄简单来说，每个进程都有一个打开的文件表（fdtable)。表中的每一项是struct file类型，包含了打开文件的一些属性比如偏移量，读写访问模式等，这是真正意义上的文件句柄。每个文件描述符都与一个文件所对应的，不同的文件描述符也可能会指向同一个文件，相同的文件描述符也可能会指向不同的文件。这个主要是因为同一个文件可以被不同的进程打开，也可以被同一个进程的多次打开。系统为每一个进程维护了一个文件描述符表，所以在不同的进程中会看到相同的fd。那么要理解具体的内部结构，需要理解下面这三个数据结构： 进程级的文件描述符表 系统级的打开文件描述符表 文件系统i-node表进程级别的文件描述符表每一条都只是记录了单个文件描述符的信息，主要包含下面几个： 控制文件描述符操作的一组标志 对打开文件句柄的引用内核对所有打开的文件都有一个系统级的文件描述符表，各条目称为打开文件句柄（open file handle）一个打开文件句柄存储了一个与一个打开文件相关的所有信息 当前文件偏移量（调用read()和write()时更新，或使用lseek()直接修改） 打开文件时的标识（open()的flags参数） 文件访问模式（读写） 与信号驱动相关的设置 对该文件i-node对象的引用 文件类型（常规文件、socket等）和访问权限 一个指针，指向该文件所持有的锁列表 文件的各种属性，时间戳啥的，巴拉巴拉进程文件描述符 -&amp;gt; 系统级文件描述符 -&amp;gt; i-node表图标略（见引用文章）在进程A中，文件描述符1和30都指向了系统文件描述符表中的21句柄，这可能是通过dup() dup2() fcnt()或者对同一个文件调用了多次open()导致的进程A的文件描述符2和进程B的文件描述符2都指向了系统文件描述符的30句柄，这可能是fork()的作用，出现了父子进程进程A的文件描述符0和进程B的文件描述符3分别指向了不同的句柄，但是最后这两个句柄都指向了i-node表中的66，也就是指向了同一个文件。这是因为进程A和B各自同一个文件发起了open()的操作总结 由于进程级的文件描述符表的存在，不同的进程中会出现相同的文件描述符，而相同的文件描述符会指向不同句柄 两个不同的文件描述符，如果指向同一个句柄，就会共享同一个文件偏移量，因此，如果其中一个文件描述符修改了偏移量（调用了read、write、lseek），另一个描述符也会感受到变化 文件描述符标志为进程和文件描述符私有，这一标志的修改不会影响同一进程或不同进程的其他文件描述符 句柄其实是指针的指针。操作系统分为用户态和内核态，用户必然会使用内核的数据，但是用户进程又不能直接获取内核数据结构，所以才出现了指针的指针——句柄，用户态通过句柄调用系统服务，进入内核之后，内核根据句柄找到真正的数据结构回顾问题 什么是文件描述符，什么是句柄？——这个问题已经得到了解答 为什么文件描述符和句柄的大小要一样？——刚开始我将文件描述符（fd）和句柄理解成一个概念，以为这两个是一个东西的不同说法。文件描述符主要还是用户态的东西，而句柄主要是内核态的东西，从上面的图就可以理解出来 为什么文件描述符和句柄的大小要不一样？——多个文件描述符可以指向同一个句柄，每个进程都会拥有一个文件描述符表。而lsof命令会漏掉很多数据：共享内存等。lsof在用户空间，主要还是从文件描述符的角度来看文件句柄。参考 https://www.jianshu.com/p/0ff9ff1d108e" }, { "title": "fork-exec 笔记", "url": "/posts/fork-exec-note/", "categories": "cs, os", "tags": "fork exec", "date": "2021-09-25 00:00:00 +0800", "snippet": "fork使用fork完，会创建一个子进程，内部处理逻辑，是让这俩进程共享代码段，同时，会新复制数据段和堆栈段（这里实际上没有做复制操作，只有在写新数据的时候，才会往不同地方写）调用完fork，返回pid，对于父进程而言，pid&amp;gt;0，是子进程的pid。对于子进程而言，pid=0调用完fork，对于父进程和子进程，会从下面一条语句开始执行。这里非常奇妙，相当于实时备份了一整个进程状态。exec一个进程一旦调用exec系列函数，它本身就相当于死亡了，系统会把代码段替换成新的程序的代码，废弃原有的数据段和堆栈段，并为新程序分配新的 数据段和堆栈段。但是，exec完，进程pid不会变，复用了父进程的pid，整个过程相当于把父进程覆盖掉了。一旦exec执行成功，原来父进程后面的代码，相当于没有任何用了，不会再执行了，强制切换到了新进程中去从头开始执行代码。如果exec执行失败，还是会返回错误信息的。设计理念接口设计的一个指导原则是“完整且最小”。“完整”的意思是，对外提供的接口必须能够满足任何使用要求。“最小”的意思是，接口功能无重复（无重叠），以能达到“正交化”水平为最佳。“完整且最小”的接口未必好用。有时候，为了使用方便或者性能或者其它种种原因，接口可以出现冗余；但冗余必然带来维护/学习等方面的代价。linux的fork/exec就是一组典型的“完整且最小”的接口。“fork”用来产生一个新进程，这个进程默认会复制自身——于是类似apache这样用到“进程池”的场景得到支持。“fork”的“复制自身”操作又是“悬挂”的，如果紧接着调用exec，复制就会取消——于是启动另外一个进程的场景得到支持。“exec”则是“启动参数指定的程序，代替自身进程”。如果不配合fork使用，它是“当前进程结束，执行指定进程”；配合fork使用，就成了“当前进程启动另一个进程”。这样一来，各种使用场景就都得到了支持；再加上内部优化，写出“性能绝佳”的“多进程协作”程序就成了可能——于是linux甚至有相当一段时间都不支持线程，因为“fork的效率实在太好了，没必要支持线程”（另一个后遗症是，虽然现在linux内核有了线程支持，但线程和fork之间的关系极为复杂，以至于几乎只能在多进程/多线程两个方案中间选择其一）。当然，为了便于使用，linux也提供了一个用起来简单一些的system调用。它和createProcess有点像，但内部仍然由fork+exec实现；此外，它执行时是阻塞的，同时还可能有很多信号之类的技术问题需要处理。现在，我们拿fork+exec和windows对比一下。如果我们需要做一个“多进程协作”的网络服务框架，要求每个工作进程在处理了若干次服务请求后退出（这是个很讨巧的、保证系统7X24稳定性的经典设计）；但相应的，这就对进程创建效率提出了极高要求（哪怕按处理50个请求退出、且请求频率是相当初级的5000次/秒，也需要每秒创建100个进程；更不要说后来更为丧心病狂的10K、100K问题了）——这种设计在windows上也能保证性能吗？为什么？当然不行。因为windows相关API被封装的太“重”了。它用起来的确方便；但方便是有代价的。起码它不能“完整”的支持多进程协作的高性能服务框架……linux的fork/exec方案也有缺陷。它的机制比较复杂，使得初学者难以理解；另外就是，当线程出现后，这个方案和多线程八字不合，混用的话有许多许多的坑等着你。反倒是windows，一旦有了线程支持，多进程互相监视保证稳定性、多线程同时执行提高效率，各种使用场景就全都被覆盖了——换句话说，现在它“完整”了，你改下方案就行。所以你看，过去毋庸置疑是linux的接口更灵活更强大（当然了，不是方便初学者的那种强大）；可一旦多了线程支持，windows方案反倒显得更“正交”了 https://www.zhihu.com/question/66902460参考 https://keenjin.github.io/2019/05/linux_fork_and_exec/" }, { "title": "flink安装 笔记", "url": "/posts/flink-install-note/", "categories": "cs", "tags": "flink", "date": "2021-09-25 00:00:00 +0800", "snippet": "flinkhttps://ci.apache.org/projects/flink/flink-docs-master/deployment/resource-providers/standalone/local.htmlmac安装过程中，需要手动 brew install mvnvn 进入flink目录，手动修改pom.xml，指定mvn版本&amp;gt;3.6.0，否则可能报错mvn archetype:generate -DarchetypeGroupId=org.apache.flink -DarchetypeArtifactId=flink-walkthrough-datastream-java -DarchetypeVersion=1.13-SNAPSHOT -DgroupId=frauddetection -DartifactId=frauddetection -Dversion=0.1 -Dpackage=spendreport -DinteractiveMode=false按照手册设置settings.xml全局配置 https://ci.apache.org/projects/flink/flink-docs-master/try-flink/datastream_api.html./bin/start-cluster.sh启动FraudDetectionJob使用如下命令：mvn exec:java -Dexec.mainClass=spendreport.FraudDetectionJobkafkamac安装kafka==&amp;gt; zookeeperTo have launchd start zookeeper now and restart at login: brew services start zookeeperOr, if you don’t want/need a background service you can just run: zkServer start==&amp;gt; kafkaTo have launchd start kafka now and restart at login: brew services start kafkaOr, if you don’t want/need a background service you can just run: zookeeper-server-start /usr/local/etc/kafka/zookeeper.properties &amp;amp; kafka-server-start /usr/local/etc/kafka/server.properties https://colobu.com/2019/09/27/install-Kafka-on-Mac/启动异常参考： https://blog.csdn.net/ASN_forever/article/details/104872917zookeeper-server-start /usr/local/etc/kafka/zookeeper.properties &amp;amp; kafka-server-start /usr/local/etc/kafka/server.propertieszkServer startkafka-server-start /usr/local/etc/kafka/server.propertieskafka-console-producer --broker-list localhost:9092 --topic test-flink-1kafka-console-consumer --bootstrap-server localhost:9092 --topic test-flink-1 --from-beginningprometheus/usr/local/bin/prometheus --config.file=/usr/local/etc/prometheus.ymlcd pushgateway-1.4.0.darwin-amd64./pushgateway安装prometheus-gateway https://cloud.tencent.com/developer/article/1690610" }, { "title": "文件描述符 笔记", "url": "/posts/file-descriptor-note/", "categories": "cs, os", "tags": "file, file-descriptor", "date": "2021-09-25 00:00:00 +0800", "snippet": "获取文件描述符从形式上来看文件描述就是一个整数，那么我们可不可以更进一步去了解一下呢？本文打算通过一步一步实验去了解文件描述符到底是什么， 并在最后通过Linux内核相关的源码进行验证。int main(int argc, char* argv[]) { // 以只读模式打开 demo.txt 文件 int fd = open(&quot;demo.txt&quot;, O_RDONLY); if (fd == -1) { perror(&quot;open demo.txt error\\n&quot;); return EXIT_FAILURE; } // 打印获取到的文件描述符 printf(&quot;demo.txt fd = %d \\n&quot;, fd); return EXIT_SUCCESS;}$ echo hello&amp;gt;&amp;gt;demo.txt$ gcc test.c -o -test$ ./test$ demo.txt fd = 3和方法签名一致，文件描述符是一个整型数。你可以尝试多次执行该程序， 你会发现打印的文件描述符始终都是 3。难道每个文件的文件描述符都是固定的？文件描述符是否固定继续实验，同时调用两次openint main(int argc, char* argv[]) { int fd_a = open(&quot;demo.txt&quot;, O_RDONLY); int fd_b = open(&quot;demo.txt&quot;, O_RDONLY); printf(&quot;fd_a = %d, fd_b = %d \\n&quot;, fd_a, fd_b); return EXIT_SUCCESS;}$ gcc test.c -o test$ ./test$ fd_a = 3, fd_b = 4尽管是同一个文件， 得到的文件描述符却并不一样，说明每个文件的描述符并不是固定的。可是文件描述符每次都是从 3 开始的，这是为什么呢 ？熟悉UNIX系统的同学应该知道，系统创建的每个进程默认会打开3个文件：标准输入(0)标准输出(1)标准错误(2)为什么是 3 ？ 因为 0、1、2 被占用了啊……文件描述符是否是递增调用两次 open ， 分别得到两个文件描述符 3、 4调用 close 函数将文件描述符 3 关闭再次调用 open 函数打开同一个文件int main(int argc, char* argv[]) { // 第一次打开 int a = open(&quot;demo.txt&quot;, O_RDONLY); // 第二次打开 int b = open(&quot;demo.txt&quot;, O_RDONLY); printf(&quot;a = %d, b = %d \\n&quot;, a, b); // 关闭a文件描述符 close(a); // 第三次打开 int c = open(&quot;demo.txt&quot;, O_RDONLY); printf(&quot;b = %d, c = %d \\n&quot;, b, c); return EXIT_SUCCESS;}$ gcc test.c -o test$ ./test$ a = 3, b = 4 b = 4, c = 3第三次打开的结果是 3 ，这说明文件描述符不是递增的。而且从结果上来看，文件描述符被回收掉后是可以再次分配的。文件描述符和多进程int main(int argc, char* argv[]) { int npid = fork(); if (npid == 0 ){ // 子进程 int child_fd = open(&quot;demo.txt&quot;, O_RDONLY); pid_t child_pid = getpid(); printf(&quot;child_pid = %d, child_fd = %d \\n&quot;, child_pid, child_fd); } else { // 父进程 int parent_fd = open(&quot;demo.txt&quot;, O_RDONLY); pid_t parent_pid = getpid(); printf(&quot;parent_pid = %d, parent_fd = %d \\n&quot;, parent_pid, parent_fd); } return EXIT_SUCCESS;}$ gcc test_process.c -o test_process$ ./test_process$ child_pid = 28212, child_fd = 3 parent_pid = 28210, child_fd = 3每个进程打开的都是同一个文件，而且返回的文件描述符也是一样的。前面我们已经得知每个文件的描述符并不是固定的，这样看来，每个进程都单独维护了一个文件描述符的集合啊。还记得最开始实验时，我们对编译好的程序多次执行都是打印的 3，但是在代码里对同一个文件 open 两次却是返回的 3 和 4 吗？这是因为在 shell 每次执行程序，其实都是创建了一个新的进程在执行。而在代码里连续调用两次，始终是在一个进程下执行的。深入内核既然实验表明每个进程单独维护了文件描述符集合， 那就从和进程相关的结构体 task_struct 入手，该结构体放在 /include/linux/sched.h 头文件中。我将这个结构体的代码精简了一下， 只保留了一些分析需要关注的属性struct task_struct { ... /* Filesystem information: */ struct fs_struct *fs; /* Open file information: */ struct files_struct *files; ... /* -1 unrunnable, 0 runnable, &amp;gt;0 stopped: */ volatile long state; pid_t pid; pid_t tgid; ...};注意 struct files_struct *files ，注释说该属性代表着打开的文件信息，那这就没得跑了。继续看 files_struct 结构体，该结构体定义在 /include/linux/fdtable.h 头文件中：struct files_struct { // 打开的文件数 atomic_t count; ... // fdtable 维护着所有的文件描述符 struct fdtable *fdt; struct fdtable fdtab; ... // 下一个可用文件描述符 unsigned int next_fd; ...};相信你也一眼就看见了 fdtable 这个结构体，见名知意，这不就是文件描述符表吗？ 那么它是不是维护了所有的文件描述符呢？struct fdtable { // 最大文件描述符 unsigned int max_fds; // 所有打开的文件 struct file **fd; /* current fd array */ ...};在 fdtable 里面有一个 file 结构体的数组，这个数组代表着该进程打开的所有文件。这个源码结构展示了每个进程单独维护了一个文件描述符的集合的信息。但是文件描述符是什么，以及它的生成规则还是没有找到。那只能换个方向，从函数调用去寻找了。open 和 close 都涉及到对文件描述符的操作，由于 close 函数更加简单，就从 close 为入口进行分析。下面是 close 函数的内部系统调用：SYSCALL_DEFINE1(close, unsigned int, fd){ int retval = __close_fd(current-&amp;gt;files, fd); ... return retval;}close 调用了 __close_fd 函数, 该函数定义在/fs/file.c文件中，下面是简化后的代码：int __close_fd(struct files_struct *files, unsigned fd){ struct file *file; struct fdtable *fdt; // 获取fdtable fdt = files_fdtable(files); // *** 通过文件描述符获取file结构体指针 file = fdt-&amp;gt;fd[fd]; rcu_assign_pointer(fdt-&amp;gt;fd[fd], NULL);// 回收文件描述符__put_unused_fd(files, fd); return filp_close(file, files);}这里面又出现了我们熟悉的结构体 files_struct，注意 file = fdt-&amp;gt;fd[fd] 这一段代码。fdt 就是 fdtable结构体，它的 fd 属性就是打开的所有文件数组，这样一看也就恍然大悟了。用户传进来的 fd 参数实际就是 fdtable 内的文件数组的索引。所以， 文件描述符其实就是file结构体数组的索引。相信关于后面被回收后的文件描述符如何再次分配文件描述符为什么从0开始文件描述符为什么不能为负数这些问题你都能迎刃而解了。参考 https://juejin.im/post/6844903962043236365" }, { "title": "fctnl 笔记", "url": "/posts/fcntl-note/", "categories": "cs", "tags": "file, fcntl", "date": "2021-09-25 00:00:00 +0800", "snippet": "file controlfcntl函数有五种功能： 复制一个现存的描述符（cmd＝F_DUPFD）。 获得/设置文件描述符标记（cmd = F_GETFD或F_SETFD）。 获得/设置文件状态标志（cmd = F_GETFL或F_SETFL）。 获得/设置异步I/O有权（cmd = F_GETOW N或F_SETOWN）。 获得/设置记录锁（cmd = F_GETLK , F_SETL K或F_ SETLKW） F_DUPFD 复制文件描述符f i l e d e s，新文件描述符作为函数值返回。它是尚未打开的各描述符中大于或等于第三个参数值（取为整型值）中各值的最小值。新描述符与 filedes共享同一文件表项（见图3-3）。但是，新描述符有它自己的一套文件描述符标志，其 FD_CLOEXEC文件描述符标志则被清除（这表示该描述符在 exec 时仍保持开放，我们将在第 8章对此进行讨论）。 F_GETFD 对应于filedes 的文件描述符标志作为函数值返回。当前只定义了一个文件描述符标志FD_CLOEXEC。 F_SETFD 对于filedes 设置文件描述符标志。新标志值按第三个参数(取为整型值)设置。应当了解很多现存的涉及文件描述符标志的程序并不使用常数FD_CLOEXEC，而是将此标志设置为0 (系统默认，在exec时不关闭)或1 (在exec时关闭)。 F_GETFL 对应于filedes 的文件状态标志作为函数值返回。在说明open函数时，已说明了文件状态标志。它们列于表3 - 2中。FD_CLOEXEC我们经常会碰到需要fork子进程的情况，而且子进程很可能会继续exec新的程序。这就不得不提到子进程中无用文件描述符的问题！fork函数的使用本不是这里讨论的话题，但必须提一下的是：子进程以写时复制（COW，Copy-On-Write）方式获得父进程的数据空间、堆和栈副本，这其中也包括文件描述符。刚刚fork成功时，父子进程中相同的文件描述符指向系统文件表中的同一项（这也意味着他们共享同一文件偏移量）。接着，一般我们会调用exec执行另一个程序，此时会用全新的程序替换子进程的正文，数据，堆和栈等。此时保存文件描述符的变量当然也不存在了，我们就无法关闭无用的文件描述符了。所以通常我们会fork子进程后在子进程中直接执行close关掉无用的文件描述符，然后再执行exec。但是在复杂系统中，有时我们fork子进程时已经不知道打开了多少个文件描述符（包括socket句柄等），这此时进行逐一清理确实有很大难度。我们期望的是能在fork子进程前打开某个文件句柄时就指定好：“这个句柄我在fork子进程后执行exec时就关闭”。其实时有这样的方法的：即所谓的 close-on-exec。最近好好看了一下open函数，其中flags参数可以传入O_CLOEXEC标记 [注意：linux 2.6.23才开始支持此标记]这样就可以一步实现上面的提到的close-on-exec的效果。 https://blog.csdn.net/chrisniu1984/article/details/7050663" }, { "title": "epoll 笔记", "url": "/posts/epoll-note/", "categories": "cs, os", "tags": "epoll", "date": "2021-09-25 00:00:00 +0800", "snippet": "文件描述符 linux下的文件描述符是一个用于表述指向文件的引用的抽象化概念(在windows下是HANDLE句柄). 文件描述符在形式上是一个非负整数值.但实际上,他是一个索引值,指向系统内核为每个进程维护的一张记录表. 在这张记录表上记录每个进程打开的文件对应的文件结构体信息.　 那么也就是说,文件描述符不存在事件这一说法,文件描述符本身不会产生事件,但文件描述符对应的文件可能会因为modify而产生事件. 这些事件是怎么产生的,由谁产生的,怎样让epoll捕捉到此事件.都是系统在对事件进行维护和通知 这是理解epoll的一个重要因素.工作模式 epoll不产生事件,但它监听并报告事件. 报告的事件类型有: EPOLLIN EPOLLOUT EPOLLPRI EPOLLERR EPOLLHUP EPOLLET EPOLLONESHOT 如果监听的文件描述符对应的文件出现了以上七种事件,就可以被epoll正确的捕捉到. epoll可以在两种模式下来捕捉监听的文件描述符产生的事件. 第一种是:ET模式,也就是Edge Triggered模式,只有文件发生变化的时候才会报告事件,意思是在一段时间内,连续的同样事件只报告一次,之后即便有相同的事件,也不再向上提交. 第二种是:LT模式,也就是Level Triggered模式,在这种模式下,epoll如实将文件上的事件向上一一传达,文件上什么时候有事件,有什么事件,epoll就向上传达什么,直到该事件被操作系统消除.应用方式 目前接触过的应用中如果使用ET模式,文件描述符必须设为非阻塞模式以避免由于一个文件的阻塞读/阻塞写操作把处理多个文件描述符的任务饿死,比如在对socket链接进行事件监听时,如果收到epoll的通知,那么就代表网络上有数据到来,然后循环recv,直到返回值小于指定的读取数据长度,错误码为EAGAIN时,表示链接上的数据已经读取完毕,应该去等待下一次的通知;又比如,在socket的accpet上,一旦收到通知,就不断地进行accept直到返回值为-1,把accept队列的所有已经完成建立连接的socket fd拿出来向上递交. 总结:事件由系统产生,epoll提供了事件通知的两种方式,文件描述符作为事件附着的抽象标识. 参考 https://www.cnblogs.com/burningTheStar/p/7459466.html" }, { "title": "色彩三要素 笔记", "url": "/posts/elements-of-color-note/", "categories": "photography", "tags": "elements-of-color", "date": "2021-09-25 00:00:00 +0800", "snippet": "色彩三要素色彩三要素（Elements of color）色彩可用的色相、饱和度（纯度）和明度来描述。色相色相，即各类色彩的相貌称谓，如大红、普蓝、柠檬黄等。色相是色彩的首要特征，是区别各种不同色彩的最准确的标准。事实上任何黑白灰以外的颜色都有色相的属性。明度明度是眼睛对光源和物体表面的明暗程度的感觉，主要是由光线强弱决定的一种视觉经验。明度可以简单理解为颜色的亮度。饱和度 (也叫纯度）饱和度是指色彩的鲜艳程度，也称色彩的纯度。饱和度取决于该色中含色成分和消色成分(灰色)的比例。含色成分越大，饱和度越大；消色成分越大，饱和度越小。参考 https://zhuanlan.zhihu.com/p/24633302色调色调是指物体反射的光线中以哪种频率占优势来决定的，不同频率产生不同颜色的感觉，色调是颜色的重要特征，它决定了颜色本质的根本特征。色调不是指颜色的性质，而是对一幅绘画作品的整体颜色的概括评价。色调是指一幅作品色彩外观的基本倾向。在明度、纯度（饱和度）、色相这三个要素中，某种因素起主导作用，我们就称之为某种色调。一幅绘画作品虽然用了多种颜色，但总体有一种倾向，是偏蓝或偏红，是偏暖或偏冷等等。这种颜色上的倾向就是一副绘画的色调。通常可以从色相、明度、冷暖、纯度四个方面来定义一幅作品的色调。色调在冷暖方面分为暖色调与冷色调：红色、橙色、黄色、棕色–为暖色，象征着：太阳、火焰、大地。绿色、蓝色、紫色–为冷色，象征着：森林、天空、大海。灰色、黑色、白色–为中间色。暖色调的亮度越高，其整体感觉越偏暖，冷色调的亮度越高，其整体感觉越偏冷。冷暖色调也只是相对而言，譬如说，红色系当中，大红与玫红在一起的时候，大红就是暖色，而玫红就被看作是冷色，又如，玫红与紫罗蓝同时出现时，玫红就是暖色。色调要区分摄影还是美术在美术、设计（常见于服装流行色）领域，色调专指整体的颜色倾向（全画面某种偏色的效果），如暖调（偏红黄）、冷调（偏蓝）等等。参考 https://baike.baidu.com/item/%E8%89%B2%E8%B0%83" }, { "title": "设计数据密集型应用 摘录", "url": "/posts/ddia-note/", "categories": "cs", "tags": "ddia", "date": "2021-09-25 00:00:00 +0800", "snippet": "背景 https://vonng.gitbooks.io/ddia-cn/content/本文为《Designing Data-Intensive Application》的摘录笔记，长期更新可靠性可扩展性可维护性水平扩展和横向扩展的定义​人们经常讨论纵向扩展（scaling up）（垂直扩展（vertical scaling），转向更强大的机器）和横向扩展（scaling out） （水平扩展（horizontal scaling），将负载分布到多台小机器上）之间的对立。跨多台机器分配负载也称为“无共享（shared-nothing）”架构。可以在单台机器上运行的系统通常更简单，但高端机器可能非常贵，所以非常密集的负载通常无法避免地需要横向扩展。现实世界中的优秀架构需要将这两种方法务实地结合，因为使用几台足够强大的机器可能比使用大量的小型虚拟机更简单也更便宜。数据模型与查询语言外键传统SQL模型（SQL：1999之前）中，最常见的规范化表示形式是将职位，教育和联系信息放在单独的表中，对User表提供外键引用，如图2-1所示。后续的SQL标准增加了对结构化数据类型和XML数据的支持;这允许将多值数据存储在单行内，并支持在这些文档内查询和索引。这些功能在Oracle，IBM DB2，MS SQL Server和PostgreSQL中都有不同程度的支持【6,7】。JSON数据类型也得到多个数据库的支持，包括IBM DB2，MySQL和PostgreSQL 【8】。第三种选择是将职业，教育和联系信息编码为JSON或XML文档，将其存储在数据库的文本列中，并让应用程序解析其结构和内容。这种配置下，通常不能使用数据库来查询该编码列中的值。NoSQL&amp;amp;文档数据库处理多对多关系NoSQL&amp;amp;文档数据库能很好处理一对一一对多的关系，但是在多对多的关系和连接已常规用在关系数据库时，文档数据库和NoSQL重启了辩论：如何最好地在数据库中表示多对多关系。那场辩论可比NoSQL古老得多，事实上，最早可以追溯到计算机化数据库系统。那时人们提出了各种不同的解决方案来解决层次模型的局限性。其中最突出的两个是关系模型（relational model）（它变成了SQL，统治了世界）和网络模型（network model）（最初很受关注，但最终变得冷门）。这两个阵营之间的“大辩论”在70年代持续了很久时间。那两个模式解决的问题与当前的问题相关，因此值得简要回顾一下那场辩论。网络模型网络模型由一个称为数据系统语言会议（CODASYL）的委员会进行了标准化，并被数个不同的数据库商实现;它也被称为CODASYL模型【16】。CODASYL模型是层次模型的推广。在层次模型的树结构中，每条记录只有一个父节点；在网络模式中，每条记录可能有多个父节点。例如，“Greater Seattle Area”地区可能是一条记录，每个居住在该地区的用户都可以与之相关联。这允许对多对一和多对多的关系进行建模。网络模型中记录之间的链接不是外键，而更像编程语言中的指针（同时仍然存储在磁盘上）。访问记录的唯一方法是跟随从根记录起沿这些链路所形成的路径。这被称为访问路径（access path）。最简单的情况下，访问路径类似遍历链表：从列表头开始，每次查看一条记录，直到找到所需的记录。但在多对多关系的情况中，数条不同的路径可以到达相同的记录，网络模型的程序员必须跟踪这些不同的访问路径。CODASYL中的查询是通过利用遍历记录列和跟随访问路径表在数据库中移动游标来执行的。如果记录有多个父结点（即多个来自其他记录的传入指针），则应用程序代码必须跟踪所有的各种关系。甚至CODASYL委员会成员也承认，这就像在n维数据空间中进行导航【17】。尽管手动选择访问路径够能最有效地利用20世纪70年代非常有限的硬件功能（如磁带驱动器，其搜索速度非常慢），但这使得查询和更新数据库的代码变得复杂不灵活。无论是分层还是网络模型，如果你没有所需数据的路径，就会陷入困境。你可以改变访问路径，但是必须浏览大量手写数据库查询代码，并重写来处理新的访问路径。更改应用程序的数据模型是很难的。关系模型相比之下，关系模型做的就是将所有的数据放在光天化日之下：一个 关系（表） 只是一个 元组（行） 的集合，仅此而已。如果你想读取数据，它没有迷宫似的嵌套结构，也没有复杂的访问路径。你可以选中符合任意条件的行，读取表中的任何或所有行。你可以通过指定某些列作为匹配关键字来读取特定行。你可以在任何表中插入一个新的行，而不必担心与其他表的外键关系iv。iv. 外键约束允许对修改约束，但对于关系模型这并不是必选项。即使有约束，外键连接在查询时执行，而在CODASYL中，连接在插入时高效完成。在关系数据库中，查询优化器自动决定查询的哪些部分以哪个顺序执行，以及使用哪些索引。这些选择实际上是“访问路径”，但最大的区别在于它们是由查询优化器自动生成的，而不是由程序员生成，所以我们很少需要考虑它们。如果想按新的方式查询数据，你可以声明一个新的索引，查询会自动使用最合适的那些索引。无需更改查询来利用新的索引。（请参阅“用于数据的查询语言”。）关系模型因此使添加应用程序新功能变得更加容易。关系数据库的查询优化器是复杂的，已耗费了多年的研究和开发精力【18】。关系模型的一个关键洞察是：只需构建一次查询优化器，随后使用该数据库的所有应用程序都可以从中受益。如果你没有查询优化器的话，那么为特定查询手动编写访问路径比编写通用优化器更容易——不过从长期看通用解决方案更好。与文档数据库相比在一个方面，文档数据库还原为层次模型：在其父记录中存储嵌套记录（图2-1中的一对多关系，如positions，education和contact_info），而不是在单独的表中。但是，在表示多对一和多对多的关系时，关系数据库和文档数据库并没有根本的不同：在这两种情况下，相关项目都被一个唯一的标识符引用，这个标识符在关系模型中被称为外键，在文档模型中称为文档引用【9】。该标识符在读取时通过连接或后续查询来解析。迄今为止，文档数据库没有走CODASYL的老路。关系型数据库与文档数据库在今日的对比将关系数据库与文档数据库进行比较时，可以考虑许多方面的差异，包括它们的容错属性（参阅第5章）和处理并发性（参阅第7章）。本章将只关注数据模型中的差异。支持文档数据模型的主要论据是架构灵活性，因局部性而拥有更好的性能，以及对于某些应用程序而言更接近于应用程序使用的数据结构。关系模型通过为连接提供更好的支持以及支持多对一和多对多的关系来反击。建议：如果应用程序中的数据具有类似文档的结构（即，一对多关系树，通常一次性加载整个树），那么使用文档模型可能是一个好主意。将类似文档的结构分解成多个表（如图2-1中的positions，education和contact_info）的关系技术可能导致繁琐的模式和不必要的复杂的应用程序代码。但是，如果你的应用程序确实使用多对多关系，那么文档模型就没有那么吸引人了。通过反规范化可以减少对连接的需求，但是应用程序代码需要做额外的工作来保持数据的一致性。通过向数据库发出多个请求，可以在应用程序代码中模拟连接，但是这也将复杂性转移到应用程序中，并且通常比由数据库内的专用代码执行的连接慢。在这种情况下，使用文档模型会导致更复杂的应用程序代码和更差的性能【15】。很难说在一般情况下哪个数据模型让应用程序代码更简单；它取决于数据项之间存在的关系种类。对于高度相联的数据，选用文档模型是糟糕的，选用关系模型是可接受的，而选用图形模型（参见“图数据模型”）是最自然的。文档和关系数据库的融合自2000年代中期以来，大多数关系数据库系统（MySQL除外）都已支持XML。这包括对XML文档进行本地修改的功能，以及在XML文档中进行索引和查询的功能。这允许应用程序使用那种与文档数据库应当使用的非常类似的数据模型。从9.3版本开始的PostgreSQL 【8】，从5.7版本开始的MySQL以及从版本10.5开始的IBM DB2 [30]也对JSON文档提供了类似的支持级别。鉴于用在Web APIs的JSON流行趋势，其他关系数据库很可能会跟随他们的脚步并添加JSON支持。在文档数据库中，RethinkDB在其查询语言中支持类似关系的连接，一些MongoDB驱动程序可以自动解析数据库引用（有效地执行客户端连接，尽管这可能比在数据库中执行的连接慢，需要额外的网络往返，并且优化更少）。随着时间的推移，关系数据库和文档数据库似乎变得越来越相似，这是一件好事：数据模型相互补充v，如果一个数据库能够处理类似文档的数据，并能够对其执行关系查询，那么应用程序就可以使用最符合其需求的功能组合。关系模型和文档模型的混合是未来数据库一条很好的路线。v. Codd对关系模型【1】的原始描述实际上允许在关系模式中与JSON文档非常相似。他称之为非简单域（nonsimple domains）。这个想法是，一行中的值不一定是一个像数字或字符串一样的原始数据类型，也可以是一个嵌套的关系（表），因此可以把一个任意嵌套的树结构作为一个值，这很像30年后添加到SQL中的JSON或XML支持。数据查询语言当引入关系模型时，关系模型包含了一种查询数据的新方法：SQL是一种 声明式 查询语言，而IMS和CODASYL使用 命令式 代码来查询数据库。那是什么意思？命令式语言告诉计算机以特定顺序执行某些操作。可以想象一下，逐行地遍历代码，评估条件，更新变量，并决定是否再循环一遍。在声明式查询语言（如SQL或关系代数）中，你只需指定所需数据的模式 - 结果必须符合哪些条件，以及如何将数据转换（例如，排序，分组和集合） - 但不是如何实现这一目标。数据库系统的查询优化器决定使用哪些索引和哪些连接方法，以及以何种顺序执行查询的各个部分。声明式查询语言是迷人的，因为它通常比命令式API更加简洁和容易。但更重要的是，它还隐藏了数据库引擎的实现细节，这使得数据库系统可以在无需对查询做任何更改的情况下进行性能提升。图数据模型如我们之前所见，多对多关系是不同数据模型之间具有区别性的重要特征。如果你的应用程序大多数的关系是一对多关系（树状结构化数据），或者大多数记录之间不存在关系，那么使用文档模型是合适的。但是，要是多对多关系在你的数据中很常见呢？关系模型可以处理多对多关系的简单情况，但是随着数据之间的连接变得更加复杂，将数据建模为图形显得更加自然。对比图形数据库与网络模型相比较在CODASYL中，数据库有一个模式，用于指定哪种记录类型可以嵌套在其他记录类型中。在图形数据库中，不存在这样的限制：任何顶点都可以具有到其他任何顶点的边。这为应用程序适应不断变化的需求提供了更大的灵活性。在CODASYL中，达到特定记录的唯一方法是遍历其中的一个访问路径。在图形数据库中，可以通过其唯一ID直接引用任何顶点，也可以使用索引来查找具有特定值的顶点。在CODASYL，记录的后续是一个有序集合，所以数据库的人不得不维持排序（这会影响存储布局），并且插入新记录到数据库的应用程序不得不担心的新记录在这些集合中的位置。在图形数据库中，顶点和边不是有序的（只能在查询时对结果进行排序。在CODASYL中，所有查询都是命令式的，难以编写，并且很容易因架构中的变化而受到破坏。在图形数据库中，如果需要，可以在命令式代码中编写遍历，但大多数图形数据库也支持高级声明式查询语言，如Cypher或SPARQL。存储与检索哈希表索引局限性：散列表必须能放进内存如果你有非常多的键，那真是倒霉。原则上可以在磁盘上保留一个哈希映射，不幸的是磁盘哈希映射很难表现优秀。它需要大量的随机访问I/O，当它变满时增长是很昂贵的，并且散列冲突需要很多的逻辑。范围查询效率不高。例如，您无法轻松扫描kitty00000和kitty99999之间的所有键——您必须在散列映射中单独查找每个键。SSTables到目前为止，但是如何让你的数据首先被按键排序呢？我们的传入写入可以以任何顺序发生。在磁盘上维护有序结构是可能的（参阅“B树”），但在内存保存则要容易得多。有许多可以使用的众所周知的树形数据结构，例如红黑树或AVL树【2】。使用这些数据结构，您可以按任何顺序插入键，并按排序顺序读取它们。现在我们可以使我们的存储引擎工作如下：写入时，将其添加到内存中的平衡树数据结构（例如，红黑树）。这个内存树有时被称为内存表（memtable）。当内存表大于某个阈值（通常为几兆字节）时，将其作为SSTable文件写入磁盘。这可以高效地完成，因为树已经维护了按键排序的键值对。新的SSTable文件成为数据库的最新部分。当SSTable被写入磁盘时，写入可以继续到一个新的内存表实例。为了提供读取请求，首先尝试在内存表中找到关键字，然后在最近的磁盘段中，然后在下一个较旧的段中找到该关键字。有时会在后台运行合并和压缩过程以组合段文件并丢弃覆盖或删除的值。这个方案效果很好。它只会遇到一个问题：如果数据库崩溃，则最近的写入（在内存表中，但尚未写入磁盘）将丢失。为了避免这个问题，我们可以在磁盘上保存一个单独的日志，每个写入都会立即被附加到磁盘上，就像在前一节中一样。该日志不是按排序顺序，但这并不重要，因为它的唯一目的是在崩溃后恢复内存表。每当内存表写出到SSTable时，相应的日志都可以被丢弃。思考如果每次写入都要同步写入磁盘日志，如何保证性能？日志结构合并树（Log-structured Merge Tree） https://www.jianshu.com/p/f911cb9e42de与往常一样，大量的细节使得存储引擎在实践中表现良好。例如，当查找数据库中不存在的键时，LSM树算法可能会很慢：您必须检查内存表，然后将这些段一直回到最老的（可能必须从磁盘读取每一个），然后才能确定键不存在。为了优化这种访问，存储引擎通常使用额外的Bloom过滤器【15】。 （布隆过滤器是用于近似集合内容的内存高效数据结构，它可以告诉您数据库中是否出现键，从而为不存在的键节省许多不必要的磁盘读取操作。注：布隆过滤器可以确定一个不存在的key或者可能存在的key" }, { "title": "一致哈希 笔记", "url": "/posts/consistent-hashing-note/", "categories": "cs", "tags": "consistent-hashing", "date": "2021-09-25 00:00:00 +0800", "snippet": "原理传统的哈希函数：what is a hash functionit maps data of arbitrary size to fixed-size valuekeys -&amp;gt; hashesproperties of a good hash function uniformity（均匀） deterministic(确定) efficiency（高效）当我们在服务器上使用hash取余，当服务器水平扩展的时候大部分的key都remapped一致哈希：a special kind of hashingonly k/n keys are remapped when resizing (k - keys count, n - bucket size)basic technique: ring-based evenly place servers onto a ring…当新增一个节点时，只有部分相邻区域的key被remapped删除一个节点时，下一个节点的负载可能存在翻倍对比operation classic hash table consistent hash tableadd a node O(k) O(K/N+logN)remove a node O(k) O(K/N+logN)add a key O(1) O(logN)remove a key O(1) O(logN)通常N较小参考 https://www.youtube.com/watch?v=lm6Zeo3tqK4" }, { "title": "计算机网络协议 笔记", "url": "/posts/computer-network-note/", "categories": "cs, network", "tags": "network", "date": "2021-09-25 00:00:00 +0800", "snippet": "协议分层报文、报文段、分组、包、数据报、帧、数据流的概念区别1.报文(message)我们将位于应用层的信息分组称为报文。报文是网络中交换与传输的数据单元，也是网络传输的单元。报文包含了将要发送的完整的数据信息，其长短不需一致。报文在传输过程中会不断地封装成分组、包、帧来传输，封装的方式就是添加一些控制信息组成的首部，那些就是报文头。2.报文段（segment）通常是指起始点和目的地都是传输层的信息单元。3.分组/包(packet)分组是在网络中传输的二进制格式的单元，为了提供通信性能和可靠性，每个用户发送的数据会被分成多个更小的部分。在每个部分的前面加上一些必要的控制信息组成的首部，有时也会加上尾部，就构成了一个分组。它的起始和目的地是网络层。4.数据报(datagram)面向无连接的数据传输，其工作过程类似于报文交换。采用数据报方式传输时，被传输的分组称为数据报。通常是指起始点和目的地都使用无连接网络服务的的网络层的信息单元。5.帧(frame)帧是数据链路层的传输单元。它将上层传入的数据添加一个头部和尾部，组成了帧。它的起始点和目的点都是数据链路层。6.数据单元（data unit）指许多信息单元。常用的数据单元有服务数据单元（SDU）、协议数据单元（PDU）。SDU是在同一机器上的两层之间传送信息。PDU是发送机器上每层的信息发送到接收机器上的相应层（同等层间交流用的）。应用层——消息传输层——数据段/报文段(segment) (注：TCP叫TCP报文段，UDP叫UDP数据报,也有人叫UDP段)网络层——分组、数据包（packet）链路层——帧（frame）物理层——P-PDU（bit）其实，segment，datagram，packet，frame是存在于同条记录中的，是基于所在协议层不同而取了不同的名字。我们可以用一个形象的例子对数据包的概念加以说明：我们在邮局邮寄产品时，虽然产品本身带有自己的包装盒，但是在邮寄的时候只用产品原包装盒来包装显然是不行的。必须把内装产品的包装盒放到一个邮局指定的专用纸箱里，这样才能够邮寄。这里，产品包装盒相当于数据包，里面放着的产品相当于可用的数据，而专用纸箱就相当于帧，且一个帧中通常只有一个数据包。Wireshark中是这么定义的：相同四元组(源地址，源端口，目的地址，目的端口)的包就为一条TCP流，即一条流有很多个包。可看到：TCP流有24条，UDP流有26条。IPv4流有19条，IPv6流有11条，为什么IPv4+IPv6流总数小于TCP+UDP流呢？因为IP流没有端口，只考虑相同二元组(源ip，目的ip)：参考 https://blog.csdn.net/a3192048/article/details/84671340" }, { "title": "计算机网络发展", "url": "/posts/computer-network-development/", "categories": "cs, network", "tags": "network", "date": "2021-09-25 00:00:00 +0800", "snippet": "在从事网络应用的开发过程中会经常遇到一些问题，有些问题是涉及到所依赖的软件或框架的实现，这类问题我们可以通过阅读该软件或框架的说明文档来解决问题；有些则是涉及到网络通信中使用的协议，那我们应该找到该协议的说明文档，即该协议定制的原始出处，这篇文章记录了我探寻这一过程的历程。这个过程起源于我在wiki上查询http-header字段时发现的一个彩蛋：“引导者”（“referrer”）这个单词，在RFC中被拼错了，因此在大部分的软件实现中也拼错了，以至于，错误的拼法成为了标准的用法，还被当成了正确的术语。检验referrer头部可以用于防止CSRF(跨站请求伪造)，我曾经困惑于它的拼写但未深究，原来它是RFC文档中的一个错误拼写，那么这里的RFC是否就是HTTP协议的标准文档？带着这个问题我查询了RFC的相关信息。征求意见稿（英语：Request For Comments，缩写为RFC），是由互联网工程任务组（IETF）发布的一系列备忘录。文件收集了有关互联网相关信息，以及UNIX和互联网社区的软件文件，以编号排定。目前RFC文件是由互联网协会（ISOC）赞助发行。RFC始于1969年，由斯蒂芬·克罗克用来记录有关ARPANET开发的非正式文档，最终演变为用来记录互联网规范、协议、过程等的标准文件。基本的互联网通信协议都有在RFC文件内详细说明。RFC文件还额外加入许多的论题在标准内，例如对于互联网新开发的协议及发展中所有的记录。例如常见互联网协议的RFC编号：IP：791, TCP：793, HTTP1.1：2616可以看到RFC(征求意见稿，Request for Comments)制定了我们常用的一些互联网协议，RFC是由IETF（互联网工程任务组，Internet Engineering Task Force）发布的，那么这个IETF又是谁？进一步查询可知它是目前TCP/IP协议族的制定者。The Internet Engineering Task Force (IETF) develops and promotes voluntary Internet standards, in particular the standards that comprise the Internet protocol suite (TCP/IP). ...The IETF started out as an activity supported by the U.S. federal government, but since 1993 it has operated as a standards development function under the auspices of the Internet Society, an international membership-based non-profit organization.与它在同一个层面的组织有ISO（国际标准化组织，International Organization for Standardization）和ITU（国际电信联盟，International Telecommunication Union）等，回忆起学生时代计算机网络中提及的OSI模型，它被制定在ISO的标准文件中，定义于ISO/IEC 7498-1，这是一个理论上的模型，也是RFC的所参考的对象。为什么我们现在使用的是RFC制定的TCP/IP协议族而不是OSI模型，这一点要结合当时的时代背景来看，在1980年代早期，个人计算机产业快速发展，客户和服务商之间对通讯技术的要求也急剧增加，这使得他们急于开发并使用一些新的通信技术规范，即使这种规范是并未被标准化的。因此，标准制定机构必须加快标准制定的进程来适应这一需求，否则他们将不得不面对并承认一些事实标准。不幸的是，这时候像ISO和CCITT（ITU前身）这样的机构并不足以应付这样的改变。所以公众更倾向于从另外一类能更快的对公众的要求作出反馈的标准制定组织来获取标准，这样的组织包括非正式的、非政府的组织，此时像IETF和W3C（万维网联盟，World Wide Web Consortium）这样的组织便诞生了。其中W3C也定制了大量影响深远的协议，例如CSS，XML，HTML等。那么IETF定制的RFC标准除了比别的标准化组织快，还有什么其他的原因使得它流行起来？主要还是得益于当时的历史进程，例如美国国防部的支持；IBM，AT&amp;amp;T等大厂的支持；AT&amp;amp;T所研发的UNIX中的使用了符合TCP/IP标准的代码。需要注意的一点是，我们普遍提及的概念TCP/IP全称是Internet protocol suite，它是一个与硬件无关的协议，所以它的最底层组件是链接（Link）而不是OSI模型中的数据链路层（Data link）或物理层（Physical），也就是说TCP/IP可以在几乎任何硬件网络技术的基础上实现。我们经常混淆TCP/IP4层模型中的链接为OSI模型中的数据链路层，这曾经也很让我困惑，既然TCP/IP与硬件无关，为何要参与设计数据链路层。我们做出了进一步的查询。The link is treated as a black box. The IETF explicitly does not intend to discuss transmission systems, but practical alternative to the OSI model.TCP/IP中的链接被视为黑盒，在现实中，它实际对应的是OSI模型中的物理层或数据链路层，或两者都包括。我们熟悉的数据链路层协议是由IEEE（电气电子工程师学会，Institute of Electrical and Electronics Engineers）制定的LLC（逻辑链路控制，Logical Link Control），它被制定在IEEE 802.2标准中。这个在电气和通信领域如雷贯耳的组织，它也参与定制了计算机网络协议中的一部分，主要是在数据链路层和物理层，例如我们使用最为广泛的，基于铜缆双绞线或光纤的以太网（IEEE 802.3），Wi-Fi（IEEE 802.11 ）等。同时IEEE在计算机硬件也制定了很多影响广泛的标准，例如POSIX（IEEE Std 1003）。另一个例子是我们生活中常用的网线即超5类双绞线（CAT-5e），它使用的是TIA/EIA-568-B标准，而它的组织也是赫赫有名的ANSI（美国国家标准学会，American National Standards Institute）。有趣的是在最新的CAT-7使用的是ISO的标准（ISO/IEC 11801 ）。到这里，我们没有必要去继续探究这些标准制定的组织和协会，只需要在使用协议的时候再去查询到它们，可以推测，他们之间并不是愉快的合作关系而是激烈的竞争关系，标准的制定权代表的是组织背后的行业（企业）的整体实力，能拿到这个话语权无疑是站在了食物链的顶端，一个新鲜的例子是华为在2017年拿下了5G（第五代移动通信系统）的标准制定权。回到我们的出发点，我们可以得出一个结论，即我们目前广泛使用的TCP/IP协议族是由IETF所制定的标准，作为网络程序的开发者，我们所集中关注的应用层和传输层的协议有很多都是他们所制定的，他们出版的RFC文档即现实意义上的标准文档。他们的标准文件可以从官方网站上免费获得https://tools.ietf.org/rfc/index。" }, { "title": "色温 笔记", "url": "/posts/color-temperature-note/", "categories": "photography", "tags": "color-temperature", "date": "2021-09-25 00:00:00 +0800", "snippet": "色温色温这个概念是从黑体辐射来的。根据日常的经验我们可以知道，给一个铁块加热，随着温度升高，铁块开始是暗红色，逐渐变成亮红色，如果能达到炼钢炉里那样的高温，铁融化成铁水，发出耀眼的金黄色光芒。而天文学家也告诉我们，宇宙中的恒星可以看做一个个黑体，随着恒星温度的升高，恒星的颜色也是从红色到黄色，非常高温度的恒星甚至发出偏蓝色的光。我们日常生活中，最重要的光源就是太阳，太阳发出的光很接近黑体光谱。我们的太阳表面温度约为 5500K，这个温度的黑体发出的光，基本就是白色。读过本专栏之前文章的读者朋友肯定能理解，这当然不是巧合。在色彩空间中我们需要定义一个白点，而常见的一些标准白点，就是以 5500K 左右的黑体辐射（也就是我们的太阳）为基础的。CIE 委员会规定的几种标准白点中，D50、D55 和 D65 是常用的白点，参照了 5000K、5500K 和 6500K 的黑体辐射来定义，模拟了不同条件下（比如地平线方向的光照，上下午的室外，正午室外）的光照。色彩空间 sRGB 所依赖的白点就是 D65。对于色温，通常有几个误解。第一个误解是混淆了色温和情感上的温度倾向。我们必须理解，色温是根据黑体辐射温度来定义的，是物理上的温度，而不是情感上的温度。从我们的情感上来说，橙色红色给人感觉温暖，蓝色给人感觉寒冷。但从色温的定义来看，红色橙色是温度较低的黑体发出的颜色，而温度高的黑体颜色反而是蓝色。第二个误解是混淆了色温针对的对象。狭义上来说，色温是针对光源的。比如晴朗正午的室外，光源是太阳（以及部分蓝天），这个时候的光源接近标准光源 D65，色温接近 6500K；又比如说傍晚日落，经过大气层的散射，阳光中的蓝光大大减少，光线颜色偏黄，光源颜色也许接近 3500K 的黑体。很多关于色温方面的争论都是从这两个误解而来，搞清楚讨论对象的定义，很多争论也就烟消云散了。参考 https://zhuanlan.zhihu.com/p/27165715白平衡在本专栏之前的文章 《色彩空间基础》 中提到，物理世界里是无所谓「颜色」这个概念的，一切都是 光谱功率分布，所谓的「颜色」，是人类视觉过程产生的一种感受。正是如此，我们才有了「三原色」理论，我们才可以用三种颜色的光来模拟世间万物的颜色。生活常识告诉我们，光源的颜色不同，照射到同一个物体上，会引起物体颜色的变化。从单纯的物理的角度来看，这个事情再正常不过了。光源有光谱功率分布，物体对不同波长的光线又有不同的反射率，那么光源照射到物体上，再反射到眼睛里的光，它的光谱功率分布就和光源密切相关了。不同的光源，照射到同样的物体上，物体反射光的光谱功率分布就会不一样。下图展示了一个例子，物体的反射率是对蓝色光更高，对红色光更低。第一行的光源是一个接近白色的光源，照射到物体上之后，反射的光线就是蓝色；而第二行的光源是一个黄色的光源，照射到物体上之后，反射的光线是偏棕红色的。这里需要再强调一下，物理上没有所谓的「颜色」的概念，只有「光谱功率分布」、「反射率」这些概念。一个物体的反射率是固定不变的，但是这个物体的「颜色」在不同条件下是不一样的。我们可以用一个标准白点（比如 D65）作为光源，照射到物体上。我们认为这个时候物体表现出来的「颜色」就是物体「本来」的颜色，可以定义为物体的「固有色」。很显然，固有色的光谱功率分布曲线与物体本身的反射率曲线两者是很接近的（参见上图中第一行的例子）。白平衡（White Balance）要做的，就是在不同的光线条件下，根据当时得到的物体颜色，尽量恢复物体的「固有色」，或者说，尽量减小光源颜色对物体颜色的影响。（在下一节会继续讨论这个定义的合理性，本小节中先按照这个定义进行讨论）。根据之前的专栏文章 《色彩空间表示与转换》 中讨论的结果，我们可以在色彩转换的过程中根据光源的情况指定不同的白点，从而得到相应正确的色彩转换结果。举例来说，在晴天正午室外拍摄的一张照片，光源近似于标准白点 D65，那么我们就用 D65 点作为白点进行色彩转换（实际上就是直接用 sRGB 空间进行展示）。又比如在黄昏的光线下拍摄了照片，此时光源接近 2500K 的黑体，那么我们从普朗克曲线上找到 2500K 的点，作为新的白点，进行色彩转换，就能得到正确的颜色。由于色彩空间的线性性质，这个转换过程就是一个矩阵乘法，非常方便。详细的计算过程可以参见之前的文章，这里不再赘述。这就是相机上色温调节的原理。也就是说，相机上的色温选项，调节的是色彩转换中的白点的位置，在普朗克曲线上选取与色温相对应的点作为白点进行色彩转换，从而得到正确的颜色。这个色温选项，是要与拍摄时的光源色温相匹配的，这也是为什么在晴朗的白天拍摄，色温控制在 5000K ~ 6000K 是比较合适的，而在黄昏时候拍摄，色温需要设置成 2500K ~ 3000K 才比较合适。我们可以看到，沿着普朗克线移动白点，总体上是调节了画面偏蓝或者偏黄的色彩倾向。当然，完整的白平衡要求白点可以任意设置，所以必须增加第二个移动方向。最合理的选择就是与普朗克线正交的另一个方向，从色品图上看大致是从左上角到右下侧直线边的方向，也就是绿色到品红的方向。正因为如此，我们可以看到，Photoshop、Lightroom 这样的照片处理软件，白平衡的面板上总是会有两个调节滑块，一个调节黄-蓝平衡，通常叫色温（Temperature），一个调节绿-品红平衡，通常叫色调（Tint）。这两个方向互相配合，才能使得白点位于色品图中的任意位置，才能对任意光线条件下拍摄的照片进行白平衡的调节。参考 https://zhuanlan.zhihu.com/p/27165715 https://www.zhihu.com/question/60266964色彩恒常性白平衡解决了「怎样才能正确得到白色」的问题，在上一小节中，我提出白平衡的目的是「在不同的光线条件下，根据当时得到的物体颜色，尽量恢复物体的固有色」，这句话看似简单，仔细想想，却又并不那么显然。一个很明显的疑问是：我们怎么知道什么是物体的固有色？白炽灯照明下的一张白纸，与阳光照明下的一张橙黄色的纸，两者从物理上而言可以是几乎一样的，或者说两者的光谱功率分布函数，可以是几乎一样的。那么我们凭什么说一张是白纸，一张是橙黄色的纸呢？答案是，不能区分。如果仅仅看这张纸的话，我们不能区分哪张是白纸，哪张是黄纸。但是，我们不仅仅只看了纸，我们还看到了周围的景物，也许是摆放纸张的书桌，也许是周围的植物盆景，也许是旁边的水果，甚至是我们自己的手。所有这一切，都帮助我们认识周围的环境，根据环境去推测这张纸本来的颜色。人类视觉过程不仅仅有视网膜的参与，更重要的还有大脑的参与。视觉是人类获取外界信息最重要的手段，在日复一日的使用中，人类大脑对外界环境建立起了复杂的感知和对应。我们的大脑早就知道，植物叶子是绿色的，樱桃是红色的，我们的皮肤是黄色的……通过对这些周围环境的感知，我们的大脑推断出了这个时候光源的情况——是白炽灯还是阳光，从而自动完成了「白平衡」的过程，使得我们迅速感知到一个物体本来的颜色是什么。这就是人类视觉的色彩恒常性。" }, { "title": "色彩空间 笔记", "url": "/posts/color-space-note/", "categories": "photography", "tags": "color-space", "date": "2021-09-25 00:00:00 +0800", "snippet": "实验基础相信大家都熟悉「三原色」理论，也知道现代显示器是按照 RGB 模式来显示色彩，不知道有没有想过，为什么是「三原色」？为什么是「三」这个数字？为什么一定要选择 RGB 红绿蓝三种颜色作为原色呢？选其他行不行？用四种颜色行不行？知乎上有个问题 红绿蓝三色是（唯一的）正交基吗？ 就提出了这个疑问，我也给出过 我的回答，可以参考。追根溯源的话，得从我们人类的视网膜说起。大部分人类的视网膜上有三种感知颜色的感光细胞，叫做视锥细胞，分别对不同波长的光线敏感，称为 L/M/S 型细胞。三种视锥细胞最敏感的波长分别是橙红色（长波，Long），绿色（中波，Medium），蓝色（短波，Short）。这三种视锥细胞的归一化感光曲线如下图所示（图片数据来自 CVLR，我重新绘制），可以看到 L 型视锥细胞与 M 型视锥细胞的感光曲线差别很小，实际上这两种视锥细胞起源于一次基因变异，在这之前人类可都是红绿色盲呢，多亏这个基因变异，让人类可以看到更加多彩的世界——这又是一个庞大的话题了，就此打住。总之，大自然的这千千万万种颜色，在人类的眼里看到，最后传送到大脑里的信号，就只有这三种视锥细胞的电信号而已。根据这三种电信号的强弱，大脑解读成了不同的颜色。这就是三原色理论的生物学依据。不仅如此，人类眼睛对不同颜色光线混合的反应还是 线性 的。根据 格拉斯曼定律（Grassmann’s Law），两束不同颜色的光 [公式] 和 [公式]，假设某个视锥细胞对他们的反应分别是 [公式] 和 [公式]，现在将他们按照一个比例混合，得到第三种颜色 [公式]，那么视锥细胞对这个混合颜色的反应也将是前两个反应的线性叠加 [公式]。格拉斯曼定律是一个实验规律，并没有物理或者生物学上的依据。然而这个规律大大简化了我们对人类彩色视觉系统的建模，并且给我们使用线性代数理论分析人类彩色视觉系统提供了一个前提和基础。色匹配函数前面已经提到，人类视网膜上有三种感知色彩的视锥细胞，所以理论上我们用三种颜色的光就可以混合出自然界中任何一种颜色来。在 20 世纪 20 年代，David Wright 和 John Guild 各自独立地领导了一些实验，通过三种颜色的光源进行匹配，得到了人眼对于不同颜色光的匹配函数。此后，多名科学家多次进行了类似的实验，加深了我们对人类彩色视觉的认识。实验过程大致是这样的，把一个屏幕用不透光的挡板分割成两个区域，左边照射某个被测试的颜色的光线，这里记为[公式] （以下用大写字母表明颜色，用小写字母表明分量大小），右边同时用三种颜色的光同时照射，这里记为[公式]，[公式]，[公式]。然后，调节右边三种颜色光源的强度，直到左右两边的颜色看上去一样为止。假设这个时候三种颜色的光源强度分别为[公式]，[公式]，[公式]，那么根据光色叠加的线性性质，我们可以写出[公式]也就是说，只要按照 (r,g,b) 的分量来混合 (R,G,B) 三种颜色的光，就可以得到 C 这个颜色的光。于是在这一系列实验里，科学家们把左边的颜色按着光谱顺序，挨个测试了一遍，得到了纯光谱色的混合叠加的数据，这就是 色匹配函数（Color Matching Function） ，并且在这个基准下定义的色彩空间，就是 CIE RGB 色彩空间。下图是 CIE RGB 的色匹配函数曲线，数据来自 CVLR，我重新绘制。浅色的细线代表实验中不同参与者个人的色匹配函数曲线，中间深色的粗线代表数据的平均值。从线性空间的角度理解色彩空间以上的实验基础提示我们，色彩空间和线性代数中的线性空间之间具有某种相似性。我们可以看到，由于人类有三种感知色彩的视锥细胞，自然界千千万万的色彩被眼睛接收后，可以用三个数值来表征。而格拉斯曼定律也揭示了色彩叠加的线性性质。这似乎意味着，色彩空间就是一个 3 维的线性空间。事实上也的确如此（详细的论证参见末尾小节）。自然界本身是没有「颜色」这个属性的，只有对不同波长光线的反射率/透过率，到达人眼中的，显然是一个连续的光谱分布函数。数学上，这是一个无穷维的函数空间（巴拿赫空间）。而人眼内的三种视锥细胞，它们的感光特性曲线相当于是在这个无穷维的函数空间中建立了三个基底。任何一个光谱分布进来，三种视锥细胞被激发。由于色视觉响应的线性性，这一过程相当于光谱分布函数与三个基底做内积，或者说，「投影」到这三个基底上。从这个观点看，人类的色视觉，是相当于在自然界所有颜色的无穷维函数空间中取了一个三维的投影。这个三维空间的基底，既可以是视锥细胞的感光特性曲线（我们的大脑就用的是这套），当然也可以是选取三种颜色的光进行组合（CIE RGB 空间），甚至还可以是用实际中不存在的「光线」进行组合 （CIE XYZ 空间）。既然这几个空间实际上是同一个线性空间，只不过由于选择了不同的基底而有不同的表达形式，那么根据线性代数的结论，这几个空间的表述形式之间，只需要通过矩阵乘法就可以完成转换，这是完全的线性变换。当然，色彩空间并不是真正数学意义上的三维线性空间。由于不存在真正数学意义的「减法」，在实际应用中是有所限制的。数学中的「线性组合」在这里就要被替换为「锥组合」，也就是每个分量都必须是大于等于 0 的。至此我们终于可以回答开头的部分问题了，为什么是三原色？因为人类对色彩的感知结果位于一个三维的线性空间中。最少需要三种颜色的光才能有足够的表达能力来表现各种颜色。为什么选 RGB 作为三原色？因为色彩空间不是真正数学意义上的线性空间，从工程角度考虑，以 RGB 作为三原色，能让显示器能够显示更多的颜色（此外，最初测试人眼对 RGB 三色光的色匹配曲线，也是希望能尽量单独地刺激三种视锥细胞）。参考 https://zhuanlan.zhihu.com/p/24214731" }, { "title": "ci-cd 笔记", "url": "/posts/cicd-note/", "categories": "cs, devops", "tags": "ci, cd", "date": "2021-09-25 00:00:00 +0800", "snippet": "CI-CD定义CIContinuous Integration (CI)CI 的意思是 持续构建 。负责拉取代码库中的代码后，执行用户预置定义好的操作脚本，通过一系列编译操作构建出一个 制品 ，并将制品推送至到制品库里面。常用工具有 Gitlab CI，Github CI，Jenkins 等。这个环节不参与部署，只负责构建代码，然后保存构建物。构建物被称为 制品，保存制品的地方被称为 “制品库”CDCD 则有2层含义： 持续部署（Continuous Deployment） 和 持续交付（Continuous Delivery） 。持续交付 的概念是：将制品库的制品拿出后，部署在测试环境 / 交付给客户提前测试。 持续部署 则是将制品部署在生产环境。可以进行持续部署的工具也有很多： Ansible 批量部署， Docker 直接推拉镜像等等。当然也包括我们后面要写到的 Kubernetes 集群部署。镜像库字面意思，镜像库就是集中存放镜像的一个文件服务。镜像库在 CI/CD 中，又称 制品库 。构建后的产物称为制品，制品则要放到制品库做中转和版本管理。常用平台有Nexus，Jfrog，Harbor或其他对象存储平台。在这里，我们选用 Nexus3 作为自己的镜像库。因为其稳定，性能好，免费，部署方便，且支持类型多，是许多制品库的首选选型。在 nexus 中，制品库一般分为以下三种类型： proxy: 此类型制品库原则上只下载，不允许用户推送。可以理解为缓存外网制品的制品库。例如，我们在拉取 nginx 镜像时，如果通过 proxy 类型的制品库，则它会去创建时配置好的外网 docker 镜像源拉取（有点像 cnpm ）到自己的制品库，然后给你。第二次拉取，则不会从外网下载。起到 内网缓存 的作用。 hosted：此类型制品库和 proxy 相反，原则上 只允许用户推送，不允许缓存。这里只存放自己的私有镜像或制品。 group：此类型制品库可以将以上两种类型的制品库组合起来。组合后只访问 group 类型制品库，就都可以访问。docker 在推送一个镜像时，镜像的 Tag (名称:版本号) 开头必须带着镜像库的地址，才可以推送到指定的镜像库。例如 jenkins-test 是不能推送到镜像库的。而 172.16.81.7:8082/jenkins-test 则可以推送到镜像库。KubernetesKubernetes 是 Google 开源的一个容器编排引擎，它支持自动化部署、大规模可伸缩、应用容器化管理。在生产环境中部署一个应用程序时，通常要部署该应用的多个实例以便对应用请求进行负载均衡。通俗些讲，可以将 Kubernetes 看作是用来是一个部署镜像的平台。可以用来操作多台机器调度部署镜像，大大地降低了运维成本。那么， Kubernetes 和 Docker 的关系又是怎样的呢？一个形象的比喻：如果你将 docker 看作是飞机，那么 kubernetes 就是飞机场。在飞机场的加持下，飞机可以根据机场调度选择在合适的时间降落或起飞。在 Kubernetes 中，可以使用集群来组织服务器的。集群中会存在一个 Master 节点，该节点是 Kubernetes 集群的控制节点，负责调度集群中其他服务器的资源。其他节点被称为 Node ， Node 可以是物理机也可以是虚拟机。Flannel前面我们在配置文件中，有提到过配置Pod子网络，Flannel 主要的作用就是如此。它的主要作用是通过创建一个虚拟网络，让不同节点下的服务有着全局唯一的IP地址，且服务之前可以互相访问和连接。Deployment如果你将 k8s 看作是一个大型机场，那么 deployment 刚好就是机场内的停机坪。根据飞机的种类进行划分停机坪，不同的停机坪都停着不同类型的飞机。只不过，deployment 要比停机坪还要灵活，随时可以根据剩余的空地大小（服务器剩余资源）和塔台的指令，增大/变小停机坪的空间。这个“增大变小停机坪空间的动作”，在k8s中就是 deployment 对它下面所属容器数量的扩容/缩小的操作。那么这也就代表，deployment是无状态的，也就不会去负责停机坪中每架飞机之间的通信和组织关系。只需要根据塔台的指令，维护好飞机的更新和进出指令即可。这个根据指令维护飞机更新和进出的行为，在k8s中就是 deployment 对他下面的容器版本更新升级，暂停和恢复更新升级的动作。在这里的容器，并不等于 Docker 中的容器。它在K8S中被称为 Pod 。那么 Pod 是什么 ?apiVersion: apps/v1kind: Deploymentmetadata: name: front-v1spec: selector: matchLabels: app: nginx-v1 replicas: 3 template: metadata: labels: app: nginx-v1 spec: containers: - name: nginx image: registry.cn-hangzhou.aliyuncs.com/janlay/k8s_test:v1 ports: - containerPort: 80PodPod 是 K8S 中最小的可调度单元（可操作/可部署单元），它里面可以包含1个或者多个 Docker 容器。在 Pod 内的所有 Docker 容器，都会共享同一个网络、存储卷、端口映射规则。一个 Pod 拥有一个 IP。但这个 IP 会随着Pod的重启，创建，删除等跟着改变，所以不固定且不完全可靠。这也就是 Pod 的 IP 漂移问题。这个问题我们可以使用下面的 Service 去自动映射我们经常会把 Pod 和 Docker 搞混，这两者的关系就像是豌豆和豌豆荚，Pod 是一个容器组，里面有很多容器，容器组内共享资源。servicedeployment 是停机坪，那么 Service 则是一块停机坪的统一通信入口。它负责自动调度和组织deployment中 Pod 的服务访问。由于自动映射 Pod 的IP，同时也解决了 Pod 的IP漂移问题。下面这张图就印证了 Service 的作用。流量会首先进入 VM（主机），随后进入 Service 中，接着 Service 再去将流量调度给匹配的 Pod 。apiVersion: v1kind: Servicemetadata: name: front-service-v1spec: selector: app: nginx-v1 ports: - protocol: TCP port: 80 targetPort: 80 type: NodePortingress在前面，我们部署了 deployment 和 Service，实现了对服务的访问。但是在实际使用中，我们还会根据请求路径前缀的匹配，权重，甚至根据 cookie/header 的值去访问不同的服务。为了达到这种负载均衡的效果，我们可以使用 k8s 的另一个组件 —— ingress在日常开发中，我们经常会遇到路径分流问题。例如当我们访问 /a 时，需要返回A服务的页面。访问 /b，需要返回服务B的页面。这时候，我们就可以使用 k8s 中的 ingress 去实现。在这里，我们选择 ingress-nginx。 ingress-nginx 是基于 nginx 的一个 ingress 实现。当然也可以实现正则匹配路径，流量转发，基于 cookie header 切分流量（灰度发布）。apiVersion: extensions/v1beta1kind: Ingressmetadata: name: nginx-demo annotations: nginx.ingress.kubernetes.io/rewrite-target: / kubernetes.io/ingress.class: nginxspec: rules: - http: paths: - path: /wss backend: serviceName: front-service-v1 servicePort: 80 backend: serviceName: front-service-v1 servicePort: 80灰度发布Canary rules are evaluated in order of precedence. Precedence is as follows: canary-by-header -&amp;gt; canary-by-cookie -&amp;gt; canary-weightk8s 会优先去匹配 header ，如果未匹配则去匹配 cookie ，最后是 weightannotations: kubernetes.io/ingress.class: nginx nginx.ingress.kubernetes.io/rewrite-target: / nginx.ingress.kubernetes.io/canary: &quot;true&quot; nginx.ingress.kubernetes.io/canary-by-cookie: &quot;users_from_Beijing&quot; nginx.ingress.kubernetes.io/canary-by-header: &quot;test-header-key&quot; nginx.ingress.kubernetes.io/canary-by-header-value: &quot;test-header-value&quot; nginx.ingress.kubernetes.io/canary-weight: &quot;50&quot;滚动发布就绪状态 第一步，我们准备一组服务器。这组服务器当前服务的版本是 V1。 接下来，我们将会使用滚动策略，将其发布到 V2 版本。升级第一个 Pod 第二步开始升级。 首先，会增加一个 V2 版本的 Pod1 上来，将 V1 版本的 Pod1 下线但不移除。此时，V1版本的 Pod1 将不会接受流量进来，而是进入一个平滑期等待状态（大约几十秒）后才会被杀掉。 第一个 Pod 升级完毕升级剩下的 Pod 与上同理，同样是新增新版本Pod后，将旧版本Pod下线进入平滑期但不删除。等平滑期度过后再删除Pod：滚动发布作为众多发布类型的一种，必然也存在着一些优点和缺点：优点 不需要停机更新，无感知平滑更新。 版本更新成本小。不需要新旧版本共存缺点 更新时间长：每次只更新一个/多个镜像，需要频繁连续等待服务启动缓冲（详见下方平滑期介绍） 旧版本环境无法得到备份：始终只有一个环境存在 回滚版本异常痛苦：如果滚动发布到一半出了问题，回滚时需要使用同样的滚动策略回滚旧版本。Kubernetes 健康探针存活探针 Pod 运行时 检测服务是否崩溃，是否需要重启服务 杀死 Pod 并重启可用探针 Pod 运行时 检测服务是不是允许被访问到。 停止Pod的访问调度，不会被杀死重启启动探针 Pod 运行时 检测服务是否启动成功 杀死 Pod 并重启 initialDelaySeconds：容器初始化等待多少秒后才会触发探针。默认为0秒。 periodSeconds：执行探测的时间间隔。默认10秒，最少1秒。 timeoutSeconds：探测超时时间。默认1秒，最少1秒。 successThreshold：探测失败后的最小连续成功数量。默认是1。 failureThreshold：探测失败后的重试次数。默认是3次，最小是1次。Kubernetes SecretSecret 是 Kubernetes 内的一种资源类型，可以用它来存放一些机密信息（密码，token，密钥等）。信息被存入后，我们可以使用挂载卷的方式挂载进我们的 Pod 内。当然也可以存放docker私有镜像库的登录名和密码，用于拉取私有镜像。Kubernetes DNS那么在 Kubernetes 中，如何做服务发现呢？我们前面写到过， Pod 的 IP 常常是漂移且不固定的，所以我们要使用 Service 这个神器来将它的访问入口固定住。但是，我们在部署 Service 时，也不知道部署后的ip和端口如何。那么在 Kubernetes 中，我们可以利用 DNS 的机制给每个 Service 加一个内部的域名，指向其真实的IP。在Kubernetes中，对 Service 的服务发现，是通过一种叫做 CoreDNS 的组件去实现的。CoreDNS 是使用 Go 语言实现的一个DNS服务器。当然，它也不只是可以用在 Kubernetes 上。也可以用作日常 DNS 服务器使用。在 Kubernetes 1.11版本后，CoreDNS 已经被默认安装进了 Kubernetes 内。在 Kubernetes DNS 里，服务发现规则有2种：跨 namespace 和同 namespace 的规则。kubernetes namespace（命名空间）是 kubernetes 里比较重要的一个概念。 在启动集群后，kubernetes 会分配一个默认命名空间，叫default。不同的命名空间可以实现资源隔离，服务隔离，甚至权限隔离。Kubernetes ConfigMap但是在日常开发部署时，我们还会遇到一些环境变量的配置：例如你的数据库地址，负载均衡要转发的服务地址等等信息。这部分内容使用 Secret 显然不合适，打包在镜像内耦合又太严重。这里，我们可以借助 Kubernetes ConfigMap 来配置这项事情ConfigMap 是 Kubernetes 的一种资源类型，我们可以使用它存放一些环境变量和配置文件。信息存入后，我们可以使用挂载卷的方式挂载进我们的 Pod 内，也可以通过环境变量注入。和 Secret 类型最大的不同是，存在 ConfigMap 内的内容不会加密。kubectl create configmap mysql-config –from-literal=host=demo-mysql-service –from-literal=port=30548 –from-literal=username=root –from-literal=database=demo-backendcurl ‘http://127.0.0.1:30795/api/user/list’ -H ‘Connection: keep-alive’ -H ‘Cache-Control: max-age=0’ -H ‘User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.141 Safari/537.36’ -H ‘DNT: 1’ -H ‘content-type: application/json’ -H ‘Accept: /’ -H ‘Referer: http://8.133.162.129:30795/’ -H ‘Accept-Language: zh-CN,zh;q=0.9,en;q=0.8’ –compressed –insecure学习心得如果要让jenkins能够访问k8s的机器，需要加入同一个安全组" }, { "title": "bloom-filter 笔记", "url": "/posts/bloom-filter-note/", "categories": "cs", "tags": "bloom-filter", "date": "2021-09-25 00:00:00 +0800", "snippet": "概念本质上布隆过滤器是一种数据结构，比较巧妙的概率型数据结构（probabilistic data structure），特点是高效地插入和查询，可以用来告诉你 “某样东西一定不存在或者可能存在”。原理 https://zhuanlan.zhihu.com/p/43263751" }, { "title": "base64 笔记", "url": "/posts/base64-note/", "categories": "cs", "tags": "base64", "date": "2021-09-25 00:00:00 +0800", "snippet": "原理Base64是一种用64个字符来表示任意二进制数据的方法。用记事本打开exe、jpg、pdf这些文件时，我们都会看到一大堆乱码，因为二进制文件包含很多无法显示和打印的字符，所以，如果要让记事本这样的文本处理软件能处理二进制数据，就需要一个二进制到字符串的转换方法。Base64是一种最常见的二进制编码方法。Base64的原理很简单，首先，准备一个包含64个字符的数组：[‘A’, ‘B’, ‘C’, … ‘a’, ‘b’, ‘c’, … ‘0’, ‘1’, … ‘+’, ‘/’]然后，对二进制数据进行处理，每3个字节一组，一共是3x8=24bit，划为4组，每组正好6个bit：这样我们得到4个数字作为索引，然后查表，获得相应的4个字符，就是编码后的字符串。所以，Base64编码会把3字节的二进制数据编码为4字节的文本数据，长度增加33%，好处是编码后的文本数据可以在邮件正文、网页等直接显示。如果要编码的二进制数据不是3的倍数，最后会剩下1个或2个字节怎么办？Base64用\\x00字节在末尾补足后，再在编码的末尾加上1个或2个=号，表示补了多少字节，解码的时候，会自动去掉。由于标准的Base64编码后可能出现字符+和/，在URL中就不能直接作为参数，所以又有一种”url safe”的base64编码，其实就是把字符+和/分别变成-和_：参考 https://www.liaoxuefeng.com/wiki/897692888725344/949441536192576" }, { "title": "Welcome to Jekyll!", "url": "/posts/welcome-to-jekyll/", "categories": "jekyll, update", "tags": "jekyll", "date": "2021-06-08 11:42:14 +0800", "snippet": "You’ll find this post in your _posts directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run jekyll serve, which launches a web server and auto-regenerates your site when a file is updated.Jekyll requires blog post files to be named according to the following format:YEAR-MONTH-DAY-title.MARKUPWhere YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and MARKUP is the file extension representing the format used in the file. After that, include the necessary front matter. Take a look at the source for this post to get an idea about how it works.Jekyll also offers powerful support for code snippets:def print_hi(name) puts &quot;Hi, #{name}&quot;endprint_hi(&#39;Tom&#39;)#=&amp;gt; prints &#39;Hi, Tom&#39; to STDOUT.Check out the Jekyll docs for more info on how to get the most out of Jekyll. File all bugs/feature requests at Jekyll’s GitHub repo. If you have questions, you can ask them on Jekyll Talk." }, { "title": "Jekyll bug report", "url": "/posts/jeklly-bug-report/", "categories": "jekyll, bug", "tags": "jekyll", "date": "2021-06-08 11:42:14 +0800", "snippet": "C代码include引用BUG// 当在代码里面使用形如：// [井号]include// 会在其他文章的Further Reading栏目下导致异常markdown中使用标签大于小于 当使用markdown语法的过程中，使用了[大于][小于]时，可能导致文章生成错误标签总结以上两个BUG都是由转义带来的问题，虽然在本地可以通过，但是在github的Actions中执行test.sh时无法通过" }, { "title": "Chirpy Starter", "url": "/posts/chirpy-starter/", "categories": "jekyll, chirpy", "tags": "jekyll", "date": "2021-06-08 11:42:14 +0800", "snippet": "Chirpy Starter The startup template for Jekyll Theme Chirpy.When installing the Chirpy theme through RubyGems, Jekyll can only read files in the folders _includes, _layout, _sass and assets, as well as a small part of options of the _config.yml file from the theme’s gem. (You can find the gem files by using the command bundle info --path jekyll-theme-chirpy). To fully use all the features of Chirpy, you need to copy the other critical files/directories from the theme’s gem to your Jekyll site.The critical files/directories to run or build the Chirpy theme are as follows:.├── _data├── _plugins├── _tabs├── _config.yml└── index.htmlSo we’ve extracted all the Chirpy gem necessary content here to help you get started quickly.InstallationUse this template to generate a new repository, and then execute:$ bundleUsagePlease see the theme’s docs.UpgradingFirst, please modify the target version number of jekyll-theme-chirpy in the Gemfile (e.g., gem &quot;jekyll-theme-chirpy&quot;, &quot;~&amp;gt; 4.0&quot;, &quot;&amp;gt;= 4.0.1&quot;).After that, execute the following command:$ bundle update jekyll-theme-chirpyAs the version upgrades, the critical files and configuration options will change. Please refer to the Upgrade Guide to keep your website files in sync with the latest version of the theme.LicenseThis work is published under MIT License.Local Usagervm use 2.7.2bundle exec jekyll s" }, { "title": "go语言 笔记2", "url": "/posts/go-learning-2/", "categories": "cs, go", "tags": "go", "date": "2021-06-08 00:00:00 +0800", "snippet": "《快学 Go 语言》第 6 课 —— 字典 https://zhuanlan.zhihu.com/p/50047198 如果你可以预知字典内部键值对的数量，那么还可以给 make 函数传递一个整数值，通知运行时提前分配好相应的内存。这样可以避免字典在长大的过程中要经历的多次扩容操作。var m = make(map[int]string, 16) 同 Python 语言一样，字典可以使用中括号来读写内部元素，使用 delete 函数来删除元素。package mainimport &quot;fmt&quot;func main() { var fruits = map[string]int { &quot;apple&quot;: 2, &quot;banana&quot;: 5, &quot;orange&quot;: 8, } // 读取元素 var score = fruits[&quot;banana&quot;] fmt.Println(score) // 增加或修改元素 fruits[&quot;pear&quot;] = 3 fmt.Println(fruits) // 删除元素 delete(fruits, &quot;pear&quot;) fmt.Println(fruits)}// -----------------------// 5// map[apple:2 banana:5 orange:8 pear:3]// map[orange:8 apple:2 banana:5] 删除操作时，如果对应的 key 不存在，delete 函数会静默处理。遗憾的是 delete 函数没有返回值，你无法直接得到 delete 操作是否真的删除了某个元素。你需要通过长度信息或者提前尝试读取 key 对应的 value 来得知。 读操作时，如果 key 不存在，也不会抛出异常。它会返回 value 类型对应的零值。如果是字符串，对应的零值是空串，如果是整数，对应的零值是 0，如果是布尔型，对应的零值是 false。 你不能通过返回的结果是否是零值来判断对应的 key 是否存在，因为 key 对应的 value 值可能恰好就是零值，比如下面的字典你就不能判断 “durin” 是否存在package mainimport &quot;fmt&quot;func main() { var fruits = map[string]int { &quot;apple&quot;: 2, &quot;banana&quot;: 5, &quot;orange&quot;: 8, } // var names = make([]string, 0, len(fruits)) // var scores = make([]int, 0, len(fruits)) var names = make([]string, len(fruits)) var scores = make([]int, len(fruits)) for name, score := range fruits { names = append(names, name) scores = append(scores, score) } fmt.Println(names, scores)}// ----------// [apple banana orange] [0 0 0 2 5 8] 这里需要留意如果使用这样的方式获取keys，那么会在[0 0 0]的基础上append var names = make([]string, len(fruits)) 《快学 Go 语言》第 7 课 —— 字符串 https://zhuanlan.zhihu.com/p/50399072 字符串通常有两种设计，一种是「字符」串，一种是「字节」串。「字符」串中的每个字都是定长的，而「字节」串中每个字是不定长的。Go 语言里的字符串是「字节」串，英文字符占用 1 个字节，非英文字符占多个字节。这意味着无法通过位置来快速定位出一个完整的字符来，而必须通过遍历的方式来逐个获取单个字符。 我们所说的字符通常是指 unicode 字符，你可以认为所有的英文和汉字在 unicode 字符集中都有一个唯一的整数编号，一个 unicode 通常用 4 个字节来表示，对应的 Go 语言中的字符 rune 占 4 个字节。在 Go 语言的源码中可以找到下面这行代码，rune 类型是一个衍生类型，它在内存里面使用 int32 类型的 4 个字节存储。 type rune int32 使用「字符」串来表示字符串势必会浪费空间，因为所有的英文字符本来只需要 1 个字节来表示，用 rune 字符来表示的话那么剩余的 3 个字节都是零。但是「字符」串有一个好处，那就是可以快速定位。 其中 codepoint 是每个「字」的其实偏移量。Go 语言的字符串采用 utf8 编码，中文汉字通常需要占用 3 个字节，英文只需要 1 个字节。len() 函数得到的是字节的数量，通过下标来访问字符串得到的是「字节」。package mainimport &quot;fmt&quot;func main() { var s = &quot;嘻哈china&quot; for i:=0;i&amp;lt;len(s);i++ { fmt.Printf(&quot;%x &quot;, s[i]) }}// -----------// e5 98 bb e5 93 88 63 68 69 6e 61 按字符 rune 遍历package mainimport &quot;fmt&quot;func main() { var s = &quot;嘻哈china&quot; for codepoint, runeValue := range s { fmt.Printf(&quot;%d %d &quot;, codepoint, int32(runeValue)) }}// -----------// 0 22075 3 21704 6 99 7 104 8 105 9 110 10 97 对字符串进行 range 遍历，每次迭代出两个变量 codepoint 和 runeValue。codepoint 表示字符起始位置，runeValue 表示对应的 unicode 编码（类型是 rune）。 在使用 Go 语言进行网络编程时，经常需要将来自网络的字节流转换成内存字符串，同时也需要将内存字符串转换成网络字节流。Go 语言直接内置了字节切片和字符串的相互转换语法。《快学 Go 语言》第 8 课 —— 结构体 https://zhuanlan.zhihu.com/p/50654803 Circle 结构体内部有三个变量，分别是圆心的坐标以及半径。特别需要注意是结构体内部变量的大小写，首字母大写是公开变量，首字母小写是内部变量，分别相当于类成员变量的 Public 和 Private 类别。内部变量只有属于同一个 package（简单理解就是同一个目录）的代码才能直接访问。 结构体的第二种创建形式是不指定字段名称来顺序字段初始化，需要显示提供所有字段的初值，一个都不能少。这种形式称之为「顺序形式」。package mainimport &quot;fmt&quot;type Circle struct { x int y int Radius int}func main() { var c Circle = Circle {100, 100, 50} fmt.Printf(&quot;%+v\\n&quot;, c)}// -------// {x:100 y:100 Radius:50} 最后我们再将三种零值初始化形式放到一起对比观察一下var c1 Circle = Circle{}var c2 Circlevar c3 *Circle = new(Circle) nil 结构体是指结构体指针变量没有指向一个实际存在的内存。这样的指针变量只会占用 1 个指针的存储空间，也就是一个机器字的内存大小。var c *Circle = nil 结构体的拷贝 结构体之间可以相互赋值，它在本质上是一次浅拷贝操作，拷贝了结构体内部的所有字段。结构体指针之间也可以相互赋值，它在本质上也是一次浅拷贝操作，不过它拷贝的仅仅是指针地址值，结构体的内容是共享的。 Go 语言的结构体方法里面没有 self 和 this 这样的关键字来指代当前的对象，它是用户自己定义的变量名称，通常我们都使用单个字母来表示。 结构体的指针方法 如果使用上面的方法形式给 Circle 增加一个扩大半径的方法，你会发现半径扩大不了。这是因为上面的方法和前面的 expandByValue 函数是等价的，只不过是把函数的第一个参数挪了位置而已，参数传递时会复制了一份结构体内容，起不到扩大半径的效果。这时候就必须要使用结构体的指针方法func (c Circle) expand() { c.Radius *= 2}func (c *Circle) expand() { c.Radius *= 2} 结构体指针方法和值方法在调用时形式上是没有区别的，只不过一个可以改变结构体内部状态，而另一个不会。指针方法使用结构体值变量可以调用，值方法使用结构体指针变量也可以调用。 匿名内嵌结构体 还有一种特殊的内嵌结构体形式，内嵌的结构体不提供名称。这时外面的结构体将直接继承内嵌结构体所有的内部字段和方法，就好像把子结构体的一切全部都揉进了父结构体一样。匿名的结构体字段将会自动获得以结构体类型的名字命名的字段名称package mainimport &quot;fmt&quot;type Point struct { x int y int}func (p Point) show() { fmt.Println(p.x, p.y)}type Circle struct { Point // 匿名内嵌结构体 Radius int}func main() { var c = Circle { Point: Point { x: 100, y: 100, }, Radius: 50, } fmt.Printf(&quot;%+v\\n&quot;, c) fmt.Printf(&quot;%+v\\n&quot;, c.Point) fmt.Printf(&quot;%d %d\\n&quot;, c.x, c.y) // 继承了字段 fmt.Printf(&quot;%d %d\\n&quot;, c.Point.x, c.Point.y) c.show() // 继承了方法 c.Point.show()}// -------// {Point:{x:100 y:100} Radius:50}// {x:100 y:100}// 100 100// 100 100// 100 100// 100 100 这里的继承仅仅是形式上的语法糖，c.show() 被转换成二进制代码后和 c.Point.show() 是等价的，c.x 和 c.Point.x 也是等价的。 Go 语言不是面向对象语言在于它的结构体不支持多态，它不能算是一个严格的面向对象语言。多态是指父类定义的方法可以调用子类实现的方法，不同的子类有不同的实现，从而给父类的方法带来了多样的不同行为。下面的例子呈现了 Java 类的多态性。《快学 Go 语言》第 9 课 —— 接口 https://zhuanlan.zhihu.com/p/50942676 空接口 如果一个接口里面没有定义任何方法，那么它就是空接口，任意结构体都隐式地实现了空接口。 Go 语言为了避免用户重复定义很多空接口，它自己内置了一个，这个空接口的名字特别奇怪，叫 interface{} ，初学者会非常不习惯。之所以这个类型名带上了大括号，那是在告诉用户括号里什么也没有。我始终认为这种名字很古怪，它让代码看起来有点丑陋。 空接口里面没有方法，所以它也不具有任何能力，其作用相当于 Java 的 Object 类型，可以容纳任意对象，它是一个万能容器。比如一个字典的 key 是字符串，但是希望 value 可以容纳任意类型的对象，类似于 Java 语言的 Map 类型，这时候就可以使用空接口类型 interface{}。 接口变量的本质 在使用接口时，我们要将接口看成一个特殊的容器，这个容器只能容纳一个对象，只有实现了这个接口类型的对象才可以放进去。 接口变量作为变量来说它也是需要占据内存空间的，通过翻阅 Go 语言的源码可以发现，接口变量也是由结构体来定义的，这个结构体包含两个指针字段，一个字段指向被容纳的对象内存，另一个字段指向一个特殊的结构体 itab，这个特殊的结构体包含了接口的类型信息和被容纳对象的数据类型信息。 接口变量的赋值 变量赋值本质上是一次内存浅拷贝，切片的赋值是拷贝了切片头，字符串的赋值是拷贝了字符串的头部，而数组的赋值呢是直接拷贝整个数组。接口变量的赋值会不会不一样呢？接下来我们做一个实验package mainimport &quot;fmt&quot;type Rect struct { Width int Height int}func main() { var a interface {} var r = Rect{50, 50} a = r var rx = a.(Rect) r.Width = 100 r.Height = 100 fmt.Println(rx)}// ------// {50 50} 指向指针的接口变量 如果将上面的例子改成指针，将接口变量指向结构体指针，那结果就不一样了package mainimport &quot;fmt&quot;type Rect struct { Width int Height int}func main() { var a interface {} var r = Rect{50, 50} a = &amp;amp;r // 指向了结构体指针 var rx = a.(*Rect) // 转换成指针类型 r.Width = 100 r.Height = 100 fmt.Println(rx)}// -------// &amp;amp;{100 100} 从输出结果中可以看出指针变量 rx 指向的内存和变量 r 的内存是同一份。因为在类型转换的过程中只发生了指针变量的内存复制，而指针变量指向的内存是共享的。《快学 Go 语言》第 10 课 —— 错误与异常 https://zhuanlan.zhihu.com/p/51164928 Go 语言规定凡是实现了错误接口的对象都是错误对象，这个错误接口只定义了一个方法。type error interface { Error() string}《快学 Go 语言》第 11 课 —— 千军万马跑协程 https://zhuanlan.zhihu.com/p/51516757package mainimport &quot;fmt&quot;import &quot;time&quot;func main() { fmt.Println(&quot;run in main goroutine&quot;) n := 3 for i:=0; i&amp;lt;n; i++ { go func() { fmt.Println(&quot;dead loop goroutine start&quot;) for {} // 死循环 }() } for { time.Sleep(time.Second) fmt.Println(&quot;main goroutine running&quot;) }} 通过调整上面代码中的变量 n 的值可以发现一个有趣的现象，当 n 值大于 3 时，主协程将没有机会得到运行，而如果 n 值为 3、2、1，主协程依然可以每秒输出一次。要解释这个现象就必须深入了解协程的运行原理 这里的n设定为cpu逻辑核心数 操作系统对线程的调度是抢占式的，也就是说单个线程的死循环不会影响其它线程的执行，每个线程的连续运行受到时间片的限制。 Go 语言运行时对协程的调度并不是抢占式的。如果单个协程通过死循环霸占了线程的执行权，那这个线程就没有机会去运行其它协程了，你可以说这个线程假死了。不过一个进程内部往往有多个线程，假死了一个线程没事，全部假死了才会导致整个进程卡死。《快学 Go 语言》第 12 课 —— 通道 https://zhuanlan.zhihu.com/p/51710515 不同的并行协程之间交流的方式有两种，一种是通过共享变量，另一种是通过队列。Go 语言鼓励使用队列的形式来交流，它单独为协程之间的队列数据交流定制了特殊的语法 —— 通道。 通道是协程的输入和输出。作为协程的输出，通道是一个容器，它可以容纳数据。作为协程的输入，通道是一个生产者，它可以向协程提供数据。通道作为容器是有限定大小的，满了就写不进去，空了就读不出来。通道还有它自己的类型，它可以限定进入通道的数据的类型。 创建通道只有一种语法，那就是 make 全局函数，提供第一个类型参数限定通道可以容纳的数据类型，再提供第二个整数参数作为通道的容器大小。大小参数是可选的，如果不填，那这个通道的容量为零，叫着「非缓冲型通道」，非缓冲型通道必须确保有协程正在尝试读取当前通道，否则写操作就会阻塞直到有其它协程来从通道中读东西。非缓冲型通道总是处于既满又空的状态。与之对应的有限定大小的通道就是缓冲型通道。在 Go 语言里不存在无界通道，每个通道都是有限定最大容量的。 通道作为容器，它可以像切片一样，使用 cap() 和 len() 全局函数获得通道的容量和当前内部的元素个数。通道一般作为不同的协程交流的媒介，在同一个协程里它也是可以使用的。 读写阻塞 通道满了，写操作就会阻塞，协程就会进入休眠，直到有其它协程读通道挪出了空间，协程才会被唤醒。如果有多个协程的写操作都阻塞了，一个读操作只会唤醒一个协程。通道空了，读操作就会阻塞，协程也会进入睡眠，直到有其它协程写通道装进了数据才会被唤醒。如果有多个协程的读操作阻塞了，一个写操作也只会唤醒一个协程。 关闭通道 Go 语言的通道有点像文件，不但支持读写操作， 还支持关闭。读取一个已经关闭的通道会立即返回通道类型的「零值」，而写一个已经关闭的通道会抛异常。如果通道里的元素是整型的，读操作是不能通过返回值来确定通道是否关闭的。 当通道空了，循环会暂停阻塞，当通道关闭时，阻塞停止，循环也跟着结束了。当循环结束时，我们就知道通道已经关闭了。 通道写安全 确保通道写安全的最好方式是由负责写通道的协程自己来关闭通道，读通道的协程不要去关闭通道。package mainimport &quot;fmt&quot;func send(ch chan int) { ch &amp;lt;- 1 ch &amp;lt;- 2 ch &amp;lt;- 3 ch &amp;lt;- 4 close(ch)}func recv(ch chan int) { for v := range ch { fmt.Println(v) }}func main() { var ch = make(chan int, 1) go send(ch) recv(ch)}// -----------// 1// 2// 3// 4 这个方法确实可以解决单写多读的场景，可要是遇上了多写单读的场合该怎么办呢？任意一个读写通道的协程都不可以随意关闭通道，否则会导致其它写通道协程抛出异常。这时候就必须让其它不相干的协程来干这件事，这个协程需要等待所有的写通道协程都结束运行后才能关闭通道。那其它协程要如何才能知道所有的写通道已经结束运行了呢？这个就需要使用到内置 sync 包提供的 WaitGroup 对象，它使用计数来等待指定事件完成。package mainimport &quot;fmt&quot;import &quot;time&quot;import &quot;sync&quot;func send(ch chan int, wg *sync.WaitGroup) { defer wg.Done() // 计数值减一 i := 0 for i &amp;lt; 4 { i++ ch &amp;lt;- i }}func recv(ch chan int) { for v := range ch { fmt.Println(v) }}func main() { var ch = make(chan int, 4) var wg = new(sync.WaitGroup) wg.Add(2) // 增加计数值 go send(ch, wg) // 写 go send(ch, wg) // 写 go recv(ch) // Wait() 阻塞等待所有的写通道协程结束 // 待计数值变成零，Wait() 才会返回 wg.Wait() // 关闭通道 close(ch) time.Sleep(time.Second)}// ---------// 1// 2// 3// 4// 1// 2// 3// 4 多路通道 在真实的世界中，还有一种消息传递场景，那就是消费者有多个消费来源，只要有一个来源生产了数据，消费者就可以读这个数据进行消费。这时候可以将多个来源通道的数据汇聚到目标通道，然后统一在目标通道进行消费。package mainimport &quot;fmt&quot;import &quot;time&quot;// 每隔一会生产一个数func send(ch chan int, gap time.Duration) { i := 0 for { i++ ch &amp;lt;- i time.Sleep(gap) }}// 将多个原通道内容拷贝到单一的目标通道func collect(source chan int, target chan int) { for v := range source { target &amp;lt;- v }}// 从目标通道消费数据func recv(ch chan int) { for v := range ch { fmt.Printf(&quot;receive %d\\n&quot;, v) }}func main() { var ch1 = make(chan int) var ch2 = make(chan int) var ch3 = make(chan int) go send(ch1, time.Second) go send(ch2, 2 * time.Second) go collect(ch1, ch3) go collect(ch2, ch3) recv(ch3)}// ---------// receive 1// receive 1// receive 2// receive 2// receive 3// receive 4// receive 3// receive 5// receive 6// receive 4// receive 7// receive 8// receive 5// receive 9// .... 但是上面这种形式比较繁琐，需要为每一种消费来源都单独启动一个汇聚协程。Go 语言为这种使用场景带来了「多路复用」语法糖，也就是下面要讲的 select 语句，它可以同时管理多个通道读写，如果所有通道都不能读写，它就整体阻塞，只要有一个通道可以读写，它就会继续。下面我们使用 select 语句来简化上面的逻辑package mainimport &quot;fmt&quot;import &quot;time&quot;func send(ch chan int, gap time.Duration) { i := 0 for { i++ ch &amp;lt;- i time.Sleep(gap) }}func recv(ch1 chan int, ch2 chan int) { for { select { case v := &amp;lt;- ch1: fmt.Printf(&quot;recv %d from ch1\\n&quot;, v) case v := &amp;lt;- ch2: fmt.Printf(&quot;recv %d from ch2\\n&quot;, v) } }}func main() { var ch1 = make(chan int) var ch2 = make(chan int) go send(ch1, time.Second) go send(ch2, 2 * time.Second) recv(ch1, ch2)}// ------------// recv 1 from ch2// recv 1 from ch1// recv 2 from ch1// recv 3 from ch1// recv 2 from ch2// recv 4 from ch1// recv 3 from ch2// recv 5 from ch1 上面是多路复用 select 语句的读通道形式，下面是它的写通道形式，只要有一个通道能写进去，它就会打破阻塞。select { case ch1 &amp;lt;- v: fmt.Println(&quot;send to ch1&quot;) case ch2 &amp;lt;- v: fmt.Println(&quot;send to ch2&quot;)} 前面我们讲的读写都是阻塞读写，Go 语言还提供了通道的非阻塞读写。当通道空时，读操作不会阻塞，当通道满时，写操作也不会阻塞。非阻塞读写需要依靠 select 语句的 default 分支。当 select 语句所有通道都不可读写时，如果定义了 default 分支，那就会执行 default 分支逻辑，这样就起到了不阻塞的效果。下面我们演示一个单生产者多消费者的场景。生产者同时向两个通道写数据，写不进去就丢弃。package mainimport &quot;fmt&quot;import &quot;time&quot;func send(ch1 chan int, ch2 chan int) { i := 0 for { i++ select { case ch1 &amp;lt;- i: fmt.Printf(&quot;send ch1 %d\\n&quot;, i) case ch2 &amp;lt;- i: fmt.Printf(&quot;send ch2 %d\\n&quot;, i) default: } }}func recv(ch chan int, gap time.Duration, name string) { for v := range ch { fmt.Printf(&quot;receive %s %d\\n&quot;, name, v) time.Sleep(gap) }}func main() { // 无缓冲通道 var ch1 = make(chan int) var ch2 = make(chan int) // 两个消费者的休眠时间不一样，名称不一样 go recv(ch1, time.Second, &quot;ch1&quot;) go recv(ch2, 2 * time.Second, &quot;ch2&quot;) send(ch1, ch2)}// ------------// send ch1 27// send ch2 28// receive ch1 27// receive ch2 28// send ch1 6708984// receive ch1 6708984// send ch2 13347544// send ch1 13347775// receive ch2 13347544// receive ch1 13347775// send ch1 20101642// receive ch1 20101642// send ch2 26775795// receive ch2 26775795// ... 注意此处，在非阻塞的channel中，往一个不带缓冲的队列中写入数据时可能会失败（数据被丢弃） 通道在其它语言里面的表现形式是队列，在 Java 语言里，带缓冲通道就是并发包内置的 java.util.concurrent.ArrayBlockingQueue，无缓冲通道也是并发包内置的 java.util.concurrent.SynchronousQueue。ArrayBlockingQueue 的内部实现形式是一个数组，多线程读写时需要使用锁来控制并发访问。不过像 Go 语言提供的多路复用效果，Java 语言就没有内置的实现了。 Go 语言的通道内部结构是一个循环数组，通过读写偏移量来控制元素发送和接受。它为了保证线程安全，内部会有一个全局锁来控制并发。对于发送和接受操作都会有一个队列来容纳处于阻塞状态的协程。type hchan struct { qcount uint // 通道有效元素个数 dataqsize uint // 通道容量，循环数组总长度 buf unsafe.Pointer // 数组地址 elemsize uint16 // 内部元素的大小 closed uint32 // 是否已关闭 0或者1 elemtype *_type // 内部元素类型信息 sendx uint // 循环数组的写偏移量 recvx uint // 循环数组的读偏移量 recvq waitq // 阻塞在读操作上的协程队列 sendq waitq // 阻塞在写操作上的协程队列 lock mutex // 全局锁}《快学 Go 语言》第 13 课 —— 并发与安全 https://zhuanlan.zhihu.com/p/52376005 日常应用中，大多数并发数据结构都是读多写少的，对于读多写少的场合，可以将互斥锁换成读写锁，可以有效提升性能。sync 包也提供了读写锁对象 RWMutex，不同于互斥锁只有两个常用方法 Lock() 和 Unlock()，读写锁提供了四个常用方法，分别是写加锁 Lock()、写释放锁 Unlock()、读加锁 RLock() 和读释放锁 RUnlock()。写锁是排他锁，加写锁时会阻塞其它协程再加读锁和写锁，读锁是共享锁，加读锁还可以允许其它协程再加读锁，但是会阻塞加写锁。 读写锁在写并发高的情况下性能退化为普通的互斥锁。《快学 Go 语言》第 14 课 —— 魔术变性指针 https://zhuanlan.zhihu.com/p/52756600 在 Go 语言里不同类型之间的转换是要受限的。普通的基础变量转换成不同的类型需要进行内存浅拷贝，而指针变量类型之间是禁止直接转换的。要打破这个限制，unsafe.Pointer 就可以派上用场，它允许任意指针类型的互转。package mainimport &quot;fmt&quot;import &quot;unsafe&quot;type Rect struct { Width int Height int}func main() { var r = Rect {50, 50} // var pw *int var pw = (*int)(unsafe.Pointer(&amp;amp;r)) // var ph *int var ph = (*int)(unsafe.Pointer(uintptr(unsafe.Pointer(&amp;amp;r)) + unsafe.Offsetof(r.Height)) *pw = 100 *ph = 100 fmt.Println(r.Width, r.Height)}// --------// 100 100 从上面 5 个 问题，我们可以得出结论，接口类型和结构体类型似乎是两个不同的世界。只有接口类型之间的赋值和转换会共享数据，其它情况都会复制数据，其它情况包括结构体之间的赋值，结构体转接口，接口转结构体。不同接口变量之间的转换本质上只是调整了接口变量内部的类型指针，数据指针并不会发生改变。《快学 Go 语言》第 15 课 —— 反射 https://zhuanlan.zhihu.com/p/53114706 reflect.Kind，reflect 包定义了十几种内置的「元类型」，每一种元类型都有一个整数编号，这个编号使用 reflect.Kind 类型表示。不同的结构体是不同的类型，但是它们都是同一个元类型 Struct。包含不同子元素的切片也是不同的类型，但是它们都会同一个元类型 Slice。type Kind uintconst ( Invalid Kind = iota // 不存在的无效类型 Bool Int Int8 Int16 Int32 Int64 Uint Uint8 Uint16 Uint32 Uint64 Uintptr // 指针的整数类型，对指针进行整数运算时使用 Float32 Float64 Complex64 Complex128 Array // 数组类型 Chan // 通道类型 Func // 函数类型 Interface // 接口类型 Map // 字典类型 Ptr // 指针类型 Slice // 切片类型 String // 字符串类型 Struct // 结构体类型 UnsafePointer // unsafe.Pointer 类型) 它是一个接口类型，里面定义了非常多的方法用于获取和这个类型相关的一切信息。这个接口的结构体实现隐藏在 reflect 包里，每一种类型都有一个相关的类型结构体来表达它的结构信息。type Type interface { ... Method(i int) Method // 获取挂在类型上的第 i&#39;th 个方法 ... NumMethod() int // 该类型上总共挂了几个方法 Name() string // 类型的名称 PkgPath() string // 所在包的名称 Size() uintptr // 占用字节数 String() string // 该类型的字符串形式 Kind() Kind // 元类型 ... Bits() // 占用多少位 ChanDir() // 通道的方向 ... Elem() Type // 数组，切片，通道，指针，字典(key)的内部子元素类型 Field(i int) StructField // 获取结构体的第 i&#39;th 个字段 ... In(i int) Type // 获取函数第 i&#39;th 个参数类型 Key() Type // 字典的 key 类型 Len() int // 数组的长度 NumIn() int // 函数的参数个数 NumOut() int // 函数的返回值个数 Out(i int) Type // 获取函数 第 i&#39;th 个返回值类型 common() *rtype // 获取类型结构体的共同部分 uncommon() *uncommonType // 获取类型结构体的不同部分} 所有的类型结构体都包含一个共同的部分信息，这部分信息使用 rtype 结构体描述，rtype 实现了 Type 接口的所有方法。剩下的不同的部分信息各种特殊类型结构体都不一样。可以将 rtype 理解成父类，特殊类型的结构体是子类，会有一些不一样的字段信息。// 基础类型 rtype 实现了 Type 接口type rtype struct { size uintptr // 占用字节数 ptrdata uintptr hash uint32 // 类型的hash值 ... kind uint8 // 元类型 ...}// 切片类型type sliceType struct { rtype elem *rtype // 元素类型}// 结构体类型type structType struct { rtype pkgPath name // 所在包名 fields []structField // 字段列表}... 不同于 reflect.Type 接口，reflect.Value 是结构体类型，一个非常简单的结构体。type Value struct { typ *rtype // 变量的类型结构体 ptr unsafe.Pointer // 数据指针 flag uintptr // 标志位}" }, { "title": "go语言 笔记1", "url": "/posts/go-learning-1/", "categories": "cs, go", "tags": "go", "date": "2021-06-08 00:00:00 +0800", "snippet": "《快学 Go 语言》第 1 课 —— Hello World https://zhuanlan.zhihu.com/p/48013279 go不需要要设置goroot和gopath（默认~/go）《快学 Go 语言》第 2 课 —— 变量基础 https://zhuanlan.zhihu.com/p/48153187var s int = 42var s = 42s := 42 如果一个变量很重要，建议使用第一种显示声明类型的方式来定义，比如全局变量的定义就比较偏好第一种定义方式。如果要使用一个不那么重要的局部变量，就可以使用第三种。比如循环下标变量如果在第一种声明变量的时候不赋初值，编译器就会自动赋予相应类型的「零值」，不同类型的零值不尽相同，比如字符串的零值不是 nil，而是空串，整形的零值就是 0 ，布尔类型的零值是 false。package mainimport &quot;fmt&quot;func main() { var value int = 42 var p1 *int = &amp;amp;value var p2 **int = &amp;amp;p1 var p3 ***int = &amp;amp;p2 fmt.Println(p1, p2, p3) fmt.Println(*p1, **p2, ***p3)}// ----------// 0xc4200160a0 0xc42000c028 0xc42000c030// 42 42 42 我们又看到了久违的指针符号 * 和取地址符 &amp;amp;，在功能和使用上同 C 语言几乎一摸一样。同 C 语言一样，指针还支持二级指针，三级指针，只不过在日常应用中，很少遇到。package mainimport &quot;fmt&quot;func main() { // 有符号整数，可以表示正负 var a int8 = 1 // 1 字节 var b int16 = 2 // 2 字节 var c int32 = 3 // 4 字节 var d int64 = 4 // 8 字节 fmt.Println(a, b, c, d) // 无符号整数，只能表示非负数 var ua uint8 = 1 var ub uint16 = 2 var uc uint32 = 3 var ud uint64 = 4 fmt.Println(ua, ub, uc, ud) // int 类型，在32位机器上占4个字节，在64位机器上占8个字节 var e int = 5 var ue uint = 5 fmt.Println(e, ue) // bool 类型 var f bool = true fmt.Println(f) // 字节类型 var j byte = &#39;a&#39; fmt.Println(j) // 字符串类型 var g string = &quot;abcdefg&quot; fmt.Println(g) // 浮点数 var h float32 = 3.14 var i float64 = 3.141592653 fmt.Println(h, i)}// -------------// 1 2 3 4// 1 2 3 4// 5 5// true// abcdefg// 3.14 3.141592653// 97 还有另外几个不常用的数据类型，读者可以暂不理会。 复数类型 complex64 和 complex128 unicode字符类型 rune uintptr 指针类型 简单一点说 rune 和 byte 的关系就好比 Python 里面的 unicode 和 byte 、Java 语言里面的 char 和 byte 。uintptr 相当于 C 语言里面的 void* 指针类型。《快学 Go 语言》第 3 课 —— 分支与循环 https://zhuanlan.zhihu.com/p/48300291func prize2(score int) string { switch { case score &amp;lt; 60: return &quot;差&quot; case score &amp;lt; 80: return &quot;及格&quot; case score &amp;lt; 90: return &quot;良&quot; default: return &quot;优&quot; }} switch 从第一个判断表达式为 true 的 case 开始执行，如果 case 带有 fallthrough，程序会继续执行下一条 case，且它不会去判断下一个 case 的表达式是否为 true。switch 有两种匹配模式，一种是变量值匹配，一种是表达式匹配。switch 还支持特殊的类型匹配语法。《快学 Go 语言》第 4 课 —— 低调的数组 https://zhuanlan.zhihu.com/p/48927056package mainimport &quot;fmt&quot;func main() { var a = [9]int{1, 2, 3, 4, 5, 6, 7, 8, 9} var b [10]int = [10]int{1, 2, 3, 4, 5, 6, 7, 8, 9, 10} c := [8]int{1, 2, 3, 4, 5, 6, 7, 8} fmt.Println(a) fmt.Println(b) fmt.Println(c)}// ---------------------// [1 2 3 4 5 6 7 8 9]// [1 2 3 4 5 6 7 8 9 10]// [1 2 3 4 5 6 7 8] Go 会在编译后的代码中插入下标越界检查的逻辑，所以数组的下标访问效率是要打折扣的，比不得 C 语言的数组访问性能。《快学 Go 语言》第 5 课 —— 灵活的切片 https://zhuanlan.zhihu.com/p/49415852 上图中一个切片变量包含三个域，分别是底层数组的指针、切片的长度 length 和切片的容量 capacity。切片支持 append 操作可以将新的内容追加到底层数组，也就是填充上面的灰色格子。如果格子满了，切片就需要扩容，底层的数组就会更换。package mainimport &quot;fmt&quot;func main() { var s1 []int = make([]int, 5, 8) var s2 []int = make([]int, 8) // 满容切片 fmt.Println(s1) fmt.Println(s2)}// -------------// [0 0 0 0 0]// [0 0 0 0 0 0 0 0] make 函数创建切片，需要提供三个参数，分别是切片的类型、切片的长度和容量。其中第三个参数是可选的，如果不提供第三个参数，那么长度和容量相等，也就是说切片的满容的 切片的初始化使用 make 函数创建的切片内容是「零值切片」，也就是内部数组的元素都是零值。Go 语言还提供了另一个种创建切片的语法，允许我们给它赋初值。使用这种方式创建的切片是满容的。 空切片在创建切片时，还有两个非常特殊的情况需要考虑，那就是容量和长度都是零的切片，叫着「空切片」，这个不同于前面说的「零值切片」。package mainimport &quot;fmt&quot;func main() { var s1 []int var s2 []int = []int{} var s3 []int = make([]int, 0) fmt.Println(s1, s2, s3) fmt.Println(len(s1), len(s2), len(s3)) fmt.Println(cap(s1), cap(s2), cap(s3))}// -----------// [] [] []// 0 0 0// 0 0 0 上面三种形式创建的切片都是「空切片」，不过在内部结构上这三种形式是有差异的，甚至第一种都不叫「空切片」，而是叫着「 nil 切片」。但是在形式上它们一摸一样，用起来没有区别。所以初级用户可以不必区分「空切片」和「 nil 切片」，到后续章节我们会仔细分析这两种形式的区别。 切片的赋值切片的赋值是一次浅拷贝操作，拷贝的是切片变量的三个域，你可以将切片变量看成长度为 3 的 int 型数组，数组的赋值就是浅拷贝。拷贝前后两个变量共享底层数组，对一个切片的修改会影响另一个切片的内容，这点需要特别注意。```gopackage mainimport “fmt”func main() { var s1 = make([]int, 5, 8) // 切片的访问和数组差不多 for i := 0; i &amp;lt; len(s1); i++ { s1[i] = i + 1 } var s2 = s1 fmt.Println(s1, len(s1), cap(s1)) fmt.Println(s2, len(s2), cap(s2))// 尝试修改切片内容 s2[0] = 255 fmt.Println(s1) fmt.Println(s2)}// ——–// [1 2 3 4 5] 5 8// [1 2 3 4 5] 5 8// [255 2 3 4 5]// [255 2 3 4 5]&amp;gt; 从上面的输出中可以看到赋值的两切片共享了底层数组。&amp;gt; 文章开头提到切片是动态的数组，其长度是可以变化的。什么操作可以改变切片的长度呢，这个操作就是追加操作。切片每一次追加后都会形成新的切片变量，如果底层数组没有扩容，那么追加前后的两个切片变量共享底层数组，如果底层数组扩容了，那么追加前后的底层数组是分离的不共享的。如果底层数组是共享的，一个切片的内容变化就会影响到另一个切片，这点需要特别注意。```gopackage mainimport &quot;fmt&quot;func main() { var s1 = []int{1,2,3,4,5} fmt.Println(s1, len(s1), cap(s1)) // 对满容的切片进行追加会分离底层数组 var s2 = append(s1, 6) fmt.Println(s1, len(s1), cap(s1)) fmt.Println(s2, len(s2), cap(s2)) // 对非满容的切片进行追加会共享底层数组 var s3 = append(s2, 7) fmt.Println(s2, len(s2), cap(s2)) fmt.Println(s3, len(s3), cap(s3))}// --------------------------// [1 2 3 4 5] 5 5// [1 2 3 4 5] 5 5// [1 2 3 4 5 6] 6 10// [1 2 3 4 5 6] 6 10// [1 2 3 4 5 6 7] 7 10 正是因为切片追加后是新的切片变量，Go 编译器禁止追加了切片后不使用这个新的切片变量，以避免用户以为追加操作的返回值和原切片变量是同一个变量。package mainimport &quot;fmt&quot;func main() { var s1 = []int{1,2,3,4,5} append(s1, 6) fmt.Println(s1)}// --------------// ./main.go:7:8: append(s1, 6) evaluated but not used 还需要注意的是追加虽然会导致底层数组发生扩容，更换的新的数组，但是旧数组并不会立即被销毁被回收，因为老切片还指向这旧数组。package mainimport &quot;fmt&quot;func main() { var s1 = []int{1,2,3,4,5,6,7} // start_index 和 end_index，不包含 end_index // [start_index, end_index) var s2 = s1[2:5] fmt.Println(s1, len(s1), cap(s1)) fmt.Println(s2, len(s2), cap(s2))}// ------------// [1 2 3 4 5 6 7] 7 7// [3 4 5] 3 5 记住前闭后开，[start_index, end_index)package mainimport &quot;fmt&quot;func main() { var s1 = []int{1, 2, 3, 4, 5, 6, 7} var s2 = s1[:5] var s3 = s1[3:] var s4 = s1[:] fmt.Println(s1, len(s1), cap(s1)) fmt.Println(s2, len(s2), cap(s2)) fmt.Println(s3, len(s3), cap(s3)) fmt.Println(s4, len(s4), cap(s4))}// -----------// [1 2 3 4 5 6 7] 7 7// [1 2 3 4 5] 5 7// [4 5 6 7] 4 4// [1 2 3 4 5 6 7] 7 7 细心的同学可能会注意到上面的 s1[:] 很特别，它和普通的切片赋值有区别么？答案是没区别使用过 Python 的同学可能会问，切片支持负数的位置么，答案是不支持，下标不可以是负数。 数组变切片对数组进行切割可以转换成切片，切片将原数组作为内部底层数组。也就是说修改了原数组会影响到新切片，对切片的修改也会影响到原数组。 copy 函数Go 语言还内置了一个 copy 函数，用来进行切片的深拷贝。不过其实也没那么深，只是深到底层的数组而已。如果数组里面装的是指针，比如 []*int 类型，那么指针指向的内容还是共享的。func copy(dst, src []T) int copy 函数不会因为原切片和目标切片的长度问题而额外分配底层数组的内存，它只负责拷贝数组的内容，从原切片拷贝到目标切片，拷贝的量是原切片和目标切片长度的较小值 —— min(len(src), len(dst))，函数返回的是拷贝的实际长度。我们来看一个例子package mainimport &quot;fmt&quot;func main() { var s = make([]int, 5, 8) for i:=0;i&amp;lt;len(s);i++ { s[i] = i+1 } fmt.Println(s) var d = make([]int, 2, 6) var n = copy(d, s) fmt.Println(n, d)}// -----------// [1 2 3 4 5]// 2 [1 2] 当比较短的切片扩容时，系统会多分配 100% 的空间，也就是说分配的数组容量是切片长度的2倍。但切片长度超过1024时，扩容策略调整为多分配 25% 的空间，这是为了避免空间的过多浪费。试试解释下面的运行结果。 Go 语言「切片」的三种特殊状态https://zhuanlan.zhihu.com/p/49529590 我们如何来分析三面四种形式的内部结构的区别呢？接下里要使用到 Go 语言的高级内容，通过 unsafe.Pointer 来转换 Go 语言的任意变量类型。因为切片的内部结构是一个结构体，包含三个机器字大小的整型变量，其中第一个变量是一个指针变量，指针变量里面存储的也是一个整型值，只不过这个值是另一个变量的内存地址。我们可以将这个结构体看成长度为 3 的整型数组 [3]int。然后将切片变量转换成 [3]int。type slice struct { array unsafe.Pointer length int capcity int}package mainimport &quot;fmt&quot;func main() { var s1 []int var s2 = []int{} var s3 = make([]int, 0) var s4 = *new([]int) var a1 = *(*[3]int)(unsafe.Pointer(&amp;amp;s1)) var a2 = *(*[3]int)(unsafe.Pointer(&amp;amp;s2)) var a3 = *(*[3]int)(unsafe.Pointer(&amp;amp;s3)) var a4 = *(*[3]int)(unsafe.Pointer(&amp;amp;s4)) fmt.Println(a1) fmt.Println(a2) fmt.Println(a3) fmt.Println(a4)}// ---------------------// [0 0 0]// [824634199592 0 0]// [824634199592 0 0]// [0 0 0] 从输出中我们看到了明显的神奇的让人感到意外的难以理解的不一样的结果。其中输出为 [0 0 0] 的 s1 和 s4 变量就是「 nil 切片」，s2 和 s3 变量就是「空切片」。824634199592 这个值是一个特殊的内存地址，所有类型的「空切片」都共享这一个内存地址。– 我本地运行的结果是： [842350747160 0 0][842350747160 0 0]var s2 = []int{}var s3 = make([]int, 0)var a2 = *(*[3]int)(unsafe.Pointer(&amp;amp;s2))var a3 = *(*[3]int)(unsafe.Pointer(&amp;amp;s3))fmt.Println(a2)fmt.Println(a3)var s5 = make([]struct{ x, y, z int }, 0)var a5 = *(*[3]int)(unsafe.Pointer(&amp;amp;s5))fmt.Println(a5)// --------// [824634158720 0 0]// [824634158720 0 0]// [824634158720 0 0]//// runtime/malloc.go// base address for all 0-byte allocationsvar zerobase uintptr// 分配对象内存func mallocgc(size uintptr, typ *_type, needzero bool) unsafe.Pointer { ... if size == 0 { return unsafe.Pointer(&amp;amp;zerobase) } ...}//// runtime/slice.go// 创建切片func makeslice(et *_type, len, cap int) slice { ... p := mallocgc(et.size*uintptr(cap), et, true) return slice{p, len, cap}} 个人理解，空切片与nil切片的区别在于切片的数组地址指向的究竟是zerobase（空切片，调用了makeslice）还是0（nil切片）package mainimport &quot;fmt&quot;func main() { var s1 []int var s2 = []int{} fmt.Println(s1 == nil) fmt.Println(s2 == nil) fmt.Printf(&quot;%#v\\n&quot;, s1) fmt.Printf(&quot;%#v\\n&quot;, s2)}// -------// true// false// []int(nil)// []int{} 所以为了避免写代码的时候把脑袋搞昏的最好办法是不要创建「 空切片」，统一使用「 nil 切片」，同时要避免将切片和 nil 进行比较来执行某些逻辑。这是官方的标准建议。 The former declares a nil slice value, while the latter is non-nil but zero-length. They are functionally equivalent—their len and cap are both zero—but the nil slice is the preferred style. 「空切片」和「 nil 切片」有时候会隐藏在结构体中，这时候它们的区别就被太多的人忽略了，下面我们看个例子type Something struct { values []int}var s1 = Something{}var s2 = Something{[]int{}}fmt.Println(s1.values == nil)fmt.Println(s2.values == nil)// --------// true// false 「空切片」和「 nil 切片」还有一个极为不同的地方在于 JSON 序列化" } ]
